## Метрика и нормирование признаков

- [Метрика и нормирование признаков](#метрика-и-нормирование-признаков)
  - [Использование метрики для восстановления данных](#использование-метрики-для-восстановления-данных)
    - [Что такое метрика](#что-такое-метрика)
    - [Свойства метрики](#свойства-метрики)
    - [Евклидова метрика L2](#евклидова-метрика-l2)
    - [Метрика Манхэттен L1](#метрика-манхэттен-l1)
    - [Метрика Чебышёва L1∞](#метрика-чебышёва-l1)
    - [Использование метрики](#использование-метрики)
    - [Штрафование ошибок](#штрафование-ошибок)
    - [Выбор метрики](#выбор-метрики)
    - [Исходный код](#исходный-код)
  - [Нормирование (нормализация) признаков](#нормирование-нормализация-признаков)
    - [Замечание об использовании метрики](#замечание-об-использовании-метрики)
    - [Способы нормирования признака](#способы-нормирования-признака)
      - [Стандартизация](#стандартизация)
      - [Диапазонное шкалирование (min-max scaling)](#диапазонное-шкалирование-min-max-scaling)
      - [Нормализация средним (centered min-max scaling)](#нормализация-средним-centered-min-max-scaling)
      - [Нормализация максимумом по модулю](#нормализация-максимумом-по-модулю)
      - [Робастное шкалирование (Robust scaler)](#робастное-шкалирование-robust-scaler)
    - [Влияние выбросов](#влияние-выбросов)
      - [Неустойчивые и робастные статистики](#неустойчивые-и-робастные-статистики)
      - [Процентили](#процентили)
    - [Использование нормализации признаков](#использование-нормализации-признаков)
    - [Выбор вида нормализации](#выбор-вида-нормализации)
      - [Нормализация для L1](#нормализация-для-l1)
      - [Нормализация для L2](#нормализация-для-l2)
      - [Нормализация для L∞](#нормализация-для-l)
      - [Программный код](#программный-код)
  - [Заключение](#заключение)
  - [Практическая работа. Восстановление данных с помощью метрики](#практическая-работа-восстановление-данных-с-помощью-метрики)
  - [Источники информации](#источники-информации)

### Использование метрики для восстановления данных
Как восстановить данные, используя меру близости объектов друг к другу? Метрика является математическим аналогом понятия близости объектов друг другу. Это пригодится для решения проблемы восстановления пропущенных данных. Допустим, у какого-то объекта (строки таблицы) пропущено какое-либо значение признака (пустая ячейка таблицы). Зная, как вычислить меру близости объектов друг другу, можно для требуемого объекта найти наиболее близкий к нему по свойствам другой объект, у которого значение в соответствующем столбце известно, и с помощью значения из ячейки близкого объекта (ближайшего соседа) восстановить значение ячейки искомого объекта. Для этого и нужно знать, как вычислить меру близости друг другу.

#### Что такое метрика
Что такое метрика? Это обобщение понятия расстояние из геометрии — например, знакомого всем из школы расстояния между двумя точками. В отличие от расстояния метрика может быть вычислена не только для пары точек, но также для объектов произвольной природы, при этом она не обязана вычисляться по известной формуле из школьного учебника геометрии: для точек $A(x_1, y_1)$ и $B(x_2, y_2)$ на плоскости расстояние $d = \sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}$. Существует несколько формул для вычисления метрики.

Допустим, даны два набора чисел:

$$ P = (p_1, p_2, \dots, p_n) $$

$$ Q = (q_1, q_2, \dots, q_n) $$

Как найти расстояние между $P$ и $Q$, т.е. вычислить значение метрики на паре $P$ и $Q$?

1. **Евклидова метрика** (L2) (как в школьном учебнике геометрии): чувствительна ко всем измерениям, подходит для нормализованных данных с равновзвешенными признаками; в sklearn: `metric='euclidean'`.

    $$ \rho(P, Q) = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2 + \dots + (p_n-q_n)^2} $$

2. **Метрика Манхеттен** (L1): устойчива к выбросам, лучше для высокомерных данных или городских сетей (grid-like); `metric='manhattan'`.

    $$ \rho(P, Q) = |p_1-q_1| + |p_2-q_2| + \dots + |p_n-q_n| $$

    При чём тут Манхеттен? Если нужно обойти квартал, то нужно идти по его сторонам, так как нельзя пройти сквозь здание.

    ![Manhattan](../img/manhattan.png)

     Зеленый отрезок на рисунке выше, выражает прямолинейный путь — кратчайшее расстояние между двумя точками. Длина этого отрезка вычисляется как евклидова метрика. В реальности же приходится идти не по прямой, а огибая кварталы. Соответственно, путь, который пешеход пройдет внутри жилого квартала, двигаясь от одной точки до другой, будет выражен синей линией, длина которой вычисляется по метрике Манхеттен. Именно отсюда метрика и заимствует свое название.

3. **Метрика Чебышёва** или **max-метрика** (L∞): фокусируется на максимальном расхождении, идеальна при доминирующем признаке или шахматных паттернах; `metric='chebyshev'`.

    $$ \rho(P, Q) = \max\{|p_1-q_1|, |p_2-q_2|, \dots, |p_n-q_n|\} $$

4. **Другие способы**: помимо этого к признакам можно предварительно применять разные функции, выполняющие некоторые промежуточные преобразования (например, $log$). Так, логарифмирование оправдано, когда значение признака $P$ сильно разрежено и значения отличаются друг от друга на порядок: $\{1, 10, 100\}$. После применения ко всем признакам логарифма по основанию 10 получается более компактный числовой ряд: $\{0, 1, 2\}$

Приведенный список не исчерпывает все возможные функции, которые могут быть приняты за метрику. Естественно, не любая функция подойдет в качестве метрики. Она должна удовлетворять некоторым свойствам.

#### Свойства метрики

1. Значение метрики на одинаковых объектов должно быть равно 0:

    $$\rho(P, P) = 0 $$

    Это означает, что расстояние от объекта до него же самого должно быть равно 0 (что, в принципе, логично).

2. Для метрики должен соблюдаться аналог симметричности:

    $$ \rho(P, Q) = \rho(Q, P) $$

    То есть расстояние от первого объекта до второго должно быть равным таковому от второго до первого. Это также разумно. Однако если такие ситуации в жизни, когда расстояние от первой точки до второй не равно таковому от второй точки до первой? Примером такой ситуации могут быть участки городской транспортной сети с односторонним движением.

3. Неравенство треугольника:

    $$ \rho(P, Q) \leq \rho(P, T) + \rho(T, Q) $$

    ![Triangle inequality](./img/triangle-inequality.png)

    Это свойство часто используется в анализе данных. Суть его заключается в том, что если имеются три объекта (точки в пространстве или на плоскости), то расстояние от точки $P$ до точки $Q$ должно быть меньше суммы расстояний через любую промежуточную точку $T$, то есть путь $PQ$ должен быть меньше $PT + TQ$. В качестве примера из жизни, когда такое неравенство не выполняется, можно привести транспортную сеть, где $P$, $T$ и $Q$ — это населенные пункты, причем дорога от $P$ до $Q$ очень извилистая и неудобная, а отрезки $PT$ и $TQ$ — прямые и максимально скоростные. Для такой тройки населенных пунктов неравенство треугольников выполняться не будет.

#### Евклидова метрика L2
Евклидово расстояние (расстояние по прямой) самое интуитивное для понимания: именно Евклидову метрику мы представляем, когда кто-то просит нас измерить расстояние между точками.

$$ d = \sqrt{\sum{(x_i-y_i)^2}} $$

<dfn title="евклидово расстояние">Евклидово расстояние</dfn> — это прямая линия между двумя точками с координатами X и Y. Например, одной из таких точек может быть город на карте с его координатами долготы и широты.

Евклидово расстояние характеризуется прямой линией. Допустим, вам нужно измерить расстояние по прямой между точками A и B на карте города, приведённой ниже.

![L2](../img/EuclidMap.webp)

Для расчёта Евклидового расстояния вам понадобятся лишь координаты этих двух точек. Дистанцию между ними можно будет рассчитать по формуле Пифагора.

Теорема Пифагора гласит, что можно рассчитать длину «диагональной стороны» (гипотенузы) прямого треугольника, зная длины его горизонтальной и вертикальной стороны (катетов). Формула выглядит так: a² + b² = c².

![L2 расчет](../img/EuclidFormul.webp)

Прим. ред. В четвёртой строке вычислений допущена ошибка: (-260)^2 = 67 600, а не 76 600. Тогда результат будет равен ~321.[^3-basic-distances-in-data-science]

#### Метрика Манхэттен L1
Расстояние L1 также известно как расстояние городских кварталов, манхэттенское расстояние, расстояние такси, метрика прямоугольного города — оно измеряет дистанцию не по кратчайшей прямой, а по блокам. Расстояние L1 измеряет дистанцию между городскими блоками: это расстояние всех прямых линий пути.

$$ d = \sum{|x_i-y_i|} $$

На следующем изображении показано расстояние L1 между двумя точками.

![L1](../img/L1DistMap.webp)

Кроме показанного пути существует несколько альтернативных способов. Например, от точки A можно подняться на два блока вверх, а потом на три блока вправо, либо же на три блока вправо и два блока вверх.

Но расстояние L1 — это всё же просто дистанция, а поэтому траектория здесь не имеет значения. Единственное, что нужно понимать, это примерный путь: нужно пройти какое-то количество X блоков на восток и Y блоков на север. Сумма расстояний этих блоков и будет расстоянием L1 от точки A до точки B.[^3-basic-distances-in-data-science]

![L1 расчет](../img/L1Formul.webp)

#### Метрика Чебышёва L1∞
Расстояние Чебышёва известно ещё как метрика шахматной доски. Чтобы понять принцип такой метрики, нужно представить короля на шахматной доске — он может ходить во всех направлениях: вперёд, назад, влево, вправо и по диагонали.

![Linf](../img/ChebyshevMap.webp)

Разница расстояния L1 и расстояния Чебышёва в том, что при переходе на одну клетку по диагонали в первом случае засчитывается два хода (например вверх и влево), а во втором случае засчитывается всего один ход.

$$ d = \max{|x_i-y_i|} $$

Ещё эти оба расстояния отличаются от Евклидового расстояния тем, что у Евклидового движение по диагонали рассчитывается по теореме Пифагора.

![Евклидова, L1 и Чебышёва — 3 основные метрики, которые пригодятся в Data Science 6](../img/3vars.webp)

Расстояние Чебышёва можно представить как проход по шахматной доске.

Вот ещё один пример представления расстояния Чебышёва. Допустим, у вас есть дрон с двумя независимыми моторами: первый мотор тянет дрон вперёд, второй — в сторону. Оба мотора могут работать одновременно и равномерно на максимуме своей мощности.

Поэтому дрон может передвинуться на одну клетку по диагонали так же быстро, как по горизонтали или вертикали.

Посмотрите ещё раз на карту города по расстоянию Чебышёва. Первый шаг — оба мотора работают одновременно, второй шаг идентичен первому, а на третьем шаге мотор, тянущий дрон вперёд, отключается, и дрон смещается в сторону.

Таким образом, расстояние Чебышёва определяется как самая большая дистанция на одной оси.

![Linf расчет](../img/ChebyshevFormul.webp)

Прим. ред. Полученный результат является условным и некорректно сравнивать его с другими результатами.[^3-basic-distances-in-data-science]

#### Использование метрики
Давайте попробуем применить метрику для решения нашей задачи. Итак, представим, что есть некоторый объект, представленный строкой таблицы. Пусть у объекта $A$ значение признака $P$ отсутствует или некорректно. Как восстановить значение $P$ для $A$?

Основная идея — рассчитать расстояние от объекта $A$ до других объектов таблицы для того, чтобы найти объекты, наиболее близко расположенные к объекту $A$. Тогда значение признака $P$ из ближайших $k$ объектов можно взять за значение признака $P$ объекта $A$.

1. Исключим пока из таблицы столбец с признаком $P$, содержащим пропуски (мы предполагаем, что остальные ячейки в таблице нормальные).

2. Найдем расстояния (с помощью некоторой метрики) от строки $A$ до остальных объектов таблицы. Получим числа

    $$ \rho(A, A_1), \rho(A, A_2), \dots, \rho(A, A_n) $$

    Эти числа выражают расстояния от объекта $A$ до соответствующего объекта из таблицы.

3. Пусть значения признака $P$ для объектов $A_1, A_2, \dots, A_n$ равны $P(A_1), P(A_2), \dots, P(A_n)$.

Итак, у нас есть числа $P(A_1), P(A_2), \dots, P(A_n)$ и $\rho(A, A_1), \rho(A, A_2), \dots, \rho(A, A_n)$. Как их собрать в одну формулу, которая позволит адекватным образом вычислять значения признака $P$ для объекта $A$?

Пример:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 3 | 4 | 5 | 3 | 4
$A_2$ | 5 | 5 | 5 | 4 | 3
$A_3$ | 4 | 3 | 3 | 2 | 5
$A$  | 5 | 4 | 3 | 3 | **?**

Если пропуск заменить на среднее или медиану по столбцу (здесь они равны друг другу), то нужно писать 4. Попытаемся заполнить пропуск с помощью различных метрик.

Сперва удаляем столбец $P$ из таблицы и находим расстояние от объекта $A$ до остальных объектов из таблицы.

Вид метрики | От $A$ до $A_1$ | От $A$ до $A_2$ | От $A$ до $A_3$
-- | -- | -- | --
Евклид | 2,83 | 2,45 | 1,73
Манхеттен | 4 | 4 | 3
Макс | 2 | 2 | 1

Теперь вопрос: как скомпоновать расстояния от объекта $A$ до всех других объектов таблицы и значения признака $P$ у остальных объектов таблицы? Метрика с точки зрения математики выражает степень близости объектов друг к другу, причем зависимость обратная — чем больше расстояние, тем меньше мера близости, и наоборот — чем меньше расстояние, тем больше мера близости между объектами.

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **?**

Для ответа на этот вопрос нужно взять комбинацию значений признака $P$ у тех объектов, для которых этот признак известен, и меры близости этих объектов к объекту $A$. **Ключевая идея**: признак $P$ для объекта $A$ должен быть близок к значению признака $P$ у близких к $A$ объектов. Линейная комбинация со значениями признака $P$ по евклидовой метрике будет выглядеть следующим образом:

$$ \cfrac{1}{\cfrac{1}{2.83}+\cfrac{1}{2.45}+\cfrac{1}{1.73}}\left( \frac{4}{2.83} + \frac{3}{2.45} + \frac{5}{1.73} \right) = 4.15 $$

Значения признака $P$ здесь у объекта домножается на меру близости (делится на значение метрики, поскольку мера близости по своему смыслу обратно пропорциональна значению метрики, поэтому если метрика очень большая, то мера близости должна быть очень маленькая) и нормирующий множитель (дробь перед скобками). Таким образом получается оценка на значение признака $P$ для объекта $A$ — восстановленное значение признака.

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **4.15**

Естественно, восстанавливаемое значение признака объекта $A$ существенно зависит от выбранной метрики.

Например, матрикс-метрика даёт:

$$ \cfrac{1}{\cfrac{1}{2}+\cfrac{1}{2}+\cfrac{1}{1}}\left( \frac{4}{2} + \frac{3}{2} + \frac{5}{1} \right) = 4.25 $$

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **4.25**

- По Евклиду = 4,15
- По Макс = 4,25
- По Манхеттену = 4,1

Формула = 1/(1/AA1 + 1/AA2 + 1/AA3) * (значA1/AA1 + значA2/AA2 + значA3/AA3)

Расстояния AA1,AA2,AA3 берутся разные в зависимости от метода. ЗначA1, ЗначA2, ЗначA3 берутся из столбца P.

Поэтому выбор оптимальной метрики для решения задачи является делом непростым.

Таким образом, формула для восстановления данных с помощью метрики выглядит следующим образом:

$$ P(A) = \cfrac{1}{\sum_{i=1}^n \cfrac{1}{\rho(A, A_i)}} \left( \sum_{j=1}^n{\cfrac{P(A_j)}{\rho(A, A_j)}} \right) $$

Это формула получения оценки признака $P$ для объекта $A$, зная значения этого признака у других объектов таблицы и зная расстояния от объекта $A$ до других объектов в таблице.

> Что происходит, если у нас встречается объект, совершенно такой же, как и восстанавливаемый (то есть метрики будут равны нулю)?
>
> Это исключительная ситуация, поскольку расстояние между восстанавливаемым объектом и его клоном будет равна 0 (и во многих формулах произойдет деление на 0). Так что способ тут простой: берем значение из клона и пишем его в пропущенную ячейку. Или в знаменатель к метрике можно добавить +1, тогда деления на 0 не будет. Ещё можно придумать такой вариант обращения метрики: max(pi)-pj.

#### Штрафование ошибок
Обозначения L1, L2, L∞ происходят из семейства Lp-норм (Minkowski norms), где метрика расстояния между точками $x$, $y$ в $n$-мерном пространстве задается как:

$$ d_p(x, y) = \left( \sum_{i=1}^n{|x_i-y_i|^p} \right)^{1/p} $$

где $p$ — параметр нормы.

Объяснение обозначений метрик:

- L1 (манхэттенская): $p=1, d_1 = \sum{|x_i-y_i|}$, сумма абсолютных разностей; устойчива к выбросам, как путь по осям в городе.

- L2 (евклидова): $p=2, d_2 = \sqrt{\sum{(x_i-y_i)^2}}$, корень из суммы квадратов; классическое евклидово расстояние, гладкая оптимизация.

- L∞ (Чебышёва, max-метрика): $p→∞, d_∞ = \max_i{|x_i-y_i|}$, предел прироста $p$ — максимальная абсолютная разность по координатам.

С ростом $p$ вклад доминирующих (больших) разностей усиливается: L1 суммирует равномерно, L2 квадратирует (штрафует средние ошибки), L∞ фокусируется только на худшей координате. В пределе $p=∞$ сумма сводится к максимуму, а для $p=0$/дробных — дискретные нормы (редко в импутации). В ML L1/L2 популярны из-за выпуклости, L∞ — для консервативных оценок.

<dfn title="штрафование ошибок">Штрафование ошибок</dfn> (error penalization) в метриках расстояния — это механизм, при котором разные Lp-нормы по-разному усиливают вклад отдельных разностей $|x_i-y_i|$ между координатами точек, делая акцент на малых, средних или крупных отклонениях для более точного измерения "расстояния" в KNN-импутации или кластеризации.

Механизм штрафования в Lp-нормах: в формуле $d_p(x, y) = \left( \sum_{i=1}^n{|x_i-y_i|^p} \right)^{1/p}$ возведение в степень $p$ "штрафует" разности:

- L1 ($p=1$): Линейное штрафование $∑∣diff∣$, равномерно суммирует все ошибки без усиления; устойчиво к выбросам (одна большая разность не доминирует).​

- L2 ($p=2$): Квадратичное штрафование $\sqrt{∑diff^2}$, средние ошибки растут быстрее (2x ошибка → 4x вклад), крупные — еще сильнее; оптимально для гауссовых данных.​

- L∞ ($p=∞$): Максимальное штрафование $max∣diff∣$, игнорирует мелкие ошибки, фокусируется только на худшей координате (консервативно).

Пример влияния для разностей:​

- L1: 1+3+10=14 (выброс влияет линейно)

- L2: √(1+9+100)≈10.5 (выброс доминирует: 10→100)

- L∞: 10 (только худшая ошибка)​

Это определяет выбор: L1 для шумных данных, L2 для гладких, L∞ для "худшего случая".

#### Выбор метрики
Выбор метрики расстояния (L1, L2, L∞) для задач вроде KNN-импутации или кластеризации зависит от свойств данных, устойчивости к выбросам, размерности пространства и предполагаемой геометрии (линейная/квадратичная/максимальная).

*Основные критерии выбора*

Критерий | L1 (Манхэттен) | L2 (Евклидова) | L∞ (Чебышёва)
-- | -- | --
Устойчивость к выбросам | Высокая (линейная сумма) | Средняя (квадраты усиливают) ​| Низкая (игнорирует мелкие) ​
Размерность данных | >50 фич (не доминирует норма) | Любая (нормализация критична) | Низкая (<10 фич) ​
Тип распределения | Скошенное/шумное | Нормальное/гауссово | Равномерное/шахматное ​
Геометрия | "Городские блоки" (grid-like) | Круглые кластеры | Квадратные кластеры

**Практические рекомендации**
- **Данные с выбросами**: L1 — сумма |diff| не штрафует (*penalizes*) экстремальные значения сильно.​

- **Высокая размерность**: L1/L∞ — "проклятие размерности" (*curse of dimensionality*) слабее влияет на L1.​

- **Временные ряды/сенсоры**: L1 для шумных данных, L2 для гладких трендов.​

- **Тестирование**: Сравните MAE/RMSE на CV после KNN с разными метриками; выберите минимальную ошибку импутации.​

Выбор подтверждают экспериментами: L1 часто выигрывает в реальных датасетах (+5-15% точности KNN).

#### Исходный код

```py
import pandas as pd
import numpy as np
from numpy import NaN

# Создадим датафрейм с пропущенным значением признака Р для объекта А
df = pd.DataFrame({'P1':[3,5,4,5], 'P2':[4,5,3,4], 'P3':[5,5,3,3], 'P4':[3,4,2,3], 'P':[4,3,5,NaN]}, index=['A1', 'A2', 'A3', 'A'])

# Посчитаем метрики
dict_metrics = {'A1':[], 'A2':[], 'A3':[]}
for i in df.index[:-1]:
  dict_metrics[i].append(np.power((df.loc['A'][:-1]-df.loc[i][:-1]).pow(2).sum(), 0.5).round(2)) # считаем Евклидово расстояние
  dict_metrics[i].append((df.loc['A'][:-1]-df.loc[i][:-1]).abs().sum()) # считаем Манхэттеновское расстояние
  dict_metrics[i].append((df.loc['A'][:-1]-df.loc[i][:-1]).abs().max()) # считаем max-метрику

metrics = pd.DataFrame(dict_metrics, index=['Euclid', 'Manhatten', 'Max'])

# Считаем варианты значений для каждой метрики
dict_value = {'Euclid':[], 'Manhatten':[], 'Max':[]}
for i in metrics.index:
  norm_mul = (1/((1/metrics.loc[i]).sum())) # нормирующий множитель
  similarity = ((df.loc[:]['P'][:-1]/metrics.loc[i]).sum()) # значение признака * мера близости(=величина, обратно пропорциональная мере расстояния)
  value_P = (norm_mul*similarity).round(2)
  dict_value[i].append(value_P)
  print(f'значение признака P для А по метрике {i}: {value_P}')
```

```py
import pandas as pd
import numpy as np

df = pd.DataFrame(
    data = {
        "Объекты": ["A1", "A2", "A3", "A"],
        "P1": [3, 5, 4, 5],
        "P2": [4, 5, 3, 4],
        "P3": [5, 5, 3, 3],
        "P4": [3, 4, 2, 3],
        "P":  [4, 3, 5, None]
    }
)
df.set_index("Объекты", inplace=True)
display(df)


# Euclidean metric
metrics = pd.DataFrame(data={"euclidean": np.sqrt(np.sum((df.loc['A'] - df.loc['A1':'A3'])**2, axis=1))})
# manhattan metric
metrics['manhattan'] = np.sum(np.abs(df.loc['A'] - df.loc['A1':'A3']), axis=1)
# max
metrics['max'] = np.max(
    np.abs(df.loc['A'] - df.loc['A1': 'A3']),
    axis=1
)
display('метрики', metrics)

# нормирующий множитель
coeff = 1 / np.sum(1 / metrics, axis=0)
display('нормирующий множитель', coeff)


# меры близости для метрик
p1 = 1 / metrics    # расчет мер близости
display('меры близости', p1)
weights = p1.T @ df.loc['A1':'A3', 'P'] #умножение мер на значения известных призоков P и суммируем
display(weights)
unknown_feature = weights * coeff
display(unknown_feature)
```

работать в ide не будет, если не добавить вначале строку
```py
from IPython.display import display
```

Более общий код. Он заменяет все возможные пропуски nan в любой таблице.

```py
def recover_by_metric(dataFrame, metric):
  clear_data = dataFrame[~dataFrame.isna().any(axis=1)]
  nan_data = dataFrame[dataFrame.isna().any(axis=1)]
  recovered_data = np.zeros(nan_data.shape)
  for i, nan_row in enumerate(nan_data.values):
    clear_indices = np.where(~np.isnan(nan_row))[0]
    metrics = np.array([metric(obj[clear_indices], nan_row[clear_indices]) for obj in clear_data.values])
    args = np.argsort(metrics)
    if metrics[args[0]] == 0:
      recovered_data[i, :] = clear_data.iloc[args[:np.sum(metrics == 0)], :].values.mean(axis=0)
    else:
      recovered_data[i, :] = ((1 / metrics) @ clear_data.values) / np.sum(1 / metrics)
      recovered_data[i, clear_indices] = nan_row[clear_indices]

  return pd.DataFrame(np.concatenate([clear_data.values, recovered_data], axis=0), columns=dataFrame.columns, index=dataFrame.index)
```

Либо такой вариант, здесь уже масштаб признаков не имеет значения.
```py
from sklearn.preprocessing import StandardScaler

def recover_by_metric(data, metric):
  data_scaled = StandardScaler().fit_transform(data.values)
  clear_data = data_scaled[~np.isnan(data_scaled).any(axis=1)]
  clear_data_origin = data[~data.isna().any(axis=1)]
  nan_data = data_scaled[np.isnan(data_scaled).any(axis=1)]
  nan_data_origin = data[data.isna().any(axis=1)]
  recovered_data = np.zeros(nan_data.shape)
  for i, nan_row in enumerate(nan_data):
    clear_indices = np.where(~np.isnan(nan_row))[0]
    metrics = np.array([metric(obj[clear_indices], nan_row[clear_indices]) for obj in clear_data])
    args = np.argsort(metrics)
    if metrics[args[0]] == 0:
      recovered_data[i, :] = clear_data_origin.iloc[args[:np.sum(metrics == 0)], :].values.mean(axis=0)
    else:
      recovered_data[i, :] = ((1 / metrics) @ clear_data_origin.values) / np.sum(1 / metrics)
      recovered_data[i, clear_indices] = nan_data_origin.iloc[i, clear_indices]

  return pd.DataFrame(np.concatenate([clear_data_origin.values, recovered_data], axis=0), columns=data.columns, index=data.index)
```

**Вопрос**: В функции `recover_by_metric(data, metric)` в `metric` что необходимо вводить?

**Ответ**: функцию, измеряющую какую-нибудь метрику. Например, из библиотеки sklearn можно подгрузить функции
```py
from sklearn.metrics import mean_squared_error

recover_by_metric(dataFrame, mean_squared_error)
```

Либо свою руками функцию можете написать
```py
def my_MSE(y_true, y_predicted):
    return ((y_true - y_predicted) ** 2).mean()


recover_by_metric(dataFrame, my_MSE)
```

Для дальнейших задач пригодится.

*Пример c тремя признаками-координатами*:
```py
# Координаты векторов P1 и P2
x1, y1, z1 = 0, 1, 2
x2, y2, z2 = 2, 1, 0

# Вычисление Евклидова расстояния
euclidean_distance = ((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)**0.5
print('Евклидово расстояние между векторами Р1 и Р2:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = abs(x2 - x1) + abs(y2 - y1) + abs(z2 - z1)
print('Расстояние Манхэттен между векторами Р1 и Р2:', manhattan_distance)

# Вычисление расстояния в max-метрике
max_metric_distance = max(abs(x2 - x1), abs(y2 - y1), abs(z2 - z1))
print('Расстояние в max-метрике между векторами Р1 и Р2:', max_metric_distance)
```

*Более общее объяснение (формулы с n кол-вом признаков)*:
```py
# Векторы признаков объектов a и b
a = (0, 1, 2, 3, 4)
b = (4, 3, 2, 1, 0)

# Количество признаков
n = len(a)

# Вычисление Евклидова расстояния
euclidean_distance = sum((b[i] - a[i])**2 for i in range(n))**0.5
print('Евклидово расстояние между объектами a и b:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = sum(abs(b[i] - a[i]) for i in range(n))
print('Расстояние Манхэттен между объектами a и b:', manhattan_distance)

# Вычисление расстояния в max-метрике
max_metric_distance = max(abs(b[i] - a[i]) for i in range(n))
print('Расстояние в max-метрике между объектами a и b:', max_metric_distance)
```

Решение с использованием библиотеки scipy (в терминале `pip install scipy`):
```py
from scipy.spatial.distance import euclidean, cityblock, chebyshev

# Векторы признаков объектов a и b
a = (0, 1, 2, 3, 4)
b = (4, 3, 2, 1, 0)

# Вычисление Евклидова расстояния
euclidean_distance = euclidean(a, b)
print('Евклидово расстояние между объектами a и b:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = cityblock(a, b)
print('Расстояние Манхэттен между объектами a и b:', manhattan_distance)

# Вычисление расстояния в max-метрике (метрика Чебышева)
max_metric_distance = chebyshev(a, b)
print('Расстояние в max-метрике между объектами a и b:', max_metric_distance)
```

```py
import numpy as np

class VectorOperations:

    def euclidean_distance(self, vector_a, vector_b):

        """Вычисляет Евклидово расстояние между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        return np.linalg.norm(vector_a - vector_b)



    def manhattan_distance(self, vector_a, vector_b, normalize=True):

        """Вычисляет расстояние Манхэттена (L1-метрика) между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        if normalize:

            vector_a = self._normalize(vector_a)

            vector_b = self._normalize(vector_b)

        return np.sum(np.abs(vector_a - vector_b))



    def max_metric_distance(self, vector_a, vector_b, normalize=True):

        """Вычисляет расстояние Чебышева (L∞-метрика) между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        if normalize:

            vector_a = self._normalize(vector_a)

            vector_b = self._normalize(vector_b)

        return np.max(np.abs(vector_a - vector_b))



    def _normalize(self, vector):

        """Нормирует вектор по L2-норме."""

        norm = np.linalg.norm(vector)

        if norm == 0:

            return vector  # Избегаем деления на ноль

        return vector / norm



    def _check_vectors(self, vector_a, vector_b):

        """Вспомогательная функция для проверки векторов."""

        if not isinstance(vector_a, np.ndarray) or not isinstance(vector_b, np.ndarray):

            raise TypeError("Векторы должны быть NumPy массивами.")

        if vector_a.shape != vector_b.shape:

            raise ValueError("Векторы должны иметь одинаковую размерность.")

        if not np.issubdtype(vector_a.dtype, np.number) or not np.issubdtype(vector_b.dtype, np.number):

            raise ValueError("Векторы должны содержать только числовые значения.")  



#Пример

vector_operations = VectorOperations()

a = np.array([5, 4,3, 3])

b = np.array([3, 4, 5, 3])



manhattan_normalized = vector_operations.manhattan_distance(a, b)

manhattan_unnormalized = vector_operations.manhattan_distance(a, b, normalize=False)



max_metric_distance_normalized = vector_operations.max_metric_distance(a, b)

max_metric_distance = vector_operations.max_metric_distance(a, b, normalize=False)



print(f"Расстояние Манхэттена (с нормированием): {manhattan_normalized}")

print(f"Расстояние Манхэттена (без нормирования): {manhattan_unnormalized}")



print(f"расстояние Чебышева (с нормированием): {max_metric_distance_normalized}")

print(f"расстояние Чебышева (без нормирования): {max_metric_distance}")
```

### Нормирование (нормализация) признаков

#### Замечание об использовании метрики
Обсудим тонкости, которые возникают при использовании метрики в задачах анализа данных. К сожаление, в данном вопросе не все так однозначно. Работа с метрикой требует аккуратности и большой осторожности. Это относится не только к проблеме восстановления данных, но и к любым другим алгоритмам, использующих понятие метрики.

При вычислении метрики исследуемые объекты представляются в виде точек в некотором пространстве (пространстве признаков), между которыми вычисляется расстояние (метрика). Например, объекты с двумя признаками можно представить в виде точек на плоскости:

![Feature space](../img/feat-space.png)

При данном подходе для адекватной работы алгоритмов анализа данных, использующих метрику, необходимо, чтобы все признаки (значения по осям) имели одинаковый масштаб. Если масштаб признака не одинаков по всем ося, то могут иметь место нежелательные эффекты. Рассмотрим следующую таблицу признаков.

Студент | Вес, кг | Рост, м
-- | -- | --
Иванов | 61 | 1,76
Сидорова | 56 | 1,50
Петров | 100 | 1,98

В данной таблице признаки заведомо имеют различный масштаб. При таком масштабе разница в весе между некоторыми объектами (студентами) может достигать 50 условных единиц, а разница в росте быть ничтожной из-за выбранной единицы измерения. Можно отметить, что в основе всех формул для вычисления метрик положено вычисление разности значений признаков. Таким образом, когда будут вычисляться расстояния между объектами, то для признака веса разница будет почти 50, а для роста она будет незначительной, что приведет к тому, что, например, некоторые признаки фактически будут проигнорированы (в след. примере различия в росте и различия в весе имеют ОЧЕНЬ разную ценность – это из-за разных единиц измерения). Так, в данном примере какая угодно разница в росте по сути не будет иметь никакого значения из-за большой разницы в весе. Это произошло из-за того, что мы имеем два признака с совершенно разными масштабами, что приведет к неадекватной работе нашего алгоритма.

Что нужно сделать, чтобы запустить корректно алгоритм, использующий метрики? При вычислении метрики все признаки необходимо приводить к единой шкале (нормировать).

#### Способы нормирования признака
Входные признаки в большинстве случаев будут иметь разный масштаб (диапазон изменения значений признака): одни признаки могут изменяться в диапазоне $[−0.01,0.01]$, другие — в $[0,100]$ и т.д. Как мы впоследствии увидим из описаний методов машинного обучения, для большинства из них масштаб признаков будет оказывать влияние на прогноз. Для таких моделей чем выше разброс значений признака, тем сильнее он будет влиять на прогноз, перекрывая влияние признаков меньшего масштабаз, поэтому, чтобы влияние всех признаков было одинаковым, их необходимо нормализовать, то есть привести к одному масштабу.[^Feature-normalization]

Допустим, имеется некоторый признак $P = (p_1, p_2, \dots, p_n)$.

- $\bar{p}$ — среднее значение;
- $s$ — отклонение (среднее квадратическое).

Наиболее распространены следующие способы
- cтандартизация (Z-стандартизация) (*standard scaler, standardization, Z-score normalization*);
- диапазонное шкалирование (*min-max scaling*);
- нормализация средним (*centered min-max scaling*);
- нормализация максимумом по модулю (*max abs scaler*);
- робастное шкалирование (*robust scaler*)

##### Стандартизация
<dfn title="стандартизация">Стандартизация</dfn> или <dfn title="z-стандартизация">Z-стандартизация</dfn> (*standard scaler, standardization, Z-score normalization*) — метод масштабирования, при котором из каждого значения вычитается среднее $μ$ по столбцу, а результат делится на стандартное отклонение $σ$:

$$ p'_i = \frac{p_i - \bar{p}}{s} $$

Полученные числа будут являться нормированными значениями признака $P$. Данное преобразование гарантирует, что после его выполнения у признака $P$ среднее значение и отклонение будут равны 0 и 1 соответственно.

$$ x'^j = \frac{x^j - \mu_j}{\sigma_j} $$

**Выходные свойства**: нулевое среднее и единичная дисперсия (ско).

**Отличительные особенности**:
- устойчив к выбросам;
- сохраняет форму распределения

**Условия использования**
- идеален для алгоритмов с евклидовой метрикой (KNN, SVM, градиентный спуск), где равный вклад фич критичен;
- подходит для предположительно нормальных данных или временных рядов (центрирование тренда).

*Реализация в Python*
```python
from sklearn.preprocessing import StandardScaler
import pandas as pd

scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(df_scaled.describe())  # mean≈0, std≈1 по столбцам
```

##### Диапазонное шкалирование (min-max scaling)
<dfn title="диапазонное шкалирование">Диапазонное шкалирование</dfn> (*min-max scaling*) — метод нормализации, который преобразует значения признаков так, чтобы они попадали в заданный диапазон, чаще всего от 0 до 1, сохраняя при этом относительные расстояния между точками и свойство «разреженных» данных, где большинство значений нули, что важно для эффективности хранения и обработки. Это противоположность стандартизации, и используется для перевода исходных значений ($x$) в новые ($x'$) по формуле:

$$ p'_i = \frac{p_i - p_{\min}}{p_{\max}-p_{\min}} $$

При этом минимальное значение признака $p$ гарантированно перейдет в 0, максимальное — в 1, а все промежуточные разместятся между ними.

$$ x'^j = \frac{x^j - \min(x^j)}{\max(x^j)-\min(x^j)} $$

**Выходные свойства**: принадлежит интервалу $[0,1]$.

**Отличительные особенности**:
- сохраняет нулевые значения (для sparse data);
- чувствителен к выбросам.

**Условия применения**:
- подходит для алгоритмов, чувствительных к масштабу, например, нейронные сети, метод k-ближайших соседей, обработки изображений/пикселей (значения 0-255 → 0-1) и когда известны теоретические границы признака;
- идеально подходит для работы с разреженными (sparse) данными, где много нулей, так как нули остаются нулями, а ненулевые значения масштабируются в $[0,1]$;
- не подходит для данных с выбросами (>1% экстремальных значений) — один outlier растягивает весь диапазон.​

*Реализация в Python (sklearn)*
```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Базовое [0,1]
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Кастомный диапазон [-1,1]
scaler_custom = MinMaxScaler(feature_range=(-1, 1))
df_custom = scaler_custom.fit_transform(df)

print(df_scaled.describe())  # min=0, max=1 по столбцам
```

##### Нормализация средним (centered min-max scaling)
Центрированная Min-Max нормализация, сочетающая вычитание среднего $μ_j$ по признаку $j$ с масштабированием по диапазону $max(x^j)−min(x^j)$, приводя данные к диапазону примерно [-0.5, 0.5] с центром в 0.

$$ x'^j = \frac{x^j - \mu_j}{\max(x^j)-\min(x^j)} $$

**Выходные свойства**: нулевое среднее, с единичным диапазоном ($[-0.5, 0.5]$ для симметричных данных).

**Отличительные особенности**:
- сохраняет относительные расстояния в пределах диапазона, как Min-Max, но центрирована для градиентного спуска.

**Условия использования**:
- алгоритмы с расстояниями (KNN), где нужен нулевой центр + ограниченный диапазон;
- градиентный спуск (нейросети), где симметрия [-0.5,0.5] ускоряет сходимость;
- данные с известными границами без сильных выбросов.

##### Нормализация максимумом по модулю
<dfn title="нормализация максимумом по модулю">Нормализация максимумом по модулю</dfn> (*MaxAbsScaler*) масштабирует каждый признак так, чтобы его значения находились в диапазоне [-1, 1] путём деления каждого значения на максимальное по абсолютной величине значение этого признака.

$$ x′ = \frac{x}{\max{(∣x∣)}} $$

**Выходные свойства**: диапазон [-1,1].

**Отличительные особенности**:
- не сдвигает данные (среднее и медиана остаются на месте), в отличие от MinMaxScaler и StandardScaler;
-  устойчив к выбросам лучше, чем MinMaxScaler, так как не зависит от минимального значения.

**Условия использования**:
- особенно полезен для разреженных данных (sparse data), так как не делает значения отрицательными и сохраняет нулевые значения;
- хорошо подходит для алгоритмов, чувствительных к масштабу, например, KNN, SVM, методы с евклидовой метрикой.

*Реализация в Python (sklearn)*
```python
from sklearn.preprocessing import MaxAbsScaler
import pandas as pd

scaler = MaxAbsScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(df_scaled.describe())  # все признаки в [-1,1]
```

##### Робастное шкалирование (Robust scaler)
<dfn title="робастное шкалирование">Робастное шкалирование</dfn> (Robust scaler) масштабирует признаки, используя медиану и межквартильный размах ($IQR$ — разница между 75-м и 25-м процентилями (квартилями), то есть $Q3 - Q1$), а не среднее значение и стандартное отклонение, что делает его устойчивым к выбросам (аномальным значениям — *outliers*). Он вычитает медиану и делит на $IQR$, сохраняя при этом форму распределения данных и эффективно нейтрализуя влияние экстремальных значений, что предпочтительнее в случаях, когда данные содержат выбросы, влияющие на StandardScaler.

$$ x′ = \frac{x- m_e}{IQR} = \frac{x- m_e}{Q_3(x) - Q_1(x)} $$

<dfn title="робастность">Робастность</dfn> — это свойство системы, модели или метода оставаться устойчивым и сохранять качество работы при наличии помех, ошибок, аномалий или отклонений во входных данных или параметрах. Это "крепкость" или "устойчивость" к нежелательным изменениям, что позволяет получать надёжные и воспроизводимые результаты, не теряя функциональности, например, в статистике, управлении и машинном обучении.

<dfn title="робастность в статистике">Робастность в статистике</dfn> — нечувствительность к выбросам (аномальным значениям) и нарушениям базовых предположений (например, нормального распределения). Робастные оценки, такие как медиана, лучше справляются с "зашумлёнными" данными, чем среднее значение.

<dfn title="робастность в алгоритмах и моделях">Робастность в алгоритмах и моделях</dfn> (ИИ) — способность модели корректно обрабатывать неидеальные или некорректные данные, не давая сбоев, а также сохранять качество предсказаний на новых, немного отличающихся данных.

Например, если статистический метод, основанный на линейной регрессии, дает стабильные и осмысленные результаты даже при наличии нескольких сильно отличающихся точек (выбросов) в данных, он считается робастным. 

**Выходные свойства**: максимальные и минимальные значения каждого признака после трансформации будут $≤1$ и $≥−1$ соответственно.

**Отличительные особенности**:

- **Устойчивость к выбросам**: Медиана и IQR менее чувствительны к экстремальным значениям, чем среднее и стандартное отклонение.
- **Сохранение формы распределения**: Не меняет форму исходных данных так сильно, как другие методы.

**Условия использования**:

- для скошенных распределений;
- когда в данных есть выбросы (экстремальные значения);
- когда важно сохранить относительные расстояния между значениями без сильного искажения;
- данные с выбросами + расстояния (KNN с L1/L2), градиентный спуск, PCA.

*Пример использования (в Python с scikit-learn)*
```python
from sklearn.preprocessing import RobustScaler
import numpy as np

data = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [100, 200]]) # С выбросом 100, 200
scaler = RobustScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)
```

#### Влияние выбросов

*[MAD]: Median Absolute Deviation
*[RMS]: Root Mean Square

Наличие выбросов (аномально больших или малых значений) искажает результат нормализации, так наличие даже одного выброса может существенно сместить среднее, дисперсию, минимум или максимум. Поэтому важно _предварительно отфильтровать выбросы из выборки_.

Альтернативно можно заменить неустойчивые к выбросам статистики на устойчивые по схеме ниже:

| Неустойчивая статистика | Робастный аналог | Значение (смысл)
-- | -- | --
среднее | медиана | центр распределения
дисперсия/СКО | медианное абсолютное отклонение | разброс распределения
минимальное значение | 1% перцентиль | нижняя граница
максимальное значение | 99% перцентиль | верхняя граница

##### Неустойчивые и робастные статистики
<dfn title="неустойчивая статистика">Неустойчивыми статистиками</dfn> называются меры центральной тенденции и разброса данных, которые сильно изменяются (становятся ненадёжными) при наличии даже небольшого числа выбросов или аномалий в выборке. Они чувствительны к крайним значениям, что делает их непригодными для реальных данных с шумом. <dfn title="робастный аналог">Робастные аналоги</dfn> — это устойчивые к выбросам альтернативы, использующие медиану или перцентили вместо средних.

**Центр распределения**

- **Неустойчивая статистика**: выборочное среднее (mean) — среднее арифметическое значений выборки, $\frac{1}{N} \sum_{n=1}^{N}{x^j}$. Один выброс может сдвинуть его значительно.

- **Робастный аналог**: Медиана $\operatorname{median}\{x^j\}$ (*median*) — величина, альтернативная выборочному среднему (*mean*) для оценки центра распределения. Для выборки значений признака медиана — это такое значение, что половина наблюдений оказываются меньше, а другая половина — больше этого значения. Для вероятностного распределения медиана — это такое значение, что половина вероятностной массы лежит слева, а другая половина — справа от значения медианы. Медиана игнорирует экстремальные значения и отражает типичное положение данных.

<figure style="background: white">

![Median](../img/median-02763de49c26966c6f76060826a0675e.png)

</figure>

**Разброс распределения**

- **Неустойчивые статистики**: Дисперсия $\frac{1}{N} \sum_{n=1}^{N}{(x^j_n)^2}$. Выбросы завышают её экспоненциально, поскольку дисперсия сильно завышается даже одним выбросом из-за квадратичных отклонений. Выбросы в данных (например, значение в 10 раз больше типичного) доминируют в сумме квадратов, делая метрику ненадёжной для реальных выборок с шумом. Это особенно актуально при восстановлении маскированных данных. Среднее квадратическое отклонение для центрированных данных $\sqrt{\frac{1}{N} \sum_{n=1}^{N}{(x^j_n)^2}}$ (RMS — *Root Mean Square*) или  или стандартное отклонение для данных с ненулевым средним отражает типичный разброс данных вокруг нуля (для RMS) или среднего (для стандартного отклонения), учитывая квадраты отклонений. Эта величина чувствительна к выбросам из-за квадратичной природы, делая её неустойчивой статистикой разброса.

- **Робастный аналог**: <dfn title="медианное абсолютное отклонение">Медианное абсолютное отклонение</dfn> (*Median Absolute Deviation*, MAD) $\operatorname{median}\{|x^j - \operatorname{median}\{x^j\}|\}$  — робастная мера разброса данных, фокусирующаяся на типичном разбросе большинства значений, игнорируя экстремальные. MAD вычисляет медиану абсолютных отклонений всех значений $x^j$ от медианы выборки. Это типичный уровень разброса большинства наблюдений, устойчивый к выбросам, в отличие от дисперсии, и интерпретируемый в единицах исходных данных. Один выброс не искажает результат, так как медиана игнорирует экстремальные значения. Идеально для задач восстановления данных с шумом, где простое среднее может подвести.

**Экстремальные значения**

- **Неустойчивые статистики**: минимум $\min{x^j}$ и максимум $\max{x^j}$ — неустойчивые статистики, так как они полностью определяются одним-единственным экстремальным значением и игнорируют информацию о всех остальных наблюдениях. Чувствительны к выбросам: один аномальный элемент (например, ошибка измерения) кардинально меняет min/max, делая их ненадёжными для описания типичного разброса. Недостаточно репрезентативны: не отражают центр или вариативность данных — 99% наблюдений могут быть в узком диапазоне, но один выброс искажает картину. Ненадёжны в малых выборках, где случайный шум легко создаст ложные экстремумы.

- **Робастные аналоги**: 1-й (1%) перцентиль (или 0.01-квантиль) и 99-й (99%) перцентиль (0.99-квантиль). Они используются как робастные аналоги минимума и максимума, фокусируясь на 1% экстремальных значений, отсекая 1% выбросов с каждой стороны. Это стандарт в boxplot'ах (1.5×IQR правило) и анализе рисков.

##### Процентили
<dfn title="p-процентная процентиль">p-процентная процентиль</dfn> — это значение в упорядоченной выборке данных, ниже которого находится p% всех наблюдений. Например, 75-й перцентиль означает, что 75% данных меньше или равно этому значению">p-процентная перцентиль</dfn> — это значение в упорядоченной выборке данных, ниже которого находится p% всех наблюдений. Например, 75-й перцентиль означает, что 75% данных меньше или равно этому значению.

Таким образом, p-процентная перцентиль (percentile) — это такое значение, что

- для выборки наблюдений $p$ процентов наблюдений лежит слева, а $(1−p)$ процентов — справа от значения.
- для вероятностного распределения признак с вероятностью $p/100$ принимает значение меньше, а с вероятностью $(1−p)/100$ — больше значения перцентили.

<dfn title="квантиль">Квантили</dfn> — обобщение перцентилей для деления данных на $k$ равных долей ($q_k = 100/k$ %-й перцентиль). Например:

- Квартили (k=4): Q1, Q2, Q3 (25%, 50%, 75%) — каждые 25%.

- Квинтили (k=5): Q(0.2), Q(0.4), Q(0.6), Q(0.8) (20%, 40%, 60%, 80%) — каждые 20%.

- Октили (k=8): O1–O7 (12.5%, 25%, ..., 87.5%) — каждые 12.5%.​

- Децили (k=10): D1–D9 (10%, 20%, ..., 90%) — каждые 10%.

Робастны к выбросам, как 1% и 99% перцентили из таблицы выше. Как видно, $p$-квантиль соответствует $(100∗p)$-процентной перцентили.[^Feature-normalization]

<dfn title="квартиль">Квартили</dfn> — это специальные перцентили, делящие данные на 4 равные части:

- Q1 (25-й перцентиль): нижняя четверть.

- Q2 (50-й перцентиль): медиана.

- Q3 (75-й перцентиль): верхняя четверть.​

Они используются для построения boxplot и оценки разброса.

<dfn title="квинтили">Квинтили</dfn> — это значения, соответствующие 20-му, 40-му, 60-му и 80-му перцентилям (плюс 0% и 100% как границы). Они обозначаются как Q(0.2), Q(0.4), Q(0.6), Q(0.8) и используются для более детального описания распределения, чем квартили.

**Применение**:
- В финансах: для анализа доходности активов (например, топ-20% портфелей).

- В статистике: дополняют квартили (4 части) и децили (10 частей) для гранулярного разброса.

Робастны к выбросам, как и другие перцентили из таблицы (1% и 99%).

#### Использование нормализации признаков
Каждый вид шкалирования применяется к каждому признаку (столбцу в матрице объекты-признаки X) _независимо_.

Возвращаясь к примеру из предыдущей темы:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 3 | 4 | 5 | 3 | 4
$A_2$ | 5 | 5 | 5 | 4 | 3
$A_3$ | 4 | 3 | 3 | 2 | 5
$A$  | 5 | 4 | 3 | 3 | **?**

По-хорошему, перед использованием метрики для восстановления значения признака $P$ у объекта $A$ необходимо нормировать все признаки кроме $P$:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 0 | 0.5 | 1 | 0.5 | 4
$A_2$ | 1 | 1 | 1 | 1 | 3
$A_3$ | 0.5 | 0 | 0 | 0 | 5
$A$  | 1 | 0.5 | 0 | 0.5 | **?**

Здесь минимальное значение (3) переходит в 0, максимальное (5) — в 1, а значение посередине (3) — соответственно, в среднее между 0 и 1 (0.5). Стоить также помнить о том, что если нужно считать расстояние между объектами (строками), то нормируют не по строкам (объектам), а по колонкам (признакам). В колонке P4: мин  2= 1, макс 4 =1, а 3 = 0.5.

> Почему не нормируется признак с пропущенными значениями? Ведь теперь все расстояния между ним и остальными исказились.

> Подспудно признак с пропусками таки нормируется. Его нормировка "зашита" в формулу восстановления пропущенного значения: там мы делим на сумму расстояний. Значения столбца $P$ остаются теми же, а потом берется их взвешенная комбинация (эти числа домножаются на числа, получаемые из метрики). И таким образом в итоге получается число где-то посередке от исходных значений.

> Если нормировать по второй формуле, используя среднее и отклонение, то для признака А1Р1 нормированное значение меньше 0 получится? А1(3) - среднее АР1(4,25) / 0,96 = -1,25 / 0,96.

> При такой нормировке среднее значение признака получается равным нулю, а остальные значения распределяются вокруг него — то есть, могут быть как отрицательными, так и положительными.

> Маленько "словил" когнитивный диссонанс, пытаясь понять, что делать, когда "отклонение" равно нулю. Но потом понял, что в этом случае можно выражение "0/0" (неопределённость) принять равным либо единице, либо какому нибудь другому числу из интервала, соответствующего используемому методу нормировки. То есть [0; 1] или [-1; 1]..

#### Выбор вида нормализации
Самым популярным методом нормализации признака является **стандартизация признака**. Вторым по популярности является **диапазонное шкалирование**. Оно хорошо тем, что значения признака из отрезка [0,MAX] переводятся в отрезок [0,1], причём ноль переходит в ноль, что полезно для разреженных данных (*sparse data*), в которых большинство значений — нули. Такие данные часто возникают на практике и эффективно кодируются разреженными матрицами (*sparse matrix*[^sparse-matrix]), которые экономично их хранят и производят операции над ними, оперируя только ненулевыми элементами. Диапазонное шкалирование _позволяет сохранить свойство разреженности_.[^Feature-normalization]

##### Нормализация для L1
Для метрики L1 (манхэттенской, сумма абсолютных разностей) лучше всего подходит **RobustScaler** или **MaxAbsScaler**, так как L1 устойчива к выбросам и не чувствительна к квадратичному штрафованию, а эти методы сохраняют свойства абсолютных значений без сильного искажения.

*Рекомендации по скейлерам для L1*

| Скейлер        | Почему подходит для L1                                                       | Недостатки для L1                   |
| -------------- | ---------------------------------------------------------------------------- | ----------------------------------- |
| RobustScaler   | Масштабирует по IQR (игнорирует выбросы), L1 суммирует                       | abs                                 |
| MaxAbsScaler   | Делит на max\|x\|, сохраняет знаки и sparse структуру (нулевые остаются 0) ​ | Чувствителен к экстремальному \|x\| |
| StandardScaler | Центрирует по среднему, L1 работает с                                        | diff                                |
| MinMax         | искажает абсолютные разности при разных диапазонах                           | Высокая чувствительность к выбросам |


L1-метрика $d = \sum{|x_i-y_i|}$ линейно суммирует абсолютные разности по координатам — выбросы в одной фиче не "перевзвешивают" остальные (в отличие от L2).​

**Почему именно RobustScaler для L1**
- RobustScaler использует медиану (устойчивая к выбросам) и IQR (игнорирует 50% экстремальных значений)

- После масштабирования |diff| остаются пропорциональными исходным, без искусственного растяжения диапазона

*Пример сравнения (Python)*
```python
from sklearn.preprocessing import RobustScaler, MaxAbsScaler
from sklearn.metrics.pairwise import manhattan_distances

# Данные с выбросом [1,2,3,100]
X = [[1,2,3], [4,5,6], [100,1,2]]

# RobustScaler сохраняет относительные L1-разности
scaler_robust = RobustScaler().fit_transform(X)
l1_robust = manhattan_distances(scaler_robust)  # Стабильные расстояния

# MinMaxScaler "ломает" из-за выброса 100
scaler_minmax = MinMaxScaler().fit_transform(X)
l1_minmax = manhattan_distances(scaler_minmax)   # Искаженные расстояния
```

**Вывод**: RobustScaler минимизирует искажение L1-расстояний при выбросах (~10-20% лучше по стабильности)

##### Нормализация для L2
Для метрики L2 (евклидова норма) лучше всего подходит Z-стандартизация (**StandardScaler**), при которой каждый признак центрируется по среднему и масштабируется на стандартное отклонение: $x' = \frac{x - \mu}{\sigma}$. Это приводит признаки к нулевому среднему и дисперсии 1, что идеально для L2, поскольку расстояния в пространстве зависят от квадрата отклонени

**Почему Z-стандартизация подходит для L2**
- L2 метрика выражается через сумму квадратов разностей, поэтому масштабирование на стандартное отклонение корректно сопоставляет важность каждой координаты.​

- Центрирование по среднему устраняет смещение, обеспечивая справедливое сравнение по всем признакам.​

- Алгоритмы с L2 штрафом (ridge-регрессия) и градиентный спуск лучше обучаются на стандартизированных данных.​

*Сравнение с другими методами нормализации*

| Метод нормализации | Среднее      | Стандартное отклонение | Диапазон    | Рекомендуется для L2?         |
| ------------------ | ------------ | ---------------------- | ----------- | ----------------------------- |
| Z-стандартизация   | Да           | Да                     | Неограничен | Да                            |
| Min-Max Scaling    | Нет          | Нет                    |             | Нет (чувствителен к выбросам) |
| RobustScaler       | Да (медиана) | Да (IQR)               | Сжатый      | Возможно, при выбросах        |
| MaxAbsScaler       | Нет          | Нет                    | [-1,1]      | Ограничено                    |

*Реализация в Python*
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

Таким образом, для сохранения корректных L2 расстояний и стабильной работы моделей с евклидовой метрикой нормирование Z-score является наиболее подходящим.

##### Нормализация для L∞
Для метрики L∞ (max-метрики, максимума абсолютных разностей) больше всего подходит диапазонное шкалирование (**Min-Max scaling**), поскольку оно приводит все значения признака в диапазон или [-1,1], что напрямую согласуется с расчитыванием максимума в пределах ограниченного интервала и позволяет избежать доминирования одного признака с большим размахом.

**Почему Min-Max нормализация лучше для L∞**
- Линейное масштабирование по диапазону ($\max−\min$) гарантирует, что все значения признака будут лежать в строго ограниченном интервале (обычно от 0 до 1), что формально соответствует определению L∞ метрики, где расстояние определяется максимальной абсолютной разницей по признакам.​

- Если не нормализовать, один признак с большой амплитудой полностью задает расстояние max-метрики, искажается сравнительный вклад остальных.​

- При L∞ важен именно максимум разности, и Min-Max сохраняет соотношение всех значений в пределах единичного интервала.​

**Альтернативные методы и ограничения**
- MaxAbsScaler сжимает данные в диапазон [-1,1] через деление на max абсолютное значение, но не учитывает минимумы, что может неравномерно распределить значения по диапазону.​

- Стандартизация (Z-score) не ограничивает диапазон, что нежелательно для L∞, так как может позволить некоторым значениям выходить далеко за границы, искажая максимумы.​

**Вывод**: для L∞ метрики нормирование по диапазону (Min-Max scaling) обеспечивает адекватное измерение расстояний, предотвращая доминирование максимальных или минимальных по масштабу признаков.

##### Программный код

```py
pip install neulab

from neulab.RestoreValue import MetricRestore

d = {'P1': [3, 5, 4, 5], 'P2': [4, 5, 3, 4], 'P3': [5, 5, 3, 3], 'P4': [3, 4, 2, 3], 'P5': [4, 3, 5, np.NaN]}
df = pd.DataFrame(data=d)

# Euclid
euclid_m = MetricRestore(df, row_start=0, row_end=9, metric='euclid')
# Manhattan
mnht_m = MetricRestore(df, row_start=0, row_end=9, metric='manhattan')
# Max
mx_m = MetricRestore(df, row_start=0, row_end=9, metric='max')

Output:
euclid_m = 4.13
mnht_m = 4.1
mx_m = 4.25
```

### Заключение
- Пропущенные значения можно восстанавливать, используя меру близости объектов друг к другу.
- Мера близости объектов вычисляется с помощью метрики.
- При использовании метрики нужно привести все признаки к одному масштабу (нормировать).
- Существует несколько способов нормирования значений признаков.

### Практическая работа. Восстановление данных с помощью метрики
Реализовать различные варианты восстановления данных в соответствии с данными индивидуального варианта, приводимыми в прилагаемом файле:

- ad-hoc методами (выборочным средним, медианой, модой и т.п.)
- при помощи метрик (L1, L2, L∞);
- с помощью коэффициента корреляции.

Для каждого восстанавливаемого признака обосновать выбор того или иного метода. Сделать выводы об эффективности реализованных методов восстановления для каждого признака на основе методов сравнения результатов восстановления с фактическими значениями отсутствующих признаков (метрик и т.п.).


### Источники информации
[^3-basic-distances-in-data-science]: [Евклидова, L1 и Чебышёва — 3 основные метрики, которые пригодятся в Data Science](https://tproger.ru/translations/3-basic-distances-in-data-science)
[^Feature-normalization]: [Нормализация признаков](https://deepmachinelearning.ru/docs/Machine-learning/Data-preprocessing/Feature-normalization)
[^sparse-matrix]: [Python-school: введение в разреженные матрицы](https://python-school.ru/blog/python/sparse-matrix/)
