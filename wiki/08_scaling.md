## Метрика и нормирование признаков

- [Метрика и нормирование признаков](#метрика-и-нормирование-признаков)
  - [Использование метрики для восстановления данных](#использование-метрики-для-восстановления-данных)
    - [Что такое метрика](#что-такое-метрика)
    - [Свойства метрики](#свойства-метрики)
    - [Евклидово расстояние (расстояние по прямой)](#евклидово-расстояние-расстояние-по-прямой)
    - [Расстояние L1 (расстояние городских кварталов)](#расстояние-l1-расстояние-городских-кварталов)
    - [Расстояние Чебышёва (метрика шахматной доски)](#расстояние-чебышёва-метрика-шахматной-доски)
    - [Использование метрики](#использование-метрики)
    - [Выбор метрики](#выбор-метрики)
    - [Исходный код](#исходный-код)
  - [Нормирование (нормализация) признаков](#нормирование-нормализация-признаков)
    - [Замечание об использовании метрики](#замечание-об-использовании-метрики)
    - [Способы нормирования признака](#способы-нормирования-признака)
      - [Стандартизация](#стандартизация)
      - [Диапазонное шкалирование (min-max scaling)](#диапазонное-шкалирование-min-max-scaling)
      - [Нормализация средним (centered min-max scaling)](#нормализация-средним-centered-min-max-scaling)
      - [Нормализация максимумом по модулю (MaxAbsScaler)](#нормализация-максимумом-по-модулю-maxabsscaler)
      - [Робастное шкалирование (Robust scaler)](#робастное-шкалирование-robust-scaler)
    - [Использование нормализации признаков](#использование-нормализации-признаков)
      - [Программный код](#программный-код)
  - [Заключение](#заключение)
  - [Источники информации](#источники-информации)


### Использование метрики для восстановления данных
Как восстановить данные, используя меру близости объектов друг к другу? Метрика является математическим аналогом понятия близости объектов друг другу. Это пригодится для решения проблемы восстановления пропущенных данных. Допустим, у какого-то объекта (строки таблицы) пропущено какое-либо значение признака (пустая ячейка таблицы). Зная, как вычислить меру близости объектов друг другу, можно для требуемого объекта найти наиболее близкий к нему по свойствам другой объект, у которого значение в соответствующем столбце известно, и с помощью значения из ячейки близкого объекта (ближайшего соседа) восстановить значение ячейки искомого объекта. Для этого и нужно знать, как вычислить меру близости друг другу.

#### Что такое метрика
Что такое метрика? Это обобщение понятия расстояние из геометрии — например, знакомого всем из школы расстояния между двумя точками. В отличие от расстояния метрика может быть вычислена не только для пары точек, но также для объектов произвольной природы, при этом она не обязана вычисляться по известной формуле из школьного учебника геометрии: для точек $A(x_1, y_1)$ и $B(x_2, y_2)$ на плоскости расстояние $d = \sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}$. Существует несколько формул для вычисления метрики.

Допустим, даны два набора чисел:

$$ P = (p_1, p_2, \dots, p_n) $$

$$ Q = (q_1, q_2, \dots, q_n) $$

Как найти расстояние между $P$ и $Q$, т.е. вычислить значение метрики на паре $P$ и $Q$?

1. **Евклидова метрика** (L2) (как в школьном учебнике геометрии): чувствительна ко всем измерениям, подходит для нормализованных данных с равновзвешенными признаками; в sklearn: `metric='euclidean'`.

    $$ \rho(P, Q) = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2 + \dots + (p_n-q_n)^2} $$

2. **Метрика Манхеттен** (L1): устойчива к выбросам, лучше для высокомерных данных или городских сетей (grid-like); `metric='manhattan'`.

    $$ \rho(P, Q) = |p_1-q_1| + |p_2-q_2| + \dots + |p_n-q_n| $$

    При чём тут Манхеттен? Если нужно обойти квартал, то нужно идти по его сторонам, так как нельзя пройти сквозь здание.

    ![Manhattan](../img/manhattan.png)

     Зеленый отрезок на рисунке выше, выражает прямолинейный путь — кратчайшее расстояние между двумя точками. Длина этого отрезка вычисляется как евклидова метрика. В реальности же приходится идти не по прямой, а огибая кварталы. Соответственно, путь, который пешеход пройдет внутри жилого квартала, двигаясь от одной точки до другой, будет выражен синей линией, длина которой вычисляется по метрике Манхеттен. Именно отсюда метрика и заимствует свое название.

3. **Метрика Чебышёва** или **max-метрика** (L∞): фокусируется на максимальном расхождении, идеальна при доминирующем признаке или шахматных паттернах; `metric='chebyshev'`.

    $$ \rho(P, Q) = \max\{|p_1-q_1|, |p_2-q_2|, \dots, |p_n-q_n|\} $$

4. **Другие способы**: помимо этого к признакам можно предварительно применять разные функции, выполняющие некоторые промежуточные преобразования (например, $log$). Так, логарифмирование оправдано, когда значение признака $P$ сильно разрежено и значения отличаются друг от друга на порядок: $\{1, 10, 100\}$. После применения ко всем признакам логарифма по основанию 10 получается более компактный числовой ряд: $\{0, 1, 2\}$

Приведенный список не исчерпывает все возможные функции, которые могут быть приняты за метрику. Естественно, не любая функция подойдет в качестве метрики. Она должна удовлетворять некоторым свойствам.

#### Свойства метрики

1. Значение метрики на одинаковых объектов должно быть равно 0:

    $$\rho(P, P) = 0 $$

    Это означает, что расстояние от объекта до него же самого должно быть равно 0 (что, в принципе, логично).

2. Для метрики должен соблюдаться аналог симметричности:

    $$ \rho(P, Q) = \rho(Q, P) $$

    То есть расстояние от первого объекта до второго должно быть равным таковому от второго до первого. Это также разумно. Однако если такие ситуации в жизни, когда расстояние от первой точки до второй не равно таковому от второй точки до первой? Примером такой ситуации могут быть участки городской транспортной сети с односторонним движением.

3. Неравенство треугольника:

    $$ \rho(P, Q) \leq \rho(P, T) + \rho(T, Q) $$

    ![Triangle inequality](./img/triangle-inequality.png)

    Это свойство часто используется в анализе данных. Суть его заключается в том, что если имеются три объекта (точки в пространстве или на плоскости), то расстояние от точки $P$ до точки $Q$ должно быть меньше суммы расстояний через любую промежуточную точку $T$, то есть путь $PQ$ должен быть меньше $PT + TQ$. В качестве примера из жизни, когда такое неравенство не выполняется, можно привести транспортную сеть, где $P$, $T$ и $Q$ — это населенные пункты, причем дорога от $P$ до $Q$ очень извилистая и неудобная, а отрезки $PT$ и $TQ$ — прямые и максимально скоростные. Для такой тройки населенных пунктов неравенство треугольников выполняться не будет.

#### Евклидово расстояние (расстояние по прямой)
Евклидово расстояние самое интуитивное для понимания: именно Евклидову метрику мы представляем, когда кто-то просит нас измерить расстояние между точками.

$$ d = \sqrt{\sum{(x_i-y_i)^2}} $$

<dfn title="евклидово расстояние">Евклидово расстояние</dfn> — это прямая линия между двумя точками с координатами X и Y. Например, одной из таких точек может быть город на карте с его координатами долготы и широты.

Евклидово расстояние характеризуется прямой линией. Допустим, вам нужно измерить расстояние по прямой между точками A и B на карте города, приведённой ниже.

![L2](../img/EuclidMap.webp)

Для расчёта Евклидового расстояния вам понадобятся лишь координаты этих двух точек. Дистанцию между ними можно будет рассчитать по формуле Пифагора.

Теорема Пифагора гласит, что можно рассчитать длину «диагональной стороны» (гипотенузы) прямого треугольника, зная длины его горизонтальной и вертикальной стороны (катетов). Формула выглядит так: a² + b² = c².

![L2 расчет](../img/EuclidFormul.webp)

Прим. ред. В четвёртой строке вычислений допущена ошибка: (-260)^2 = 67 600, а не 76 600. Тогда результат будет равен ~321.[^3-basic-distances-in-data-science]

#### Расстояние L1 (расстояние городских кварталов)
Расстояние L1 также известно как расстояние городских кварталов, манхэттенское расстояние, расстояние такси, метрика прямоугольного города — оно измеряет дистанцию не по кратчайшей прямой, а по блокам. Расстояние L1 измеряет дистанцию между городскими блоками: это расстояние всех прямых линий пути.

$$ d = \sum{|x_i-y_i|} $$

На следующем изображении показано расстояние L1 между двумя точками.

![L1](../img/L1DistMap.webp)

Кроме показанного пути существует несколько альтернативных способов. Например, от точки A можно подняться на два блока вверх, а потом на три блока вправо, либо же на три блока вправо и два блока вверх.

Но расстояние L1 — это всё же просто дистанция, а поэтому траектория здесь не имеет значения. Единственное, что нужно понимать, это примерный путь: нужно пройти какое-то количество X блоков на восток и Y блоков на север. Сумма расстояний этих блоков и будет расстоянием L1 от точки A до точки B.[^3-basic-distances-in-data-science]

![L1 расчет](../img/L1Formul.webp)

#### Расстояние Чебышёва (метрика шахматной доски)
Расстояние Чебышёва известно ещё как расстояние шахматной доски. Чтобы понять принцип такой метрики, нужно представить короля на шахматной доске — он может ходить во всех направлениях: вперёд, назад, влево, вправо и по диагонали.

![Linf](../img/ChebyshevMap.webp)

Разница расстояния L1 и расстояния Чебышёва в том, что при переходе на одну клетку по диагонали в первом случае засчитывается два хода (например вверх и влево), а во втором случае засчитывается всего один ход.

$$ d = \max{|x_i-y_i|} $$

Ещё эти оба расстояния отличаются от Евклидового расстояния тем, что у Евклидового движение по диагонали рассчитывается по теореме Пифагора.

![Евклидова, L1 и Чебышёва — 3 основные метрики, которые пригодятся в Data Science 6](../img/3vars.webp)

Расстояние Чебышёва можно представить как проход по шахматной доске.

Вот ещё один пример представления расстояния Чебышёва. Допустим, у вас есть дрон с двумя независимыми моторами: первый мотор тянет дрон вперёд, второй — в сторону. Оба мотора могут работать одновременно и равномерно на максимуме своей мощности.

Поэтому дрон может передвинуться на одну клетку по диагонали так же быстро, как по горизонтали или вертикали.

Посмотрите ещё раз на карту города по расстоянию Чебышёва. Первый шаг — оба мотора работают одновременно, второй шаг идентичен первому, а на третьем шаге мотор, тянущий дрон вперёд, отключается, и дрон смещается в сторону.

Таким образом, расстояние Чебышёва определяется как самая большая дистанция на одной оси.

![Linf расчет](../img/ChebyshevFormul.webp)

Прим. ред. Полученный результат является условным и некорректно сравнивать его с другими результатами.[^3-basic-distances-in-data-science]

#### Использование метрики
Давайте попробуем применить метрику для решения нашей задачи. Итак, представим, что есть некоторый объект, представленный строкой таблицы. Пусть у объекта $A$ значение признака $P$ отсутствует или некорректно. Как восстановить значение $P$ для $A$?

Основная идея — рассчитать расстояние от объекта $A$ до других объектов таблицы для того, чтобы найти объекты, наиболее близко расположенные к объекту $A$. Тогда значение признака $P$ из ближайших $k$ объектов можно взять за значение признака $P$ объекта $A$.

1. Исключим пока из таблицы столбец с признаком $P$, содержащим пропуски (мы предполагаем, что остальные ячейки в таблице нормальные).

2. Найдем расстояния (с помощью некоторой метрики) от строки $A$ до остальных объектов таблицы. Получим числа

    $$ \rho(A, A_1), \rho(A, A_2), \dots, \rho(A, A_n) $$

    Эти числа выражают расстояния от объекта $A$ до соответствующего объекта из таблицы.

3. Пусть значения признака $P$ для объектов $A_1, A_2, \dots, A_n$ равны $P(A_1), P(A_2), \dots, P(A_n)$.

Итак, у нас есть числа $P(A_1), P(A_2), \dots, P(A_n)$ и $\rho(A, A_1), \rho(A, A_2), \dots, \rho(A, A_n)$. Как их собрать в одну формулу, которая позволит адекватным образом вычислять значения признака $P$ для объекта $A$?

Пример:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 3 | 4 | 5 | 3 | 4
$A_2$ | 5 | 5 | 5 | 4 | 3
$A_3$ | 4 | 3 | 3 | 2 | 5
$A$  | 5 | 4 | 3 | 3 | **?**

Если пропуск заменить на среднее или медиану по столбцу (здесь они равны друг другу), то нужно писать 4. Попытаемся заполнить пропуск с помощью различных метрик.

Сперва удаляем столбец $P$ из таблицы и находим расстояние от объекта $A$ до остальных объектов из таблицы.

Вид метрики | От $A$ до $A_1$ | От $A$ до $A_2$ | От $A$ до $A_3$
-- | -- | -- | --
Евклид | 2,83 | 2,45 | 1,73
Манхеттен | 4 | 4 | 3
Макс | 2 | 2 | 1

Теперь вопрос: как скомпоновать расстояния от объекта $A$ до всех других объектов таблицы и значения признака $P$ у остальных объектов таблицы? Метрика с точки зрения математики выражает степень близости объектов друг к другу, причем зависимость обратная — чем больше расстояние, тем меньше мера близости, и наоборот — чем меньше расстояние, тем больше мера близости между объектами.

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **?**

Для ответа на этот вопрос нужно взять комбинацию значений признака $P$ у тех объектов, для которых этот признак известен, и меры близости этих объектов к объекту $A$. **Ключевая идея**: признак $P$ для объекта $A$ должен быть близок к значению признака $P$ у близких к $A$ объектов. Линейная комбинация со значениями признака $P$ по евклидовой метрике будет выглядеть следующим образом:

$$ \cfrac{1}{\cfrac{1}{2.83}+\cfrac{1}{2.45}+\cfrac{1}{1.73}}\left( \frac{4}{2.83} + \frac{3}{2.45} + \frac{5}{1.73} \right) = 4.15 $$

Значения признака $P$ здесь у объекта домножается на меру близости (делится на значение метрики, поскольку мера близости по своему смыслу обратно пропорциональна значению метрики, поэтому если метрика очень большая, то мера близости должна быть очень маленькая) и нормирующий множитель (дробь перед скобками). Таким образом получается оценка на значение признака $P$ для объекта $A$ — восстановленное значение признака.

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **4.15**

Естественно, восстанавливаемое значение признака объекта $A$ существенно зависит от выбранной метрики.

Например, матрикс-метрика даёт:

$$ \cfrac{1}{\cfrac{1}{2}+\cfrac{1}{2}+\cfrac{1}{1}}\left( \frac{4}{2} + \frac{3}{2} + \frac{5}{1} \right) = 4.25 $$

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **4.25**

- По Евклиду = 4,15
- По Макс = 4,25
- По Манхеттену = 4,1

Формула = 1/(1/AA1 + 1/AA2 + 1/AA3) * (значA1/AA1 + значA2/AA2 + значA3/AA3)

Расстояния AA1,AA2,AA3 берутся разные в зависимости от метода. ЗначA1, ЗначA2, ЗначA3 берутся из столбца P.

Поэтому выбор оптимальной метрики для решения задачи является делом непростым.

Таким образом, формула для восстановления данных с помощью метрики выглядит следующим образом:

$$ P(A) = \cfrac{1}{\sum_{i=1}^n \cfrac{1}{\rho(A, A_i)}} \left( \sum_{j=1}^n{\cfrac{P(A_j)}{\rho(A, A_j)}} \right) $$

Это формула получения оценки признака $P$ для объекта $A$, зная значения этого признака у других объектов таблицы и зная расстояния от объекта $A$ до других объектов в таблице.

> Что происходит, если у нас встречается объект, совершенно такой же, как и восстанавливаемый (то есть метрики будут равны нулю)?
>
> Это исключительная ситуация, поскольку расстояние между восстанавливаемым объектом и его клоном будет равна 0 (и во многих формулах произойдет деление на 0). Так что способ тут простой: берем значение из клона и пишем его в пропущенную ячейку. Или в знаменатель к метрике можно добавить +1, тогда деления на 0 не будет. Ещё можно придумать такой вариант обращения метрики: max(pi)-pj.

#### Выбор метрики
Обозначения L1, L2, L∞ происходят из семейства Lp-норм (Minkowski norms), где метрика расстояния между точками $x$, $y$ в $n$-мерном пространстве задается как:

$$ d_p(x, y) = \left( \sum_{i=1}^n{|x_i-y_i|^p} \right)^{1/p} $$

где $p$ — параметр нормы.

Объяснение обозначений метрик:

- L1 (манхэттенская): $p=1, d_1 = \sum{|x_i-y_i|}$, сумма абсолютных разностей; устойчива к выбросам, как путь по осям в городе.

- L2 (евклидова): $p=2, d_2 = \sqrt{\sum{(x_i-y_i)^2}}$, корень из суммы квадратов; классическое евклидово расстояние, гладкая оптимизация.

- L∞ (Чебышёва, max-метрика): $p→∞, d_∞ = \max_i{|x_i-y_i|}$, предел прироста $p$ — максимальная абсолютная разность по координатам.

С ростом $p$ вклад доминирующих (больших) разностей усиливается: L1 суммирует равномерно, L2 квадратирует (штрафует средние ошибки), L∞ фокусируется только на худшей координате. В пределе $p=∞$ сумма сводится к максимуму, а для $p=0$/дробных — дискретные нормы (редко в импутации). В ML L1/L2 популярны из-за выпуклости, L∞ — для консервативных оценок.

<dfn title="штрафование ошибок">Штрафование ошибок</dfn> (error penalization) в метриках расстояния — это механизм, при котором разные Lp-нормы по-разному усиливают вклад отдельных разностей $|x_i-y_i|$ между координатами точек, делая акцент на малых, средних или крупных отклонениях для более точного измерения "расстояния" в KNN-импутации или кластеризации.

Механизм штрафования в Lp-нормах: в формуле $d_p(x, y) = \left( \sum_{i=1}^n{|x_i-y_i|^p} \right)^{1/p}$ возведение в степень $p$ "штрафует" разности:

- L1 ($p=1$): Линейное штрафование $∑∣diff∣$, равномерно суммирует все ошибки без усиления; устойчиво к выбросам (одна большая разность не доминирует).​

- L2 ($p=2$): Квадратичное штрафование $\sqrt{∑diff^2}$, средние ошибки растут быстрее (2x ошибка → 4x вклад), крупные — еще сильнее; оптимально для гауссовых данных.​

- L∞ ($p=∞$): Максимальное штрафование $max∣diff∣$, игнорирует мелкие ошибки, фокусируется только на худшей координате (консервативно).

Пример влияния для разностей:​

- L1: 1+3+10=14 (выброс влияет линейно)

- L2: √(1+9+100)≈10.5 (выброс доминирует: 10→100)

- L∞: 10 (только худшая ошибка)​

Это определяет выбор: L1 для шумных данных, L2 для гладких, L∞ для "худшего случая".

#### Исходный код

```py
import pandas as pd
import numpy as np
from numpy import NaN

# Создадим датафрейм с пропущенным значением признака Р для объекта А
df = pd.DataFrame({'P1':[3,5,4,5], 'P2':[4,5,3,4], 'P3':[5,5,3,3], 'P4':[3,4,2,3], 'P':[4,3,5,NaN]}, index=['A1', 'A2', 'A3', 'A'])

# Посчитаем метрики
dict_metrics = {'A1':[], 'A2':[], 'A3':[]}
for i in df.index[:-1]:
  dict_metrics[i].append(np.power((df.loc['A'][:-1]-df.loc[i][:-1]).pow(2).sum(), 0.5).round(2)) # считаем Евклидово расстояние
  dict_metrics[i].append((df.loc['A'][:-1]-df.loc[i][:-1]).abs().sum()) # считаем Манхэттеновское расстояние
  dict_metrics[i].append((df.loc['A'][:-1]-df.loc[i][:-1]).abs().max()) # считаем max-метрику

metrics = pd.DataFrame(dict_metrics, index=['Euclid', 'Manhatten', 'Max'])

# Считаем варианты значений для каждой метрики
dict_value = {'Euclid':[], 'Manhatten':[], 'Max':[]}
for i in metrics.index:
  norm_mul = (1/((1/metrics.loc[i]).sum())) # нормирующий множитель
  similarity = ((df.loc[:]['P'][:-1]/metrics.loc[i]).sum()) # значение признака * мера близости(=величина, обратно пропорциональная мере расстояния)
  value_P = (norm_mul*similarity).round(2)
  dict_value[i].append(value_P)
  print(f'значение признака P для А по метрике {i}: {value_P}')
```

```py
import pandas as pd
import numpy as np

df = pd.DataFrame(
    data = {
        "Объекты": ["A1", "A2", "A3", "A"],
        "P1": [3, 5, 4, 5],
        "P2": [4, 5, 3, 4],
        "P3": [5, 5, 3, 3],
        "P4": [3, 4, 2, 3],
        "P":  [4, 3, 5, None]
    }
)
df.set_index("Объекты", inplace=True)
display(df)


# Euclidean metric
metrics = pd.DataFrame(data={"euclidean": np.sqrt(np.sum((df.loc['A'] - df.loc['A1':'A3'])**2, axis=1))})
# manhattan metric
metrics['manhattan'] = np.sum(np.abs(df.loc['A'] - df.loc['A1':'A3']), axis=1)
# max
metrics['max'] = np.max(
    np.abs(df.loc['A'] - df.loc['A1': 'A3']),
    axis=1
)
display('метрики', metrics)

# нормирующий множитель
coeff = 1 / np.sum(1 / metrics, axis=0)
display('нормирующий множитель', coeff)


# меры близости для метрик
p1 = 1 / metrics    # расчет мер близости
display('меры близости', p1)
weights = p1.T @ df.loc['A1':'A3', 'P'] #умножение мер на значения известных призоков P и суммируем
display(weights)
unknown_feature = weights * coeff
display(unknown_feature)
```

работать в ide не будет, если не добавить вначале строку
```py
from IPython.display import display
```

Более общий код. Он заменяет все возможные пропуски nan в любой таблице.

```py
def recover_by_metric(dataFrame, metric):
  clear_data = dataFrame[~dataFrame.isna().any(axis=1)]
  nan_data = dataFrame[dataFrame.isna().any(axis=1)]
  recovered_data = np.zeros(nan_data.shape)
  for i, nan_row in enumerate(nan_data.values):
    clear_indices = np.where(~np.isnan(nan_row))[0]
    metrics = np.array([metric(obj[clear_indices], nan_row[clear_indices]) for obj in clear_data.values])
    args = np.argsort(metrics)
    if metrics[args[0]] == 0:
      recovered_data[i, :] = clear_data.iloc[args[:np.sum(metrics == 0)], :].values.mean(axis=0)
    else:
      recovered_data[i, :] = ((1 / metrics) @ clear_data.values) / np.sum(1 / metrics)
      recovered_data[i, clear_indices] = nan_row[clear_indices]

  return pd.DataFrame(np.concatenate([clear_data.values, recovered_data], axis=0), columns=dataFrame.columns, index=dataFrame.index)
```

Либо такой вариант, здесь уже масштаб признаков не имеет значения.
```py
from sklearn.preprocessing import StandardScaler

def recover_by_metric(data, metric):
  data_scaled = StandardScaler().fit_transform(data.values)
  clear_data = data_scaled[~np.isnan(data_scaled).any(axis=1)]
  clear_data_origin = data[~data.isna().any(axis=1)]
  nan_data = data_scaled[np.isnan(data_scaled).any(axis=1)]
  nan_data_origin = data[data.isna().any(axis=1)]
  recovered_data = np.zeros(nan_data.shape)
  for i, nan_row in enumerate(nan_data):
    clear_indices = np.where(~np.isnan(nan_row))[0]
    metrics = np.array([metric(obj[clear_indices], nan_row[clear_indices]) for obj in clear_data])
    args = np.argsort(metrics)
    if metrics[args[0]] == 0:
      recovered_data[i, :] = clear_data_origin.iloc[args[:np.sum(metrics == 0)], :].values.mean(axis=0)
    else:
      recovered_data[i, :] = ((1 / metrics) @ clear_data_origin.values) / np.sum(1 / metrics)
      recovered_data[i, clear_indices] = nan_data_origin.iloc[i, clear_indices]

  return pd.DataFrame(np.concatenate([clear_data_origin.values, recovered_data], axis=0), columns=data.columns, index=data.index)
```

**Вопрос**: В функции `recover_by_metric(data, metric)` в `metric` что необходимо вводить?

**Ответ**: функцию, измеряющую какую-нибудь метрику. Например, из библиотеки sklearn можно подгрузить функции
```py
from sklearn.metrics import mean_squared_error

recover_by_metric(dataFrame, mean_squared_error)
```

Либо свою руками функцию можете написать
```py
def my_MSE(y_true, y_predicted):
    return ((y_true - y_predicted) ** 2).mean()


recover_by_metric(dataFrame, my_MSE)
```

Для дальнейших задач пригодится.

*Пример c тремя признаками-координатами*:
```py
# Координаты векторов P1 и P2
x1, y1, z1 = 0, 1, 2
x2, y2, z2 = 2, 1, 0

# Вычисление Евклидова расстояния
euclidean_distance = ((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)**0.5
print('Евклидово расстояние между векторами Р1 и Р2:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = abs(x2 - x1) + abs(y2 - y1) + abs(z2 - z1)
print('Расстояние Манхэттен между векторами Р1 и Р2:', manhattan_distance)

# Вычисление расстояния в max-метрике
max_metric_distance = max(abs(x2 - x1), abs(y2 - y1), abs(z2 - z1))
print('Расстояние в max-метрике между векторами Р1 и Р2:', max_metric_distance)
```

*Более общее объяснение (формулы с n кол-вом признаков)*:
```py
# Векторы признаков объектов a и b
a = (0, 1, 2, 3, 4)
b = (4, 3, 2, 1, 0)

# Количество признаков
n = len(a)

# Вычисление Евклидова расстояния
euclidean_distance = sum((b[i] - a[i])**2 for i in range(n))**0.5
print('Евклидово расстояние между объектами a и b:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = sum(abs(b[i] - a[i]) for i in range(n))
print('Расстояние Манхэттен между объектами a и b:', manhattan_distance)

# Вычисление расстояния в max-метрике
max_metric_distance = max(abs(b[i] - a[i]) for i in range(n))
print('Расстояние в max-метрике между объектами a и b:', max_metric_distance)
```

Решение с использованием библиотеки scipy (в терминале `pip install scipy`):
```py
from scipy.spatial.distance import euclidean, cityblock, chebyshev

# Векторы признаков объектов a и b
a = (0, 1, 2, 3, 4)
b = (4, 3, 2, 1, 0)

# Вычисление Евклидова расстояния
euclidean_distance = euclidean(a, b)
print('Евклидово расстояние между объектами a и b:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = cityblock(a, b)
print('Расстояние Манхэттен между объектами a и b:', manhattan_distance)

# Вычисление расстояния в max-метрике (метрика Чебышева)
max_metric_distance = chebyshev(a, b)
print('Расстояние в max-метрике между объектами a и b:', max_metric_distance)
```

```py
import numpy as np

class VectorOperations:

    def euclidean_distance(self, vector_a, vector_b):

        """Вычисляет Евклидово расстояние между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        return np.linalg.norm(vector_a - vector_b)



    def manhattan_distance(self, vector_a, vector_b, normalize=True):

        """Вычисляет расстояние Манхэттена (L1-метрика) между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        if normalize:

            vector_a = self._normalize(vector_a)

            vector_b = self._normalize(vector_b)

        return np.sum(np.abs(vector_a - vector_b))



    def max_metric_distance(self, vector_a, vector_b, normalize=True):

        """Вычисляет расстояние Чебышева (L∞-метрика) между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        if normalize:

            vector_a = self._normalize(vector_a)

            vector_b = self._normalize(vector_b)

        return np.max(np.abs(vector_a - vector_b))



    def _normalize(self, vector):

        """Нормирует вектор по L2-норме."""

        norm = np.linalg.norm(vector)

        if norm == 0:

            return vector  # Избегаем деления на ноль

        return vector / norm



    def _check_vectors(self, vector_a, vector_b):

        """Вспомогательная функция для проверки векторов."""

        if not isinstance(vector_a, np.ndarray) or not isinstance(vector_b, np.ndarray):

            raise TypeError("Векторы должны быть NumPy массивами.")

        if vector_a.shape != vector_b.shape:

            raise ValueError("Векторы должны иметь одинаковую размерность.")

        if not np.issubdtype(vector_a.dtype, np.number) or not np.issubdtype(vector_b.dtype, np.number):

            raise ValueError("Векторы должны содержать только числовые значения.")  



#Пример

vector_operations = VectorOperations()

a = np.array([5, 4,3, 3])

b = np.array([3, 4, 5, 3])



manhattan_normalized = vector_operations.manhattan_distance(a, b)

manhattan_unnormalized = vector_operations.manhattan_distance(a, b, normalize=False)



max_metric_distance_normalized = vector_operations.max_metric_distance(a, b)

max_metric_distance = vector_operations.max_metric_distance(a, b, normalize=False)



print(f"Расстояние Манхэттена (с нормированием): {manhattan_normalized}")

print(f"Расстояние Манхэттена (без нормирования): {manhattan_unnormalized}")



print(f"расстояние Чебышева (с нормированием): {max_metric_distance_normalized}")

print(f"расстояние Чебышева (без нормирования): {max_metric_distance}")
```

### Нормирование (нормализация) признаков

#### Замечание об использовании метрики
Обсудим тонкости, которые возникают при использовании метрики в задачах анализа данных. К сожаление, в данном вопросе не все так однозначно. Работа с метрикой требует аккуратности и большой осторожности. Это относится не только к проблеме восстановления данных, но и к любым другим алгоритмам, использующих понятие метрики.

При вычислении метрики исследуемые объекты представляются в виде точек в некотором пространстве (пространстве признаков), между которыми вычисляется расстояние (метрика). Например, объекты с двумя признаками можно представить в виде точек на плоскости:

![Feature space](../img/feat-space.png)

При данном подходе для адекватной работы алгоритмов анализа данных, использующих метрику, необходимо, чтобы все признаки (значения по осям) имели одинаковый масштаб. Если масштаб признака не одинаков по всем ося, то могут иметь место нежелательные эффекты. Рассмотрим следующую таблицу признаков.

Студент | Вес, кг | Рост, м
-- | -- | --
Иванов | 61 | 1,76
Сидорова | 56 | 1,50
Петров | 100 | 1,98

В данной таблице признаки заведомо имеют различный масштаб. При таком масштабе разница в весе между некоторыми объектами (студентами) может достигать 50 условных единиц, а разница в росте быть ничтожной из-за выбранной единицы измерения. Можно отметить, что в основе всех формул для вычисления метрик положено вычисление разности значений признаков. Таким образом, когда будут вычисляться расстояния между объектами, то для признака веса разница будет почти 50, а для роста она будет незначительной, что приведет к тому, что, например, некоторые признаки фактически будут проигнорированы (в след. примере различия в росте и различия в весе имеют ОЧЕНЬ разную ценность – это из-за разных единиц измерения). Так, в данном примере какая угодно разница в росте по сути не будет иметь никакого значения из-за большой разницы в весе. Это произошло из-за того, что мы имеем два признака с совершенно разными масштабами, что приведет к неадекватной работе нашего алгоритма.

Что нужно сделать, чтобы запустить корректно алгоритм, использующий метрики? При вычислении метрики все признаки необходимо приводить к единой шкале (нормировать).

#### Способы нормирования признака
Входные признаки в большинстве случаев будут иметь разный масштаб (диапазон изменения значений признака): одни признаки могут изменяться в диапазоне $[−0.01,0.01]$, другие — в $[0,100]$ и т.д. Как мы впоследствии увидим из описаний методов машинного обучения, для большинства из них масштаб признаков будет оказывать влияние на прогноз. Для таких моделей чем выше разброс значений признака, тем сильнее он будет влиять на прогноз, перекрывая влияние признаков меньшего масштабаз, поэтому, чтобы влияние всех признаков было одинаковым, их необходимо нормализовать, то есть привести к одному масштабу.[^Feature-normalization]

Допустим, имеется некоторый признак $P = (p_1, p_2, \dots, p_n)$.

- $\bar{p}$ — среднее значение;
- $s$ — отклонение (среднее квадратическое).

Наиболее распространены следующие способы

##### Стандартизация
<dfn title="стандартизация">Стандартизация</dfn> или <dfn title="z-стандартизация">Z-стандартизация</dfn> (*standard scaler, standardization, Z-score normalization*) — метод масштабирования, при котором из каждого значения вычитается среднее $μ$ по столбцу, а результат делится на стандартное отклонение $σ$:

$$ p'_i = \frac{p_i - \bar{p}}{s} $$

Полученные числа будут являться нормированными значениями признака $P$. Данное преобразование гарантирует, что после его выполнения у признака $P$ среднее значение и отклонение будут равны 0 и 1 соответственно.

$$ x'^j = \frac{x^j - \mu_j}{\sigma_j} $$

**Выходные свойства**: нулевое среднее и единичная дисперсия (ско).

**Отличительные особенности**:
- устойчив к выбросам;
- сохраняет форму распределения

**Условия использования**
- идеален для алгоритмов с евклидовой метрикой (KNN, SVM, градиентный спуск), где равный вклад фич критичен;
- подходит для предположительно нормальных данных или временных рядов (центрирование тренда).

*Реализация в Python*
```python
from sklearn.preprocessing import StandardScaler
import pandas as pd

scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(df_scaled.describe())  # mean≈0, std≈1 по столбцам
```

##### Диапазонное шкалирование (min-max scaling)
<dfn title="диапазонное шкалирование">Диапазонное шкалирование</dfn> (*min-max scaling*) — метод нормализации, который преобразует значения признаков так, чтобы они попадали в заданный диапазон, чаще всего от 0 до 1, сохраняя при этом относительные расстояния между точками и свойство «разреженных» данных, где большинство значений нули, что важно для эффективности хранения и обработки. Это противоположность стандартизации, и используется для перевода исходных значений ($x$) в новые ($x'$) по формуле:

$$ p'_i = \frac{p_i - p_{\min}}{p_{\max}-p_{\min}} $$

При этом минимальное значение признака $p$ гарантированно перейдет в 0, максимальное — в 1, а все промежуточные разместятся между ними.

$$ x'^j = \frac{x^j - \min(x^j)}{\max(x^j)-\min(x^j)} $$

**Выходные свойства**: принадлежит интервалу $[0,1]$.

**Отличительные особенности**:
- сохраняет нулевые значения (для sparse data);
- чувствителен к выбросам.

**Условия применения**:
- подходит для алгоритмов, чувствительных к масштабу, например, нейронные сети, метод k-ближайших соседей, обработки изображений/пикселей (значения 0-255 → 0-1) и когда известны теоретические границы признака;
- идеально подходит для работы с разреженными (sparse) данными, где много нулей, так как нули остаются нулями, а ненулевые значения масштабируются в $[0,1]$;
- не подходит для данных с выбросами (>1% экстремальных значений) — один outlier растягивает весь диапазон.​

*Реализация в Python (sklearn)*
```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Базовое [0,1]
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Кастомный диапазон [-1,1]
scaler_custom = MinMaxScaler(feature_range=(-1, 1))
df_custom = scaler_custom.fit_transform(df)

print(df_scaled.describe())  # min=0, max=1 по столбцам
```

##### Нормализация средним (centered min-max scaling)
Центрированная Min-Max нормализация, сочетающая вычитание среднего $μ_j$ по признаку $j$ с масштабированием по диапазону $max(x^j)−min(x^j)$, приводя данные к диапазону примерно [-0.5, 0.5] с центром в 0.

$$ x'^j = \frac{x^j - \mu_j}{\max(x^j)-\min(x^j)} $$

**Выходные свойства**: нулевое среднее, с единичным диапазоном ($[-0.5, 0.5]$ для симметричных данных).

**Отличительные особенности**:
- сохраняет относительные расстояния в пределах диапазона, как Min-Max, но центрирована для градиентного спуска.

**Условия использования**:
- алгоритмы с расстояниями (KNN), где нужен нулевой центр + ограниченный диапазон;
- градиентный спуск (нейросети), где симметрия [-0.5,0.5] ускоряет сходимость;
- данные с известными границами без сильных выбросов.

##### Нормализация максимумом по модулю (MaxAbsScaler)
Масштабирует каждый признак так, чтобы его значения находились в диапазоне [-1, 1] путём деления каждого значения на максимальное по абсолютной величине значение этого признака.

$$ x′ = \frac{x}{\max{(∣x∣)}} $$

**Выходные свойства**: диапазон [-1,1].

**Отличительные особенности**:
- не сдвигает данные (среднее и медиана остаются на месте), в отличие от MinMaxScaler и StandardScaler;
-  устойчив к выбросам лучше, чем MinMaxScaler, так как не зависит от минимального значения.

**Условия использования**:
- особенно полезен для разреженных данных (sparse data), так как не делает значения отрицательными и сохраняет нулевые значения;
- хорошо подходит для алгоритмов, чувствительных к масштабу, например, KNN, SVM, методы с евклидовой метрикой.

*Реализация в Python (sklearn)*
```python
from sklearn.preprocessing import MaxAbsScaler
import pandas as pd

scaler = MaxAbsScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(df_scaled.describe())  # все признаки в [-1,1]
```

##### Робастное шкалирование (Robust scaler)
Масштабирует признаки, используя медиану и межквартильный размах ($IQR$ — разница между 75-м и 25-м процентилями (квартилями), то есть $Q3 - Q1$), а не среднее значение и стандартное отклонение, что делает его устойчивым к выбросам (аномальным значениям — *outliers*). Он вычитает медиану и делит на $IQR$, сохраняя при этом форму распределения данных и эффективно нейтрализуя влияние экстремальных значений, что предпочтительнее в случаях, когда данные содержат выбросы, влияющие на StandardScaler. 

$$ x′ = \frac{x- m_e}{IQR} = \frac{x- m_e}{Q_3(x) - Q_1(x)} $$

**Выходные свойства**: максимальные и минимальные значения каждого признака после трансформации будут $≤1$ и $≥−1$ соответственно.

**Отличительные особенности**:

- **Устойчивость к выбросам**: Медиана и IQR менее чувствительны к экстремальным значениям, чем среднее и стандартное отклонение.
- **Сохранение формы распределения**: Не меняет форму исходных данных так сильно, как другие методы.

**Условия использования**:

- для скошенных распределений;
- когда в данных есть выбросы (экстремальные значения);
- когда важно сохранить относительные расстояния между значениями без сильного искажения;
- данные с выбросами + расстояния (KNN с L1/L2), градиентный спуск, PCA.

*Пример использования (в Python с scikit-learn)*
```python
from sklearn.preprocessing import RobustScaler
import numpy as np

data = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [100, 200]]) # С выбросом 100, 200
scaler = RobustScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)
```

#### Использование нормализации признаков
Каждый вид шкалирования применяется к каждому признаку (столбцу в матрице объекты-признаки X) _независимо_.

Самым популярным методом нормализации признака является **стандартизация признака**. Вторым по популярности является **диапазонное шкалирование**. Оно хорошо тем, что значения признака из отрезка [0,MAX] переводятся в отрезок [0,1], причём ноль переходит в ноль, что полезно для разреженных данных (*sparse data*), в которых большинство значений — нули. Такие данные часто возникают на практике и эффективно кодируются разреженными матрицами (*sparse matrix*[^sparse-matrix]), которые экономично их хранят и производят операции над ними, оперируя только ненулевыми элементами. Диапазонное шкалирование _позволяет сохранить свойство разреженности_.[^Feature-normalization]

Возвращаясь к примеру из предыдущей темы:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 3 | 4 | 5 | 3 | 4
$A_2$ | 5 | 5 | 5 | 4 | 3
$A_3$ | 4 | 3 | 3 | 2 | 5
$A$  | 5 | 4 | 3 | 3 | **?**

По-хорошему, перед использованием метрики для восстановления значения признака $P$ у объекта $A$ необходимо нормировать все признаки кроме $P$:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 0 | 0.5 | 1 | 0.5 | 4
$A_2$ | 1 | 1 | 1 | 1 | 3
$A_3$ | 0.5 | 0 | 0 | 0 | 5
$A$  | 1 | 0.5 | 0 | 0.5 | **?**

Здесь минимальное значение (3) переходит в 0, максимальное (5) — в 1, а значение посередине (3) — соответственно, в среднее между 0 и 1 (0.5). Стоить также помнить о том, что если нужно считать расстояние между объектами (строками), то нормируют не по строкам (объектам), а по колонкам (признакам). В колонке P4: мин  2= 1, макс 4 =1, а 3 = 0.5.

> Почему не нормируется признак с пропущенными значениями? Ведь теперь все расстояния между ним и остальными исказились.

> Подспудно признак с пропусками таки нормируется. Его нормировка "зашита" в формулу восстановления пропущенного значения: там мы делим на сумму расстояний. Значения столбца $P$ остаются теми же, а потом берется их взвешенная комбинация (эти числа домножаются на числа, получаемые из метрики). И таким образом в итоге получается число где-то посередке от исходных значений.

> Если нормировать по второй формуле, используя среднее и отклонение, то для признака А1Р1 нормированное значение меньше 0 получится? А1(3) - среднее АР1(4,25) / 0,96 = -1,25 / 0,96.

> При такой нормировке среднее значение признака получается равным нулю, а остальные значения распределяются вокруг него — то есть, могут быть как отрицательными, так и положительными.

> Маленько "словил" когнитивный диссонанс, пытаясь понять, что делать, когда "отклонение" равно нулю. Но потом понял, что в этом случае можно выражение "0/0" (неопределённость) принять равным либо единице, либо какому нибудь другому числу из интервала, соответствующего используемому методу нормировки. То есть [0; 1] или [-1; 1]..

##### Программный код

```py
pip install neulab

from neulab.RestoreValue import MetricRestore

d = {'P1': [3, 5, 4, 5], 'P2': [4, 5, 3, 4], 'P3': [5, 5, 3, 3], 'P4': [3, 4, 2, 3], 'P5': [4, 3, 5, np.NaN]}
df = pd.DataFrame(data=d)

# Euclid
euclid_m = MetricRestore(df, row_start=0, row_end=9, metric='euclid')
# Manhattan
mnht_m = MetricRestore(df, row_start=0, row_end=9, metric='manhattan')
# Max
mx_m = MetricRestore(df, row_start=0, row_end=9, metric='max')

Output:
euclid_m = 4.13
mnht_m = 4.1
mx_m = 4.25
```

### Заключение

- Пропущенные значения можно восстанавливать, используя меру близости объектов друг к другу.
- Мера близости объектов вычисляется с помощью метрики.
- При использовании метрики нужно привести все признаки к одному масштабу (нормировать).
- Существует несколько способов нормирования значений признаков.

### Источники информации
[^3-basic-distances-in-data-science]: [Евклидова, L1 и Чебышёва — 3 основные метрики, которые пригодятся в Data Science](https://tproger.ru/translations/3-basic-distances-in-data-science)
[^Feature-normalization]: [Нормализация признаков](https://deepmachinelearning.ru/docs/Machine-learning/Data-preprocessing/Feature-normalization)
[^sparse-matrix]: [Python-school: введение в разреженные матрицы](https://python-school.ru/blog/python/sparse-matrix/)
