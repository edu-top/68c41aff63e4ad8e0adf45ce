<link href="../styles.css" rel="stylesheet" />

## 02. Введение в большие данные

- [02. Введение в большие данные](#02-введение-в-большие-данные)
  - [Большие данные](#большие-данные)
    - [Данные и информация](#данные-и-информация)
    - [Что такое большие данные?](#что-такое-большие-данные)
    - [Особенности больших данных](#особенности-больших-данных)
  - [Работа с Big Data](#работа-с-big-data)
  - [Хранение и обработка больших данных](#хранение-и-обработка-больших-данных)
    - [Хранилище данных](#хранилище-данных)
      - [Решения на основе данных](#решения-на-основе-данных)
      - [Планирование ресурсов предприятия](#планирование-ресурсов-предприятия)
      - [Роль и функции КХД](#роль-и-функции-кхд)
      - [ETL и ELT](#etl-и-elt)
      - [Построение хранилищ данных](#построение-хранилищ-данных)
      - [Основныхе характеристики КХД](#основныхе-характеристики-кхд)
    - [Озеро данных](#озеро-данных)
      - [Применение Data Lake](#применение-data-lake)
      - [Примеры использования Data Lake](#примеры-использования-data-lake)
      - [Как устроено озеро данных](#как-устроено-озеро-данных)
      - [Отличия Data Lake от хранилища данных](#отличия-data-lake-от-хранилища-данных)
      - [Преимущества Data Lake](#преимущества-data-lake)
      - [Недостатки Data Lake](#недостатки-data-lake)
      - [Советы по использованию Data Lake](#советы-по-использованию-data-lake)
    - [Витрина данных](#витрина-данных)
      - [Отличия витрины от хранилища данных](#отличия-витрины-от-хранилища-данных)
      - [Преимущества Data Mart](#преимущества-data-mart)
      - [Типы Data Mart](#типы-data-mart)
      - [Применение витрин данных](#применение-витрин-данных)
      - [Реализация Data Mart](#реализация-data-mart)
    - [OLTP](#oltp)
      - [Как работает OLTP](#как-работает-oltp)
      - [Особенности OLTP](#особенности-oltp)
      - [Требования к OLTP](#требования-к-oltp)
      - [Где применяется OLTP](#где-применяется-oltp)
      - [Преимущества и недостатки OLTP](#преимущества-и-недостатки-oltp)
      - [Популярные системы и платформы](#популярные-системы-и-платформы)
    - [OLAP](#olap)
      - [Суть технологии](#суть-технологии)
      - [Как работают OLAP-системы](#как-работают-olap-системы)
        - [Хранение данных](#хранение-данных)
        - [Средства ETL](#средства-etl)
        - [Сервер](#сервер)
        - [Аналитические инструменты](#аналитические-инструменты)
      - [Требования к OLAP-системам](#требования-к-olap-системам)
      - [Чем отличаются технологии OLAP и OLTP](#чем-отличаются-технологии-olap-и-oltp)
      - [Преимущества и недостатки](#преимущества-и-недостатки)
      - [Популярные системы и платформы](#популярные-системы-и-платформы-1)
        - [СУБД Greenplum](#субд-greenplum)
  - [Аналитика больших данных](#аналитика-больших-данных)
    - [Основная терминология](#основная-терминология)
    - [Бизнес-аналитика](#бизнес-аналитика)
      - [Преимущества внедрения бизнес-аналитики BI](#преимущества-внедрения-бизнес-аналитики-bi)
      - [Какие задачи решают BI-системы для руководителей и аналитиков](#какие-задачи-решают-bi-системы-для-руководителей-и-аналитиков)
      - [Примеры задач, решаемых с помощью BI](#примеры-задач-решаемых-с-помощью-bi)
      - [BI и сквозная аналитика: в чем различие?](#bi-и-сквозная-аналитика-в-чем-различие)
      - [Ключевые этапы внедрения BI](#ключевые-этапы-внедрения-bi)
      - [Примеры используемых систем](#примеры-используемых-систем)
  - [Разработка систем Big Data](#разработка-систем-big-data)
    - [Специалисты для работы с Big Data](#специалисты-для-работы-с-big-data)
    - [Этапы разработки систем Big Data](#этапы-разработки-систем-big-data)
  - [Направления работы в области Big Data](#направления-работы-в-области-big-data)
  - [Применение Big Data](#применение-big-data)
  - [Преимущества и недостатки Big Data](#преимущества-и-недостатки-big-data)
  - [Наука о данных](#наука-о-данных)
    - [Направления и специальности](#направления-и-специальности)
    - [Уровни компетенций](#уровни-компетенций)
  - [Взаимосвязь машинного обучения и больших данных](#взаимосвязь-машинного-обучения-и-больших-данных)
  - [Интернет вещей](#интернет-вещей)
    - [Архитектура IoT-систем](#архитектура-iot-систем)
    - [Как интернет вещей собирает малые данные](#как-интернет-вещей-собирает-малые-данные)
    - [Как работает интернет вещей с Big Data](#как-работает-интернет-вещей-с-big-data)
    - [Роль Big Data и ML в IoT](#роль-big-data-и-ml-в-iot)
  - [Представление данных для машинного обучения. Признаки объектов](#представление-данных-для-машинного-обучения-признаки-объектов)
      - [Типы признаков](#типы-признаков)
      - [Ограничения табличного представления объектов](#ограничения-табличного-представления-объектов)
  - [Заключение](#заключение)
      - [Выводы](#выводы)
  - [Практическая работа. Представление признаков в табличной форме](#практическая-работа-представление-признаков-в-табличной-форме)
    - [Задание](#задание)
  - [Глоссарий](#глоссарий)
  - [Источники информации](#источники-информации)

### Большие данные
В мире, где цифровизация стала основой нашего образа жизни, мы столкнулись с новым явлением, получившим название «большие данные». Но что стоит за этим термином, и каковы его реальные масштабы?

> Большие данные, или Big Data, – это не просто объемная информация. Это данные такого масштаба, скорости и разнообразия, что традиционные методы их обработки становятся неэффективными. Мы говорим о терабайтах и петабайтах данных, поступающих из самых разных источников: от социальных сетей до космических телескопов.

Однако, объем – это лишь верхушка айсберга. Большие данные представляют собой не только квантовое увеличение информации, но и качественное изменение. Они включают в себя структурированную, полуструктурированную и неструктурированную информацию, требующую специализированных подходов к хранению, обработке и анализу.

Работа с большими данными выдвигает перед IT-специалистами целый ряд вызовов. С одной стороны, это вопросы хранения и доступа к данным. С другой – необходимость преобразования этих массивов информации в понятные и полезные знания, способные повлиять на принятие решений в бизнесе, науке и других сферах.

Для начала стоит разобраться, чем отличаются данные от информации.

#### Данные и информация
<dfn title="данные">Данные</dfn> — это необработанные, сырые факты и цифры, не имеющие самостоятельного смысла, в то время как <dfn title="информация">информация</dfn> — это обработанные, структурированные и контекстуализированные данные, которые приобретают значение и цель для пользователя. Информация помогает понять закономерности, принимает форму отчета или сводки, тогда как данные — это отдельные фрагменты пазла, которые становятся цельной картиной только после их сборки.

**Ключевые отличия**
- **Структура**: данные — это неорганизованная совокупность фактов (числа, текст, изображения), а информация — это данные, которые были упорядочены и связаны в логическую последовательность. 
- **Смысл и контекст**: данные не несут конкретного смысла или назначения сами по себе, в отличие от информации, которая имеет значение и цель, добавляемую при её обработке. 
- **Цель использования**: Данные служат основой для анализа и принятия решений, а информация помогает осмысливать эти данные и формировать суждения. 
- **Превращение**: данные превращаются в информацию в результате обработки, интерпретации, анализа и добавления контекста. 

**Пример**
- **Данные**: Список цифр в электронной таблице, например: "150 (Январь), 300 (Февраль)".
- **Информация**: Отчет, который анализирует эти цифры и показывает, что "В начале года (январь-февраль) наблюдался рост расходов на транспорт, что требует дальнейшего изучения". 

Отличие информации от данных состоит в том, что:
1) **Данные** — это фиксированные сведения о событиях и явлениях, которые хранятся на определенных носителях, а **информация** появляется в результате обработки данных при решении конкретных задач.

    Например, в базах данных хранятся различные данные, а по определенному запросу система управления базой данных выдает требуемую информацию.

2) Данные — это носители информации, а не сама информация.

3) Данные превращаются в информацию только тогда, когда ими заинтересуется человек. Человек извлекает информацию из данных, оценивает, анализирует ее и по результатам анализа принимает то или иное решение.

    Данные превращаются в информацию несколькими путями:

   - контекстуализация: мы знаем, для чего эти данные нужны;
   - категоризация: мы разбиваем данные на типы и компоненты;
   - подсчет: мы обрабатываем данные математически;
   - коррекция: мы исправляем ошибки и ликвидируем пропуски;
   - сжатие: мы сжимаем, концентрируем, агрегируем данные.

   Таким образом, если существует возможность использовать данные для уменьшения неопределенности знаний о каком-либо предмете, то данные превращаются в информацию. Поэтому можно утверждать, что информацией являются используемые данные.

4) Информацию можно измерять. Мера измерения содержательности информации связана с изменением степени неосведомленности получателя и основана на методах теории информации.[^38087337]

#### Что такое большие данные?
<dfn title="big data">Big Data</dfn> — это крупные массивы разнообразной информации и стек специальных технологий для работы с ней. Термин применяется к таким объемам данных, с которыми пользовательский компьютер и офисные программы не справятся. С помощью анализа больших данных бизнес может получить возможность принимать решения по развитию продукта и завоевывать конкурентное преимущество.

Термин Big Data появился еще в прошлом веке, но начал набирать популярность, когда появились первые крупные интернет-сервисы. Компании столкнулись с тем, что пользователи загружают на сайты колоссальные объемы неструктурированного контента.

Это заставило разработчиков придумывать новые типы хранилищ данных, поскольку стандартных уже не хватало. Первой платформой, которая взяла на себя работу с такими объемами данных, стала [Hadoop](https://ru.wikipedia.org/wiki/Hadoop). К настоящему времени она обладает мощным стеком инструментов.[^bigdata]

#### Особенности больших данных
<dfn title="большие данные">Большие данные</dfn> — это наборы данных настолько объёмные или сложные, что традиционные методы обработки не справляются с ними эффективно. Они характеризуются "3V":

1. **Volume** (*объём*) — очень большой объём информации (терабайты и петабайты). Как правило, информации должно поступать более 150 Гб в сутки.

2. **Velocity** (*скорость*) — данные поступают очень быстро (поток данных в реальном времени). Для работы с массивами информации в режиме реального времени требуются повышенные вычислительные мощности.

3. **Variety** (*разнообразие*) — данные бывают разных типов и форматов. Поступающая информация имеет разные форматы или степень структурированности. Например, контент социальных сетей может сильно различаться даже в пределах одной страницы.

Иногда добавляют еще три дополнительных признака больших данных, которые точно помогают определить, что речь идет именно о Big Data:

4. **Veracity** (*достоверность*) — источникам данных можно доверять, а результат их обработки обладает достоверностью, достаточной для принятия решений.

5. **Variability** (*вариативность*) — поток данных изменчив, на него может влиять даже время суток или погода. Например, в час пик приходит больше данных от таксистов.

6. **Value** (*ценность*) — данные могут иметь разное значение для компании. Например, сделки с крупными покупателями имеют большее значение, чем с мелкими.

![Big Data 6V](../img/bigdata_6v-300x222.png)

*Примеры типов данных*

Большие данные |	Обычные данные
-- | --
Записи всех звонков сотрудников крупного колл-центра |	Бухгалтерские отчеты компании в Excel
Поисковые запросы, переходы по ссылкам, движения и нажатия мыши всех пользователей поисковой системы |	ФИО и возраст всех пользователей отдельного сервиса
Сведения о перемещениях таксистов, трафик и спрос на поездки |	Расписание маршрутов всего общественного транспорта области
Информация о покупках клиентов банка и снятии ими наличных в терминалах и отделениях |	Список клиентов с просроченными задолженностями

Еще одна особенность больших данных заключается в их распределенной структуре — для сбора и анализа информации одновременно используется множество инструментов. Получается что-то наподобие воронок, которые пропускают информацию из разных источников, попутно обрабатывая ее. В умелых руках это дает ряд преимуществ:

- **Расширяемость** — платформы для работы с Big Data можно горизонтально масштабировать до тех пор, пока хватает вычислительных мощностей.
- **Отказоустойчивость** — сбой в одном потоке не нарушает работу других.
- **Локализация** — информация обрабатывается на тех же серверах, где она находится, что минимизирует затраты на транспортировку.[^bigdata]

Источники больших данных:

- социальные сети;
- сенсоры и IoT-устройства;
- транзакционные системы (банки, интернет-магазины);
- логи серверов и системных устройств.

### Работа с Big Data
*[DAS]: Direct-attached storage
*[NAS]: Network Attached Storage
*[SAN]: Storage Area Network

До начала создания базы данных нужно определить, какие технологии планируется использовать для сбора, хранения, обработки и анализа информации. Чтобы лучше понимать эти процессы, рассмотрим этапы работы (**пайплайн**) с Big Data:

1. <dfn title="сбор данных">Сбор данных</dfn> (*data collection*) — процессы получения информации из различных источников для последующего анализа и использования.

    Все начинается с интеграции технологий сбора информации, определения ее источников и необходимой обработки. Это могут быть действия пользователей сайта, отчеты о продажах, статистические, медицинские и любые другие данные, которые ценны для компании. К процессу также подключаются специалисты по Data Cleaning, которые настраивают фильтры для будущего анализа.

    **Основные этапы**: сбор данных охватывает извлечение сырых данных из источников, таких как датчики, базы данных, веб-формы или документы. Ключевые шаги: идентификация источников, захват данных (сканирование, импорт, опросы) и начальная проверка целостности. Это обеспечивает готовность данных для обработки в ML или аналитике.

    **Методы сбора**
      - **Автоматизированный**: через API, IoT-устройства, логи систем или веб-скрейпинг.

      - **Ручной**: опросы, интервью, анкетирование или ввод из документов.

      - **Гибридный**: комбинация, например, сканирование бумажных форм с OCR для цифровизации.

    Эти компоненты формируют фундамент пайплайна данных в Big Data и машинном обучении.

2. <dfn title="хранение данных">Хранение данных</dfn> (*data retention*, *data storage*) — процессы сохранения, организации и обеспечения доступа к информации на цифровых носителях.

    **Основные компоненты**: хранение охватывает выбор носителей (диски, облака, СХД), структуру данных (реляционные БД, NoSQL, озера данных) и управление жизненным циклом: от загрузки до архивирования. Ключевые задачи — обеспечение целостности, масштабируемости и безопасности через резервное копирование и репликацию. На данном этапе используются такие инструменты, как NoSQL базы данных (MongoDB, Cassandra), распределённые файловые системы (например, HDFS).

    **Типы систем хранения**
    - **Блоковое**: данные как блоки (DAS, SAN) для высокопроизводительных приложений.

    - **Файловое**: иерархическая структура (NAS) для общего доступа.

    - **Объектное**: для неструктурированных данных в Big Data (S3, MinIO).

    <details>
    <summary>Про DAS, NAS, SAN</summary>

    <dfn title="NAS">NAS</dfn> (англ. *Network Attached Storage*, сетевое хранилище) — сервер для хранения данных на файловом уровне. По сути представляет собой компьютер с некоторым дисковым массивом, подключённый к сети (обычно локальной) и поддерживающий работу по принятым в ней протоколам. Несколько таких компьютеров могут быть объединены в одну систему.[^NAS]

    <dfn title="SAN">SAN</dfn> (англ. *Storage Area Network*, сеть хранения данных) — архитектурное решение для подключения внешних устройств хранения данных, таких как дисковые массивы, ленточные библиотеки, оптические приводы к серверам таким образом, чтобы операционная система распознала подключённые ресурсы как локальные. SAN характеризуются предоставлением так называемых сетевых блочных устройств (обычно посредством протоколов Fibre Channel, iSCSI или AoE), в то время как сетевые хранилища данных (англ. Network Attached Storage, NAS) нацелены на предоставление доступа к хранящимся на их файловой системе данным при помощи сетевой файловой системы (такой как NFS, SMB/CIFS, или Apple Filing Protocol). При этом категоричное разделение SAN и NAS является искусственным: с появлением iSCSI началось взаимное проникновение технологий с целью повышения гибкости и удобства их применения.[^SAN]

    <dfn title="DAS">DAS</dfn> (англ. *Direct-attached storage* — система хранения данных с прямым подключением, дисковое хранилище) — запоминающее устройство, непосредственно подключённое к серверу или рабочей станции, без помощи сети хранения данных. Это ретроним, используемый в основном для отличия несетевых устройств хранения от SAN и NAS. Де-факто DAS — это быстрое (если интерфейс быстрый) локальное хранилище, доступное только тому устройству, к которому оно подключено. Жёсткий диск внутри ПК тоже своего рода DAS. DAS часто называют «островами информации»[^DAS].

    </details>

    Эти элементы формируют базу для последующего анализа и ML.

    Для больших объемов информации недостаточно будет даже нескольких компьютеров, поэтому компании прибегают к услугам облачных провайдеров и задействуют распределенные вычислительные мощности. Примеры технологий, которые используются для хранения:

   - <dfn title="data warehouse">Data Warehouse</dfn> (*хранилище данных*) — единое корпоративное хранилище с обработанной и структурированной информацией. Хранилище упрощает анализ полученных данных, но требует структурированности.
   - <dfn title="data vault">Data Vault</dfn> (*свод данных*) — одна из моделей хранилища Data Warehouse с временными отметками размещения данных, которые позволяют проследить изменение хранимой информации во времени.
   - <dfn title="data lake">Data Lake</dfn> (*озеро данных*) — данные в хранилище поступают непрерывно в неструктурированном или, наоборот, структурированном или слабоструктурированном виде. Используется для сбора данных из разных источников в режиме реального времени.
   - <dfn title="data mart">Data Mart</dfn> (*витрина данных*) — хранилище данных, предназначенных для повседневного использования. Поступающую информацию необходимо тщательно обрабатывать, но после этого к ней проще регулярно обращаться.

3. <dfn title="обработка данных">Обработка данных</dfn> (*data processing*) — преобразование сырых данных в пригодный для анализа формат.

    **Основные этапы**
    - <dfn title="очистка данных">Очистка данных</dfn> (*data cleaning*) — процесс выявления и устранения ошибок, несоответствий и "грязи" в датасетах для повышения их качества. Очистка обеспечивает точность ML-моделей, снижая шум и предвзятость. Включает следующие методы: удаление дублей, пропусков, выбросов и исправление ошибок.

    - <dfn title="трансформация данных">Трансформация данных</dfn> (*data transformation*) — это процесс преобразования сырых данных в удобный для анализа или хранения формат. Включает следующие методы: нормализация, кодирование категориальных переменных, масштабирование признаков.

    - <dfn title="интеграция данных">Интеграция даных</dfn> (*data integration*) — объединение данных из разных источников в единый набор данных (*dataset*).

    - <dfn title="агрегация данных">Агрегация данных</dfn> (*data aggregation*) — процесс обобщения информации в сводную форму для анализа. Агрегация в пайплайне следует после интеграции: сначала сливаются источники, затем данные суммируются. Уменьшает объём данных, ускоряя обработку без потери ключевых инсайтов. Агрегация снижает нагрузку на хранение, интеграция обеспечивает полноту. Результат — компактный датасет с метриками, удобный для отчётов и визуализации.

    - <dfn title="обогащение данных">Обогащение данных</dfn> (*data enrichment*) — добавления дополнительной ценной информации к существующим наборам (например, дополняет записи новыми атрибутами из внешних источников). Отличается от агрегации расширением (добавление), а не сжатием данных.

    Обработка обеспечивает качество данных для ML-моделей: снижает шум, повышает точность прогнозов и ускоряет вычисления.

    Для обработки крупных объемов информации используется технология [MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html). Массивы распределяется на разных узлах, которые могут параллельно их обрабатывать, даже если на одном узле случилась ошибка. На MapReduce, например, работают кластеры Apache Spark™, Apache Hadoop®.

4. <dfn title="анализ данных">Анализ данных</dfn> — это процесс исследования, фильтрации и моделирования информации для извлечения полезных знаний и поддержки решений.

    Заключительным этапом работы является анализ — получение самого ценного из всего хранилища данных. С помощью СУБД, нейросетей и других инструментов массивы информации преобразуются в таблицы, диаграммы, графики и другое (**визуализация данных**). Анализ начинается с **описательного** (что произошло?), переходит к **диагностическому** (почему?), **предиктивному** (что будет?) и **пресскриптивному** (что делать?). Включает статистику, визуализацию и ML для выявления паттернов (интеллектуальную аналитику). На данном этапе используются такие инструменты, как MapReduce, Apache Hadoop, Apache Spark.

    **Визуализация** используется на финальных шагах анализа для представления результатов: графики, дашборды, тепловые карты помогают выявить паттерны и тренды. **Вывод инсайтов** следует сразу после — интерпретация визуализаций для бизнес-решений или гипотез. <dfn title="инсайт">Инсайт</dfn> (*insight* — с англ. "прозрение", "озарение") в анализе данных — это глубокое, неожиданное понимание или ценное открытие, полученное из данных, которое приводит к практическим рекомендациям. Инсайт отличается от обычных фактов тем, что объясняет "почему" и "как использовать": например, не просто "продажи упали на 20%", а "отток вырос из-за задержек доставки в регионы X и Y". Он рождается на стыке данных, гипотез и контекста. Вывод инсайтов завершает анализ данных: после визуализации и моделирования аналитик интерпретирует результаты для бизнеса.

    **Примеры назначения анализа**

    - <dfn title="сравнительная аналитика">Сравнительная аналитика</dfn> (*competitive analytics*) — изучение поведения потребителей и их вовлеченность в режиме реального времени, чтобы сравнить продукт компании с продуктами конкурентов.
    - <dfn title="аналитика настроений">Аналитика настроений</dfn> (*sentiment analysis*) — изучение отзывов клиентов и обсуждений продукта в соцсетях, чтобы выявить слабые стороны продукта и уровень удовлетворенности потребителей.
    - <dfn title="маркетинговая аналитика">Маркетинговая аналитика</dfn> (*marketing analytics*) — изучение данных о клиентах, чтобы улучшить маркетинговые компании и разработать бизнес-инициативы.
    - <dfn title="интеллектуальный анализ данных">Интеллектуальный анализ данных</dfn> (*data mining*) — подмножество анализа для поиска скрытых зависимостей, процесс «просеивания» больших массивов данных с целью извлечь из них ценную информацию для конкретного применения. Он является неотъемлемой частью науки о данных и бизнес-аналитики и направлен в первую очередь на поиск закономерностей.

_Пример_: Анализ кликов пользователей на сайте для персонализации рекламы в реальном времени.

*Пример MapReduce (подсчёт слов) (псевдокод)*:
```py
Map(key, value):
  for word in value:
    Emit(word, 1)

Reduce(key, values):
  sum = 0
  for v in values:
    sum += v
  Emit(key, sum)
```

### Хранение и обработка больших данных
*[DWH]: Data Wharehouse
*[CRM]: Customer Relationship Management
*[ERP]: Enterprise Resource Planning

Современные организации генерируют и собирают данные в огромных объёмах и разнообразии форм. Эффективное хранение таких данных требует подходов, которые обеспечивают не только надёжность и масштабируемость, но и удобство доступа для анализа, прогнозирования и принятия решений. Для решения задач хранения и обработки используется большое количество разнообразных архитектурных моделей и хранилищ данных: от традиционных реляционных баз данных и локальных систем до специализированных решений для больших данных, включая Data Lake, хранилища данных (Data Warehouse) и промежуточные структуры Data Mart. Особое внимание уделяется облачным хранилищам и гибридным схемам, которые позволяют сочетать преимущества локальных и удалённых инфраструктур.

**Основные концепции**

- <dfn title="data lake">Data Lake</dfn> (озеро данных): универсальный репозиторий для хранения данных в формате «как есть» — в структурированном, полуструктурированном и неструктурированном виде. Такой подход упрощает сбор и первичную агрегацию данных из разных источников, сохраняя их неизменными до момента необходимого анализа.

- <dfn title="data warehouse">Data Warehouse</dfn> (DWH, хранилище данных): централизованное хранилище структурированных данных, оптимизированное под единый путь анализа, бизнес-отчётности и сложные запросы. Часто применяется после этапов очистки и трансформации данных.

- <dfn title="data mart">Data Mart</dfn> (витрина данных): локальное или специализированное подручное хранилище данных, ориентированное на определённый бизнес-подразделение или процесс, с более быстрым доступом и меньшим объёмом данных по сравнению с DWH.

- <dfn title="реляционная база данных">Реляционные базы данных</dfn> (*relational databases*): классические системные решения для хранения структурированной информации с поддержкой транзакционности, целостности данных и SQL-запросов.

- **Облачные хранилища и гибридные подходы**: аренда ресурсов и сервисов в облаке, которые позволяют масштабироваться под нагрузку, интегрировать локальные и облачные источники данных, снижая капитальные затраты и усилия по администрированию.

- **Метаданные и управление качеством данных**: ключевые компоненты любой архитектуры больших данных, обеспечивающие поиск, контекстуализацию и доверие к данным через каталоги, линейки данных и политику доступа.

**Зачем нужна такая гибкость**

- **Масштабируемость**: рост объёмов данных требует инфраструктуры, которая может динамически наращивать мощность и хранить данные любого типа.

- **Разнообразие источников**: современные предприятия собирают данные из CRM, ERP, IoT-устройств, лог-файлов, социальных сетей и внешних сервисов; единое решение должно поддерживать разные форматы.

- **Разделение задач**: разные потребители — аналитики, дата-сайентисты, бизнес-итоги — нуждаются в разных режимах доступа и скорости отклика.

- **Эффективность анализа**: правильная комбинация хранилищ позволяет ускорить бизнес-аналитику, уменьшить издержки на хранение и повысить скорость внедрения инсайтов.

*Реализация хранения Big Data с помощью популярных сервисов*

Назначение |	Сервис (инструмент)
-- | --
Обработка транзакций |	Apache HBase, Amazon DynamoDB, Google Cloud Spanner, Microsoft SQL Server (In-Memory/OLTP), PostgreSQL with WAL-based replication
Запросы и отчеты |	Apache Hive, Apache Impala, Amazon Athena, Google BigQuery, Amazon Redshift
New SQL |	Apache Cassandra, Google Cloud Spanner, CockroachDB, VoltDB, TiDB
Документоориентированная СУБД |	MongoDB, Couchbase, Amazon DocumentDB, Firebase Firestore, RavenDB
Резидентная СУБД |	Redis, SQLite, Derby (Embedded DB), H2, LevelDB
БД «ключ — значение» |	Redis, DynamoDB, Riak, RocksDB, Memcached
БД временных рядов |	InfluxDB, TimescaleDB, OpenTSDB, Prometheus, Druid
Потоковая обработка |	Apache Kafka, Apache Flink, Spark Streaming, Apache Storm, Google Cloud Dataflow
Полнотекстовый поиск |	Apache Solr, Elasticsearch, Algolia, OpenSearch, Sphinx
Очередь сообщений	| Apache Kafka, RabbitMQ, AWS SQS, Google Cloud Pub/Sub, NATS

#### Хранилище данных
*[КХД]: Корпоративное хранилище данных
*[ERP]: Enterprise Resource Planning
*[BI]: Business Intelligence
*[ETL]: Extract, Transform, Load
*[OLTP]: On-Line Transaction Processing
*[ELT]: Extract, Load, Transform

В современном мире объем данных, которые генерируются компаниями, растет с каждым днем. Особенно для крупных организаций с множеством источников информации и множеством подразделений, анализ данных и доступ к необходимым данным становится ключевой составляющей успешного бизнеса. Однако, часто возникают проблемы с эффективностью использования этой информации. Реализовать Data driven подход в управлении невозможно без построения корпоративного хранилища данных (КХД).

##### Решения на основе данных
<dfn title="решения на основе данных">Решения на основе данных</dfn> (*data-driven approach* — управляемый данными подход) — это стратегия принятия решений в бизнесе и управлении, основанная не на интуиции или опыте, а на глубоком анализе фактических данных и метрик для повышения точности, снижения рисков и поиска точек роста. Он включает сбор, обработку, анализ данных и их использование для формирования планов действий, от маркетинга до управления продуктом.

**Основные принципы**:
- **Решения на основе фактов**: вместо «мнения самого высокооплачиваемого человека» (HiPPO) опираются на цифры.
- **Понимание пользователей**: анализ данных помогает понять, как пользователи взаимодействуют с продуктом, выявить проблемы и потребности.
- **Поиск точек роста**: данные указывают, куда вкладывать ресурсы, какие функции улучшать, а от каких отказываться.

**Как это работает**:
1. **Сбор данных**: использование систем аналитики (Google Analytics, Яндекс.Метрика) и других источников.
2. **Обработка и анализ**: применение инструментов для анализа больших объемов информации (Data Management).
3. **Формирование гипотез**: на основе анализа выдвигаются гипотезы о том, как улучшить показатели.
4. **Тестирование и внедрение**: гипотезы проверяются (например, A/B-тестирование) и внедряются, если подтвердились (пример с формой заявки).

**Ключевые элементы внедрения**:
- **Качественные данные**: важны точные и полные данные.
- **Инструменты**: системы хранения и анализа данных.
- **Культура**: обучение и вовлечение всех сотрудников, чтобы данные стали частью повседневной работы.
- **Методология**: Четкие правила работы с данными (Data Governance).

**Преимущества**:
- Высокая точность и скорость принятия решений.
- Снижение субъективности и рисков.
- Эффективное развитие продукта и бизнеса.

**Сложности**:
- Зависимость от качества данных.
- Требуются специальные знания для интерпретации.
- Необходимы затраты на ПО и персонал.

</details>

##### Планирование ресурсов предприятия

<dfn title="корпоративное хранилище данных">Корпоративное хранилище данных</dfn> (*Data Warehouse*, DWH) — централизованная система, которая собирает, очищает и объединяет исторические данные из разных источников (CRM, с сайта, из мобильного приложения, электронных таблиц (Excel) и ERP) в единое место для последующего анализа, формирования отчётов, тестирования гипотез и принятия обоснованных решений, не влияя при этом на производительность операционных систем.

ERP (enterprise resource planning) — это планирование ресурсов предприятия. В наиболее общем виде ERP можно определить как совокупность всех базовых бизнес-процессов, необходимых для управления компанией: финансы, управление персоналом, производство, цепочка поставок, услуги, закупки и многое другое. На самом базовом уровне ERP помогает эффективно управлять всеми этими процессами в интегрированной системе. 

<dfn title="ERP">ERP</dfn> — организационная стратегия интеграции производства и операций, управления трудовыми ресурсами, финансового менеджмента и управления активами, ориентированная на непрерывную балансировку и оптимизацию ресурсов предприятия посредством специализированного интегрированного пакета прикладного программного обеспечения, обеспечивающего общую модель данных и процессов для всех сфер деятельности.[^erp]

<dfn title="ERP-система">ERP-система</dfn> — конкретный программный пакет, реализующий стратегию ERP. По сути это центральная нервная система предприятия, которая обеспечивает автоматизацию, интеграцию и интеллектуальность, необходимые для эффективного выполнения всех повседневных бизнес-операций. Большинство или все данные организации должны храниться в системе ERP, чтобы обеспечить единый источник достоверной информации в масштабе всей компании.

Современные ERP-системы — это не что иное, как базовые, и они мало похожи на ERP-системы десятилетий назад. Теперь они поставляются в облаке и используют новейшие технологии, такие как искусственный интеллект (ИИ) и машинное обучение, для интеллектуальной автоматизации, повышения эффективности и мгновенного анализа в масштабе всей компании. [^810741]

##### Роль и функции КХД
В отличие от обычной базы, которая нужна для оперативной работы приложений, например чтобы быстро оформлять заказы или собирать данные пользователей, хранилище создаётся именно для аналитики. В нём данные очищаются, приводятся к единому формату и структурируются так, чтобы их было удобно анализировать. DWH структурирует данные, хранит их в течение долгого времени, обеспечивая быстрый доступ к ним для аналитики (OLAP), в отличие от операционных баз данных (OLTP).

Разработка и модернизация хранилища данных КХД позволяет решить проблему мгновенного доступа к необходимой информации. Использование передовых методологий и концепций Data Warehouse (DWH) по хранению данных с поддержкой историчности изменений гарантирует быстрый и удобный доступ к необходимым данным. Корпоративное хранилище данных при этом выполняет роль единого источника и информационного центра для всех участников вашего производственного процесса, позволяя существенно повысить эффективность и оперативность принятия решений.

**Ключевые функции DWH**
- **Интеграция**: объединение информации из множества разнородных источников в одну систему.
- **Очистка и трансформация** (ETL): обработка данных, удаление дубликатов, исправление ошибок, приведение к единому формату.
- **Хранение**: долгосрочное сохранение исторических данных для анализа трендов и закономерностей.
- **Анализ**: поддержка сложных запросов, создание отчётов, бизнес-аналитика (BI), самообслуживание пользователей.
- **Изоляция**: анализ данных не нагружает основные рабочие системы, работающие в реальном времени.

##### ETL и ELT

<dfn title="ETL">ETL</dfn> (Extract, Transform, Load — «Извлечение, Преобразование, Загрузка») — это процесс интеграции данных, который собирает информацию из разных источников, очищает, преобразует её в единый формат и загружает в централизованное хранилище (например, «склад данных» или DWH) для дальнейшего анализа, отчетности и поддержки бизнес-решений. Он позволяет объединять разнородные данные, делая их согласованными и пригодными для использования. 

**Три этапа ETL**
- **Извлечение** (*Extract*): сбор данных из различных источников, таких как базы данных, файлы, облачные сервисы, CRM, API.
- **Преобразование** (*Transform*): приведение данных к единому виду: очистка от ошибок, удаление дубликатов, агрегирование, пересчет показателей, применение бизнес-правил.
- **Загрузка** (*Load*): перенос подготовленных данных в целевую систему — хранилище данных, где они становятся доступными для анализа.

**Зачем нужен ETL**
- **Единая картина данных**: создает единый источник правды для всей компании.
- **Принятие решений**: предоставляет чистые, готовые данные для бизнес-аналитики.
- **Интеграция систем**: объединяет информацию из разных отделов и приложений.

<dfn title="ELT">ELT</dfn> (Extract, Load, Transform — Извлечение, Загрузка, Преобразование) — современный метод интеграции данных, противоположный ETL, где данные сначала загружаются в хранилище, а затем преобразуются по мере необходимости, используя мощь облачных хранилищ для гибкого анализа больших объемов.

При таком подходе данные извлекаются (Extract) из источников, сразу загружаются (Load) в целевую систему (например, хранилище данных), а затем преобразуются (Transform) уже внутри этой системы, когда это необходимо для анализа. Это дает быструю загрузку сырых данных, гибкость (преобразования по запросу), позволяет организовать эффективную работу с большими данными (Big Data) в облачных средах (озера данных, хранилища).

Таким образом, В ETL сначала идет преобразование данных, а потом загрузка в хранилище; в ELT — загрузка, а потом преобразование.

##### Построение хранилищ данных

Разработка хранилища данных DWH включает создание многослойной архитектуры, где каждый слой и каждая таблица выполняют строго определённую функцию. Внедрение начинается с создания концепции и охватывает создание слоёв: Operational Data Store (ODS) для хранения оперативных данных, Detail Data Store (DDS) для детализированных данных и сервисного слоя для построения витрин данных, бизнес-аналитики и аналитических отчетов.[^postroenie-hranilishh-dannyh] Для обеспечения масштабируемости системы следует использовать самые современные подходы.

Решения по DWH обычно включают комплексные механизмы управления качеством данных. Система объединяет данные из различных источников в исходном качестве, обеспечивая оперативное устранение ошибок и проверку целостности. Современные хранилища поддерживают автоматическую валидацию исходной информации и получения данных без необходимости ручного вмешательства.

Для построения DWH обычно используются масштабируемые решения, такие как Greenplum, ClickHouse и Hadoop, позволяющие обрабатывать большие объёмы структурированных и полуструктурированных данных. Платформа КХД позволяет интегрировать данные из разных источников и создавать агрегированные и сводные отчеты. Система хранит данные в различных форматах и обеспечивает эффективную работу с детализированной информацией для бизнес-аналитики.

Хранилище данных проектируется для работы с историческими данными и отслеживания изменений во времени. Управление данными включает автоматическое версионирование и архивирование, что позволяет проводить глубокий временной анализ для бизнес-аналитики.

##### Основныхе характеристики КХД
Качественные хранилища данных должны следующими характеристиками:

- **Универсальность и высокая производительность**

    Используемые методологии по хранению и организации данных должны позволять интегрировать данные из различных источников, включая баз данных различных типов, обладают невероятной производительностью, обеспечивая мгновенный доступ к большим объемам данных. Это особенно важно для крупных компаний с множеством источников информации, так как правильно организованное хранилище данных (КХД) позволяет обрабатывать данные как в пакетном так и потоковом режиме, что значительно ускоряет процесс принятия решений и повышает оперативность бизнес-процессов, обеспечивая их совместимость и доступность в едином формате.

- **Надежность и качество данных**

    При разработке хранилищ данных желательно внедрение инструментов автоматизации тестирования ETL/ELT-процессов а так же инструменты для маскирования данных, для обеспечения сохранности коммерческой тайны и персональных данных компании, гарантируя актуальные данные в системе.

- **Скорость**

    Используемые подходы должны позволять хранить и обрабатывать значительные объемы данных, как пакетно, так и в режиме реального времени, обеспечивая возможность быстро получать и обрабатывать поступающие в хранилище данные .

- **Простота использования**

    Архитектура разрабатываемых корпоративных хранилищ данных должна быть интуитивно понятна и легка в освоении, что позволяет сотрудникам компании быстро адаптироваться к использованию новой системы. Например, для организации хранения данных может быть использована методологию Data Vault 2.0

Платформа КХД должна обеспечивать полный цикл поддержки аналитических процессов для бизнес-аналитики: от сбора и трансформации данных до формирования отчётов и интерактивных дашбордов. КХД обычно поддерживает автоматическую оркестрацию ETL/ELT-процессов, построение витрин данных, и предоставление как детализированных, так и агрегированных представлений. Решение следует адаптировать под современные BI-инструменты, обеспечить соответствие требованиям корпоративной отчётности и современным тенденциям развития аналитических систем.

**Зачем DWH нужно бизнесу**
- Получение единого источника "правды" о компании.
- Снижение издержек на ручной сбор и интеграцию данных.
- Улучшение качества принимаемых решений.
- Понимание динамики бизнеса, выявление причинно-следственных связей (например, как маркетинг повлиял на продажи).

#### Озеро данных
<dfn title="озеро данных">Озеро данных</dfn> (*Data Lake*) — это логическая совокупность репозиториев данных, предназначенных для хранения и анализа больших данных в их исходном формате. В отличие от традиционного понимания централизованного хранилища, Data Lake может быть распределенным по множеству физических местоположений, включая облачные платформы, on-premises инфраструктуру или гибридные среды. Концепция озера данных представляет собой эволюционный подход к управлению корпоративными данными, обеспечивающий гибкость, масштабируемость и экономическую эффективность в условиях постоянно растущих объемов информации.[^data-lake]

<details>
<summary>Об on-premise и on-cloud решениях</summary>

**On-premise** – это метод развертывания программного обеспечения или приложений, которые устанавливаются и функционируют на собственных серверах и инфраструктуре компании-заказчика. **On-cloud** (облачные) решения предполагают, что доступ к программному обеспечению осуществляется через серверы стороннего поставщика.[^on-premise-i-on-cloud-v-chem-raznitsa-i-chto-vybrat]

<dfn title="on-premise">On-premise</dfn> (также “on-premises” и “on-prem”, в переводе с англ. — локальный) в контексте компаний означает модель развертывания программного обеспечения и инфраструктуры, когда все ресурсы, включая оборудование и данные, находятся внутри компании, на её собственных серверах, а не в облаке. Локальное решение относится к программной или технологической инфраструктуре, которая устанавливается и обслуживается на собственных серверах или оборудовании компании. Локальные решения считаются более безопасными, поскольку компания имеет полный контроль над инфраструктурой, но они также могут быть более дорогостоящими и трудоемкими в управлении по сравнению с облачными или веб-решениями.[^on-premise-i-on-cloud-v-chem-raznitsa-i-chto-vybrat] В отличие от облачных решений, где компания арендует ресурсы у стороннего провайдера, on-premise модель предполагает полный контроль и ответственность за инфраструктуру со стороны компании.[^chto-znachit-on-premise]

<dfn title="облачное решение">Облачное решение</dfn> (англ. *on-cloud*) представляет собой программное обеспечение или услугу, предоставляемую через интернет или сеть удаленных серверов. Подобное решение позволяет компаниям получать доступ и использовать программное обеспечение или сервис из любого места, где есть подключение к интернету, поскольку данные и приложения размещаются на удаленных серверах, а не на собственном компьютере пользователя или на локальных серверах. Облачные решения часто обеспечивают такие преимущества, как масштабируемость, доступность и экономичность, поскольку они не требуют поддержки аппаратной или программной инфраструктуры. В качестве примеров можно вспомнить такие популярные облачные решения, как 1С Облако, Trello, Figma, «Битрикс24» и пр.[^on-premise-i-on-cloud-v-chem-raznitsa-i-chto-vybrat]

</details>

<dfn title="data lake">Data Lake</dfn> или Озеро данных — технология для получения и управления данными в разных форматах: в необработанном, неупорядоченном или, наоборот, структурированном или слабоструктурированном виде, в едином репозитории. Данные, которые можно хранить в озере:[^datalake]

Тип данных |	Примеры
-- | --
Неструктурированные |	Текстовые документы, медицинские данные, изображения и видео
Слабоструктурированные |	Файлы в формате xml, edi, json и лог-файлы
Структурированные |	Строки и столбцы реляционных БД, таблицы Excel

##### Применение Data Lake
Термин придуман в 2010-м году основателем компании Pentaho Джеймсом Диксоном. Описывая концепцию, он сравнил Data Lake и Data Mart. Витрины данных похожи на бутилированную воду — очищенную и упакованную. Озера данных — это открытые водоемы, в которые вода стекается из различных источников. В водоемы можно погружаться, а можно брать образцы с поверхности.

Так, озера данных удобны для сбора, хранения и обработки больших потоков информации, которая поступает непрерывно. Если грамотно их использовать, они станут надежным инструментом для следующих отраслей:

- **Телекоммуникации**. Озера данных часто применяются для хранения и анализа данных о клиентах, трафике, сетевых устройствах и других факторах, которые влияют на бизнес телекоммуникационных компаний.
- **Нефтегазовая промышленность**. Data Lake собирают терабайты данных и используют их в прогнозных моделях для разведки месторождений, управления цепочками поставок и техническим обслуживанием.
- **Медицина**. База данных о пациентах, их диагнозах и способах лечения может быть использована для автоматизации диагностики.
- **Розничная торговля**. Data lake позволяет хранить и анализировать данные о продажах, клиентах, инвентаре и других факторах, которые важны для розничных компаний.
- **Организации, занимающиеся финансами, страхованием, логистикой, закупками** — любой бизнес, который обрабатывает огромные объемы данных, может извлечь выгоду из использования Data Lake.

##### Примеры использования Data Lake
- **Омниканальный маркетинг**.

    Приложения для смартфонов нередко собирают информацию о действиях пользователя, а озера данных позволяют оперативно ее получать. На основе этой информации маркетологи могут делать специальные предложения или персонализированные скидки. Так, стриминговый сервис Netflix, Inc. с помощью Data Lake [получает](https://www.protocol.com/enterprise/how-netflix-and-uber-helped-create-the-data-lakehouse-by-preserving-an-open-source-tradition) данные о просмотренных пользователями фильмах и сериалах.

- **Цифровая цепочка поставок**.

    В цифровой цепочке поставок часто необходимо собирать большие объемы данных разного формата. Например, информация из цеха, отчеты о доставке и оплате. Благодаря Data Lake производитель может объединить их. Одним из первых пользователей Data Lake в промышленности [стал](https://www.ge.com/news/press-releases/ge-announces-first-data-lake-approach-industrial-internet-better-access-analyze-and) международный производитель техники General Electric.

- **Интернет вещей**.

    [Интернет вещей](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82_%D0%B2%D0%B5%D1%89%D0%B5%D0%B9) непрерывно расширяется за счет многочисленных датчиков, которые устанавливаются на транспортные средства. С помощью них отслеживается передвижение транспорта, безопасность его эксплуатации, расход топлива и другое. Озера данных популярны для сбора такой информации, поскольку не требуют ее структурирования. Например, сервис доставки и такси Uber с помощью Data Lake [следит](https://www.protocol.com/enterprise/how-netflix-and-uber-helped-create-the-data-lakehouse-by-preserving-an-open-source-tradition) за своими автомобилями.

##### Как устроено озеро данных
Озеро данных можно рассматривать как шаблон проектирования — формализованные рекомендации, которые можно использовать для решения распространенных проблем при разработке инфраструктуры. Четыре основные особенности устройства Data Lake:

- Данные необработаны либо слабообработаны.
- Большой срок хранения данных.
- Есть возможность преобразования данных.
- Поддерживаются разные схемы чтения данных.

Файлы в Data Lake хранятся на нескольких серверах, куда поступают из таких источников, как CRM-системы, социальные сети, интернет-магазины, датчики на производстве и прочих. Поступающей в озеро информации присваиваются метаданные: время поступления, источник, формат, структура и другое.

Все это может использоваться для извлечения данных в будущем, чтобы провести аналитику или обучить чему-то искусственный интеллект. Способы организации Data Lake могут включать HDFS, S3, Data Vault и распределенные файловые системы. Для размещения озера данных можно использовать как локальное хранилище, так и облачное.

##### Отличия Data Lake от хранилища данных
У озера данных и хранилища данных есть существенные различия, которые надо учитывать при выборе способа хранения информации:

| Область сравнения     | Data Lake                                    | Хранилище данных                |
| --------------------- | -------------------------------------------- | ------------------------------- |
| Сбор данных           | Данные любой структуры и из любых источников | Данные приведены к единому виду |
| Обработка данных      | Осуществляется после сбора                   | Осуществляется перед сбором     |
| Основные пользователи | Специалисты по глубокому анализу данных      | Оперативные пользователи        |
| Стоимость хранения    | Ниже                                         | Выше                            |
| Получение данных      | Высокая скорость получения                   | Низкая скорость получения       |

##### Преимущества Data Lake
- **Гибкие варианты использования**.

    При использовании озер данных не нужно заранее знать, как их необходимо будет анализировать. Например, данные из одного и того же озера можно использовать для поиска совпадающих записей или удаления дублирующихся, преобразования данных для внешней интеграции, классификации и кластеризации или машинного обучения.

- **Снижение эксплуатационных расходов**.

    Традиционные хранилища данных для аналитики и систем поддержки принятия решений используются уже более 30 лет. Озера данных совмещают в себе лучшие открытые и бесплатные технологии, что позволяет сэкономить на сборе и обработке информации.

- **Быстрый доступ к данным**.

    Информация — это стратегический актив, на основе которого можно разрабатывать инновации. Data Lake позволяет быстрее получить данные и принять необходимые решения. Искусственный интеллект также зависит от больших объемов разнообразной информации, быстрый доступ к которой можно организовать с помощью озер данных.

- **Совместное использование**.

    Крупные организации традиционно работают в разрозненных группах, каждая из которых может использовать разные типы данных. Объединенное хранилище для них — отличное решение, чтобы наладить совместную работу между командами.

- **Масштабируется бесконечно**.

    Благодаря низкой стоимости Data Lake не имеют ограничений по размеру. Также озера данных способны масштабироваться горизонтально и вертикально, что позволяет обрабатывать вплоть до нескольких петабайт данных.

##### Недостатки Data Lake
Озера данных оптимизированы для высокой пропускной способности, но ради этого приходится жертвовать качеством данных:

- В Data Lake не требуется структурировать данные, поэтому их сложнее анализировать.
- Data Lake не имеет инструментов для интегрированного или целостного получения всех данных.
- Без квалифицированного контроля за озерами данных трудно гарантировать конфиденциальность и безопасность хранилища.
- Для сбора реляционных данных есть гораздо более удобные решения, чем Data Lake.
- Если управление озером организовано плохо, в нем быстро накапливаются большие объемы неконтролируемых, и, возможно, бесполезных данных. Для эффективной фильтрации данных и отсечения недостоверных источников требуется высокая квалификация.

##### Советы по использованию Data Lake
Для извлечения максимальной выгоды из озер данных нужно с умом подойти к их использованию. К счастью, технология уже обкатана многими компаниями, которые сформулировали основные правила работы с озерами. Можно выделить три основных момента, которые помогут развернуть озеро данных, избежав возможных проблем в будущем:

1. **Главное — сбор данных**.

    Прежде всего необходимо максимально конкретизировать, что именно собирать, потому что озеро может поместить в себя что угодно и превратиться в болото. Грамотная настройка источников информации и фильтров в дальнейшем сильно упростит анализ и поможет сэкономить.

2. **Максимальная детализация метаданных**.

    Данные в озерах часто неструктурированные или слабоструктурированные, но навести порядок в хранилище можно с помощью каталогизации и метаданных. Затраченные усилия непременно окупятся, когда придет время анализировать результаты.

3. **План уничтожения данных**.

    Распространенная ошибка при работе с большими объемами информации — отсутствие плана по избавлению от ненужной информации. Если до сбора данных в озеро их удастся правильно разметить, то это поможет не удалить что-то нужное вместе с мусором, а также избавит от проблем с различными регуляторами и соответствием [регламентам](https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%89%D0%B8%D0%B9_%D1%80%D0%B5%D0%B3%D0%BB%D0%B0%D0%BC%D0%B5%D0%BD%D1%82_%D0%BF%D0%BE_%D0%B7%D0%B0%D1%89%D0%B8%D1%82%D0%B5_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85).

#### Витрина данных
<dfn title="витрина данных">Витрина данных</dfn> (*data mart*) — база данных, предназначенная для решения специализированной задачи или набора задач из одной предметной области, например по поиску наименьшей цены товара, расчету загрузки производственных мощностей предприятия, организации тематических рассылок и т.п.

Альтернативная технология хранения данных — Data Lake или Озеро данных. Витрины данных похожи на бутилированную воду — очищенную и упакованную. Озера данных — это открытые водоемы, в которые вода стекается из различных источников.[^datamart]

##### Отличия витрины от хранилища данных
В отличие от хранилища, где размещаются большие объемы разнородной информации, требующей высокой скорости доступа, в витрине содержатся только однородные данные. К этим данным предъявляются высокие требования по достоверности и оперативности обновления.

| Характеристика                | Хранилище данных | Витрина данных       |
| ----------------------------- | ---------------- | -------------------- |
| Тип хранимой информации       | Разнородная      | Однородная           |
| Объем хранимой информации     | Большой          | Средний или малый    |
| Скорость доступа к информации | Средняя          | Высокая              |
| Достоверность информации      | Средняя          | Высокая              |
| Период обновления информации  | Произвольный     | Минимально возможный |

##### Преимущества Data Mart
- Время и стоимость создания витрины намного меньше затрат на создание хранилища.
- Данные для витрины можно размещать на разных аппаратно-программных комплексах и собирать по заранее настроенным запросам пользователя.
- Так как витрина представляет специализированную однородную информацию, ее может настроить один специалист в предметной области.
- Витрина обеспечивает быстрый доступ к необходимым пользователю сведениям.
- Благодаря относительно небольшому объему данных сокращается время анализа информации на витрине.
- За счет простой внутренней структуры витрину можно быстро перестроить в случае изменения информационной модели или задач пользователя.
- Так как решения Data Mart подразумевают деление данных по предметным областям, витрина позволяет гибко настраивать права доступа пользователей к информации.

##### Типы Data Mart
Витрины данных бывают зависимыми, независимыми и гибридными.

- **Зависимая витрина данных**

    Данные для витрин собираются централизованно через хранилище данных. С помощью специализированных наборов запросов информация из хранилища перегружается в витрины с разными назначениями. Преимущество этого типа Data Mart — единый источник информации и простое обновление данных на витринах по запросам к хранилищу. Основное условие успешной работы — наличие хранилища с заранее верифицированными данными.

- **Независимая витрина данных**

    Данные для витрин не хранятся, а собираются по запросам из разнородных источников (например, сети интернет), что уменьшает затраты на создание и поддержание в актуальном состоянии баз данных предприятия. Для этого типа Data Mart сложнее получать актуальную информацию. Кроме того, полученные данные требуют дополнительного анализа и верификации. Независимая витрина подходит для небольших проектов, ограниченных временными и бюджетными рамками.

- **Гибридная витрина данных**

    Сочетает преимущества обоих предыдущих типов. Наличие хранилища позволяет автоматизировать обновление витрин и увеличить скорость доступа к актуальным данным. Запросы ко внешним источникам обеспечивают отсутствие жесткой привязки к одному источнику данных и гибкость получаемой информации.

##### Применение витрин данных
Витрины данных позволяют оптимизировать работу отдельных пользователей или подразделений. Важно понимать, что Data Mart не предназначены для решения задач комплексной цифровизации бизнес-процессов.

Витрины обеспечивают:

- **Единство хранящихся данных**.

    Поскольку все представляемые данные выбираются из единого хранилища, это гарантирует непротиворечивость информации для всех пользователей и подразделений. Отсутствуют потери времени на дополнительную верификацию и сопряжение данных.

- **Высокую скорость доступа к информации**.

    Пользователи получают доступ к нужному подразделу информации с помощью заранее подготовленных запросов к единому хранилищу и автоматических механизмов выгрузки данных в специализированные СУБД — витрины данных. Это экономит время сотрудников, так как не требует постоянного доступа к центральному хранилищу и помощи квалифицированных ИТ-специалистов. Вся информация доступна для назначенных витрин и прав пользователей по мере необходимости.

- **Быстрое принятие решений на основе хранящейся статистики**.

    Витрина данных предоставляет доступ ко всей необходимой аналитике в объеме назначенных прав пользователей и потребностей подразделений. Статистические данные, собранные в едином хранилище для решения других задач, не мешают анализировать информацию по выбранной витрине.

- **Простое создание и применение**.

    Так как витрина данных настраивается на обслуживание задач конкретного направления, подразделения или пользователя, создание витрин проще и быстрее, чем создание единой аналитической базы данных. Для работы витрины достаточно наличия хранилища неструктурированных данных и запросов по выборке интересующей нас информации из этого хранилища.

- **Гибкость и масштабирование решений**.

    Поскольку представляемые данные изначально собираются не в витрины, а в хранилища данных, для перенастройки требуемой аналитики достаточно изменить запросы по выборке информации.

    Лучшей аналогией этого процесса является куб (хранилище данных), у которого мы можем увидеть любую грань (витрину) по нашему выбору.

- **Анализ динамики получаемой информации**.

    Для анализа изменения данных достаточно добавить к запросам по выборке информации при построении витрины еще один срез — время. Поскольку данные поступают в единое хранилище постоянно, это не требует перестройки работы хранилища в целом.

При создании Data Mart важно не попасть в другую крайность. Множество разнородных независимых витрин для разных пользователей в конечном итоге приводят к трудностям актуализации информации и снижают достоверность данных. При необходимости предоставления инструментария Data Mart для различных областей в рамках одного предприятия необходимо планировать разработку зависимых решений на основе единого хранилища данных.

Идеальной средой разработки подобных решений является облачное хранилище.[^datamart]

##### Реализация Data Mart
Чтобы успешно решать аналитические задачи с использованием зависимых витрин, необходимо реализовать следующие этапы:

1. Сбор и подготовка данных.
2. Организация многоуровневого хранения данных.
3. Создание удобных инструментов для анализа данных.

#### OLTP
*[СРВ]: Система реального времени

<dfn title="OLTP">OLTP</dfn> (*OnLine Transaction Processing*) — система обработки транзакций в реальном времени.

<dfn title="транзакция">Транзакция</dfn> (*transaction*) — набор запросов, который выполняется в единой очереди. При неудачном или ошибочном завершении любого из запросов отменяется результат работы всего набора запросов.

<dfn title="реальное время">Реальное время</dfn> — режим работы системы, когда важны не только правильность обработки и предоставления информации, но и своевременность ее обработки. Информация в системах реального времени (СРВ) должна быть обработана за определенное фиксированное время, либо до наступления какого-либо контрольного события.[^oltp]

##### Как работает OLTP
OLTP-система обеспечивает непрерывную запись информации в базу данных. Это могут быть сигналы с устройств автоматического сбора данных (например, датчиков технологического процесса), данные о денежных переводах клиентов банка и т.п.

Одновременно OLTP обслуживает выполнение запросов от других систем на чтение данных из базы: от информации об удачном или неудачном завершении определенной транзакции до выборки информации для аналитических отчетов.

Все входящие (на запись) и исходящие (на чтение) запросы инициируются различными пользователями, внешними системами и алгоритмами и выполняются условно параллельно.

##### Особенности OLTP
В отличие от OLAP-систем (OnLine Analytical Processing), OLTP не предусматривает работу со сложной аналитикой, так как:

- база данных OLTP одномерна;
- в базе отсутствует агрегированная (сводная) информация для анализа;
- OLTP-система не оптимизирована для выполнения вложенных запросов на выборку данных из разных таблиц.

Если требуется получить и проанализировать срез данных из OLTP, необходимо:

1. Запустить запрос на чтение из базы данных по интересующему нас фильтру (периоду, объекту и т.п.).
2. Выгрузить результат запроса для автоматизированного анализа в специализированную систему, например OLAP, либо в файл электронной таблицы (*xls* и т.п.) для ручного анализа.

Ориентированность OLTP на высокую скорость обработки простых транзакций имеет и свои слабые стороны:

- **Низкая производительность при анализе больших объемов информации** (статистические расчеты, Big Data).

    Каждая транзакция в системе для обеспечения принципа изолированности требует выделения отдельных ресурсов. При значительном количестве подобных транзакций (от миллиона и выше) это приведет к постоянным блокировкам записей и таблиц базы данных и общему снижению производительности системы.

- **Сложность создания аналитических запросов для выборки данных**.

    Высокая степень нормализации OLTP базы данных требует соответствующей высокой вложенности выражений в запросе. К тому же наименования объектов в базе не являются мнемоничными или интуитивно понятными. Пользователю для создания запроса необходимо привлечение разработчика системы или администратора базы данных.

- **Снижение производительности системы при запросах к таблицам с избыточным количеством полей и записей**.

    Это связано с тем, что структура реляционной базы данных плохо адаптирована для хранения большого объема информации в одной таблице. Решением проблемы являются изменение архитектуры системы (применение вместо одной большой сводной таблицы с данными нескольких связанных между собой по ключевым полям таблиц) или перенос архивных данных в системы других типов (например, в хранилище или киоск данных).

##### Требования к OLTP
*[ACID]: Atomicity, Consistency, Isolation, Durability

Основные требования к OLTP-системам:

1. **Достоверность информации**. Вся хранящаяся информация должна подчиняться принципу ACID:

      - **Atomicity** — атомарность или неделимость информации.

        Каждая транзакция в системе или завершается успешно, или полностью откатывается к исходным значениям.

      - **Consistency** — консистентность или согласованность.

        Информация не берется из ниоткуда и не исчезает в никуда, а просто передается от источника информации к ее приемнику.

      - **Isolation** — изолированность или блокировка.

        Для обеспечения достоверности информации действия пользователя (чтение или запись) должны быть изолированы от действий других пользователей.

      - **Durability** — устойчивость.

        Успешно проведенная транзакция должна быть защищена от любого внешнего сбоя, как программного, так и аппаратного.

2. **Высокая скорость работы с информацией** (как чтение, так и запись в базу данных). Идеальным вариантом является работа системы полностью из оперативной памяти с периодическим сохранением информации на постоянный носитель.

3. **Легкая масштабируемость**. OLTP система должна строиться по принципу нормализации, когда исключается избыточное хранение данных. Данные хранятся только в одном месте (ячейке таблицы), прочие таблицы при необходимости используют ссылки на эти ячейки без дублирования данных.

При этом требований к хранимой структуре данных не предъявляется, поэтому такая система не предназначена для сложного аналитического учета, который требуется, например, от OLAP.

Под эти условия работы попадают системы, построенные на реляционных базах данных.

##### Где применяется OLTP
*[SCADA]: Supervisory Control and Data Acquisition
*[ПЛК]: Программируемый логический контроллер
*[IIoT]: Industrial Internet of Things
*[HMI]: Human-Machine Interface

OLTP используется везде, где необходим доступ в реальном времени к большим объемам однотипной информации:

- в автоматизированных системах управления технологическими процессами (SCADA) при оперативной обработке сигналов, поступающих с различных датчиков;
- в банковском секторе при обработке платежных транзакций;
- в ERP-системах предприятий при работе алгоритмов адресного складского хранения;
- в онлайн-магазинах и электронных торговых площадках при работе с заказами и лотами.

<dfn title="SCADA">SCADA</dfn> (*Supervisory Control and Data Acquisition*) — это программно-аппаратный комплекс для **диспетчерского управления и сбора данных** в реальном времени, используемый для мониторинга, контроля и автоматизации технологических процессов на промышленных объектах, от энергетики и ЖКХ до производства. Системы SCADA собирают данные с датчиков, отображают их в понятном виде (человеко-машинный интерфейс), позволяют оператору управлять оборудованием удаленно, сигнализируют о неполадках, повышают точность процессов и снижают брак.

**Ключевые функции SCADA**:
- **Сбор и обработка данных**: получение информации (температура, давление, уровень) с оборудования и ее агрегация.
- **Диспетчерский контроль**: предоставление оператору единой картины процесса для принятия решений.
- **Удаленное управление**: возможность дистанционно воздействовать на оборудование.
- **Мониторинг и сигнализация**: отслеживание отклонений, оповещение об аварийных ситуациях.
- **Анализ и отчетность**: формирование отчетов для оценки качества и эффективности.

**Как это работает**:
1. **Сбор данных**: датчики и контроллеры (ПЛК) собирают информацию с объектов. ПЛК (<dfn title="программируемый логический контроллер">программируемый логический контроллер</dfn>) — специализированный промышленный микрокомпьютер для автоматизации процессов, надежно работающий в жестких условиях. Читает данные с датчиков, анализирует их по программе и управляет исполнительными механизмами (двигатели, насосы, клапаны). Создан для работы в режиме реального времени в суровых промышленных условиях, в отличие от обычных компьютеров, ориентированных на пользователя. Используется для управления станками, производственными линиями, системами вентиляции, освещением, насосами. Интегрируются в IIoT (промышленный интернет вещей) для сбора данных и предиктивного обслуживания оборудования.

2. **Передача данных**: данные передаются по сети на серверы.

3. **Обработка и отображение**: компьютеры с SCADA-ПО обрабатывают данные и выводят их на экраны операторов (HMI). <dfn title="HMI">HMI</dfn> (*Human-Machine Interface*, *человеко-машинный интерфейс*) — это устройство или программный комплекс, обеспечивающий взаимодействие человека с машиной, компьютером или процессом, чаще всего в промышленности, позволяя оператору наблюдать, контролировать и управлять оборудованием через визуальные экраны, кнопки и сенсорные панели для мониторинга данных и команд в реальном времени. HMI-панели не требуют подключения к ПК и напрямую связываются с PLC (программируемыми логическими контроллерами) для визуализации процессов, сбора данных и управления производственными операциями, повышая эффективность автоматизации. В итоге, HMI выступает как "лицо" промышленной системы, делая сложный технологический процесс доступным для оператора.

4. **Управление**: оператор видит ситуацию и может давать команды, которые выполняются через те же контроллеры.

##### Преимущества и недостатки OLTP
OLTP-системы отлично подходят для управления операциями в реальном времени благодаря высокой скорости и надежности обработки тысяч мелких транзакций (вставки, обновления, удаления) с мгновенным откликом, гарантируя целостность данных (ACID), но недостаток в их неэффективности для сложных аналитических запросов (OLAP), которые могут замедлить всю систему и требуют выгрузки данных в отдельные хранилища для анализа, так как они не оптимизированы для чтения больших объемов агрегированной информации.

**Преимущества OLTP**
- **Высокая скорость и отзывчивость**: мгновенная обработка транзакций (миллисекунды) для оперативных задач (оплата, бронирование).
- **Масштабируемость**: легко адаптируются к росту числа пользователей и объема транзакций.
- **Целостность данных** (ACID): обеспечивают надежность операций и согласованность данных даже при сбоях.
- **Высокая доступность**: минимизируют время простоя, что критично для систем реального времени.
- **Простота операций**: оптимизированы для простых, частых операций (INSERT, UPDATE, DELETE).
- **Параллельный доступ**: эффективно обрабатывают одновременные запросы от множества пользователей.

**Недостатки OLTP**
- **Плохо подходят для аналитики** (OLAP): cложные аналитические запросы «тормозят» систему, так как требуют сканирования больших объемов данных и сложных соединений таблиц.
- **Не хранят агрегаты**: база одномерна и не содержит сводной информации для анализа.
- **Сложность баланса**: сложно найти баланс между скоростью транзакций и потребностями аналитики.
- **Нагрузка на производительность**: большой объем исторических данных может замедлять работу, требуя выноса в DWH.

Таким образом, OLTP — идеальный выбор для фронт-офисных приложений, ритейла, финансов, логистики, где важна оперативная обработка каждой транзакции в реальном времени, а не глубокий анализ.

##### Популярные системы и платформы
Популярные OLTP-системы — это реляционные СУБД, ориентированные на быстрые транзакции: Oracle Database, MS SQL Server, PostgreSQL, MySQL, а также облачные решения вроде Amazon RDS, Azure SQL Database, Yandex Managed Service for YDB, обеспечивающие масштабируемость и доступность. Они используются везде, где важна моментальная обработка множества мелких операций: банки, e-commerce, логистика, мобильные приложения, управление запасами.

**Классические реляционные OLTP-системы** (on-premise и облако)
- **Oracle Database**: мощное решение для критически важных систем.
- **Microsoft SQL Server**: широко используется, включая In-Memory OLTP для ускорения транзакций.
- **PostgreSQL**: гибкая и мощная СУБД с открытым исходным кодом.
- **MySQL**: популярна для веб-приложений.
- **IBM DB2**: еще одна классическая корпоративная СУБД.

**Облачные и распределенные OLTP-платформы** (Managed Services)
- **Amazon RDS/Aurora**: управляемые версии PostgreSQL, MySQL, SQL Server и др..
- **Azure SQL Database**: облачный аналог SQL Server.
- **Yandex Managed Service for YDB**: распределенная отказоустойчивая база данных с поддержкой ACID и SQL-подобным языком YQL, отличная для масштабируемых онлайн-сервисов.
- **DynamoDB** (AWS): NoSQL решение от Amazon для высокой производительности при больших нагрузках, схожее с документо-ориентированными базами данных.

**Новые подходы и NoSQL-решения** (часто используются в гибридных сценариях)
- MongoDB, Cassandra, Redis, Tarantool: предлагают альтернативы для специфических задач, не всегда строгие к ACID, но удобные для определенных нагрузок и масштабирования, часто в связке с классическими OLTP.

**Примеры применения**
- **Банкинг**: онлайн-платежи, банкоматы, учет операций.
- **E-commerce**: обработка заказов, корзины, управление товарами.
- **Телекоммуникации**: биллинг, управление звонками.
- **Логистика**: учет запасов, бронирование билетов.
- **Здравоохранение**: электронные карты пациентов.

#### OLAP
<dfn>OLAP</dfn> (OnLine Analytical Processing) — оперативная аналитическая обработка данных или анализ данных в реальном времени. Термин впервые использовал создатель теории реляционных баз данных Эдгар Кодд в 1993 г. в статье «OLAP для пользователей-аналитиков: каким он должен быть».[^olap]

##### Суть технологии
Бизнес-данные разной формы собирают из множества источников. При этом их объем настолько большой, что без применения автоматических систем его не проанализировать. Тогда на помощь приходят аналитические системы OLAP. Они обрабатывают огромные массивы данных с учетом множества критериев и максимально быстро предоставляют нужную информацию из разнородных хранилищ. Благодаря этому удается оперативно принимать важные бизнес-решения.

Технология объединяет хранилища данных, сервер, управляющий массивами данных, а также программное обеспечение для визуализации аналитической информации и составления отчетов. Сервер группирует данные в многомерную иерархию, которая позволяет ему эффективно и быстро получать в наглядном виде необходимые сведения — с помощью динамической обработки сложных запросов.

##### Как работают OLAP-системы
OLAP состоит из нескольких компонентов:

- Хранилище данных.
- Средства ETL.
- Сервер.
- Аналитические инструменты.

###### Хранение данных
*[MOLAP]: Multidimensional Online Analytical Processing
*[ROLAP]: Relational Online Analytical Processing
*[HOLAP]: Hybrid Online Analytical Processing

Данные появляются вне аналитической системы и имеют самую разную форму и представление. А для оперативной обработки структура данных должна быть оптимизирована под ее особые требования. Решение этой проблемы — специальные хранилища, где данные заранее импортированы из различных источников, а затем очищены, преобразованы в нужный формат и упорядочены по заданному принципу — в многомерные или классические реляционные базы данных.

1. **Многомерные хранилища**

    Такая система называется <dfn title="MOLAP">MOLAP</dfn> (*Multidimensional Online Analytical Processing*) — технология анализа данных, которая хранит и обрабатывает данные в виде многомерных кубов (массивов) для очень быстрого выполнения сложных аналитических запросов, агрегируя и оптимизируя их заранее, что обеспечивает высокую производительность для бизнес-аналитики, но требует больше ресурсов и сложнее обновляется при больших изменениях данных.

    **Как это работает**
    - **Многомерные кубы**: для хранения строится OLAP-куб — многомерный массив данных, упорядоченный по измерениям или категориям. Данные хранятся в структуре, похожей на куб, где измерения (например, "Продукт", "Время", "Регион") определяют оси, а значения (например, "Продажи") находятся внутри ячеек.

    - **Предварительная агрегация**: с помощью кубов создаются информативные сводные таблицы. В центре куба расположена двумерная таблица фактов, которые характеризуют взаимодействие элементов из разных измерений. Многие сводные данные вычисляются заранее (кэшируются), что позволяет мгновенно получать срезы и детализировать информацию.

    - **Хранение**: Используется специализированное хранилище многомерных массивов, а не реляционная база данных, что ускоряет доступ.

    ![Multidimensional storage](../svg/multidimensional-storage.svg)

    MOLAP — cамый быстрый вид аналитических систем: сервер напрямую извлекает из куба меры, которые соответствуют поступающим запросам.

    **Преимущества**
    - **Высокая скорость**: быстрое выполнение запросов благодаря предварительной обработке.
    - **Мощная аналитика**: отлично подходит для сложных вычислений и анализа данных под разными углами.
    - **Естественная модель**: многомерная модель интуитивно понятна для анализа бизнеса.

    Однако такая база данных может очень сильно разрастаться и занимать значительный объем диска и оперативной памяти, что повышает нагрузку на всю систему. MOLAP обычно поддерживает однопользовательский режим записи и многопользовательский режим чтения.

    **Недостатки**
    - **Масштабируемость**: может быть неэффективен при работе с очень большими и постоянно меняющимися объемами данных.
    - **Сложность обновления**: изменение структуры или данных часто требует полной или частичной переагрегации кубов.
    - **Стоимость**: требует инвестиций в специализированное ПО и оборудование.

    **Где используется**
    - **Бизнес-аналитика** (*Business Intelligence*): Для формирования отчетов, анализа продаж, логистики, финансов.

    **Инструменты**: Microsoft Analysis Services, Oracle OLAP, Excel (для небольших объемов).

2. **Реляционные хранилища**

    Такая система называется <dfn title="ROLAP">ROLAP</dfn> (*Relational Online Analytical Processing*) — это подход к аналитической обработке данных, который использует традиционные реляционные базы данных (РБД) и язык SQL для многомерного анализа больших объёмов информации, в отличие от MOLAP, где данные хранятся в многомерных кубах. Он позволяет пользователям выполнять сложные запросы к данным в реальном времени, извлекая срезы данных напрямую из хранилищ (часто в схемах "звезда" или "снежинка"), что обеспечивает гибкость и масштабируемость.

    **Как работает ROLAP**
    - **Хранение данных**: Используются стандартные реляционные таблицы (факты и измерения) в СУБД. Реализуется в виде простого хранилища: данные нормализованы по множеству взаимосвязанных таблиц.
    - **Анализ**: При запросе пользователя ROLAP-сервер генерирует сложные SQL-запросы к базе данных.
    - **Результат**: Данные обрабатываются и возвращаются пользователю, часто через кэш для ускорения.

    Технология позволяет интегрировать систему OLAP с уже используемой, например, учетной системой.

    **Преимущества ROLAP**
    - **Масштабируемость**: Отлично подходит для больших объемов данных.
    - **Гибкость**: Использует возможности реляционных СУБД.
    - **Актуальность данных**: Обеспечивает анализ "свежих" данных, так как не требует предварительного построения кубов.
    - **Интеграция**: Легко интегрируется с существующими инструментами SQL и BI.

    Недостаток системы ROLAP заключается в ее медлительности — ведь структура данных в хранилище не оптимизирована для OLAP. Система *ROLAP гораздо лучше масштабируется и способна анализировать обширные и подробные данные.

    **Недостатки ROLAP**
    - **Производительность**: Может быть медленнее, чем MOLAP, из-за сложности SQL-запросов.
    - **Сложность**: Требует оптимизации для высокой производительности при сложных запросах.

    **MOLAP vs ROLAP**
    - **MOLAP**: хранит данные в многомерном кубе (быстрый, но ресурсоемкий).
    - **ROLAP** (Relational OLAP): работает непосредственно с реляционной базой данных (более гибкий, но медленнее).

3. **Гибридные решения**

    Самая распространенная система OLAP — HOLAP. <dfn title="HOLAP">HOLAP</dfn> (*Hybrid Online Analytical Processing*) — это технология гибридной аналитической обработки данных, которая сочетает лучшие черты MOLAP (многомерного OLAP) и ROLAP (реляционного OLAP), храня агрегированные данные в многомерных кубах для быстрого анализа, а исходные данные — в реляционных базах, что обеспечивает гибкость и масштабируемость для сложных запросов и детализации (drill-down) до первичного уровня. Эта модель позволяет быстро получать сводные отчеты (как MOLAP) и одновременно обращаться к детальным данным (как ROLAP), что критически важно для бизнес-аналитики.

    **Как работает HOLAP**
    - **Хранение данных**: агрегированные, суммированные данные хранятся в многомерных кубах (как в MOLAP), а исходные, детальные данные остаются в реляционной базе данных (как в ROLAP).

    - **Обработка запросов**:
        - **Сводные запросы**: если запрос касается агрегированных данных (например, общие продажи за квартал), система быстро отвечает, используя данные из куба (MOLAP-эффект).
        - **Детальные запросы**: если требуется детализация до уровня отдельных транзакций, система обращается к исходным данным в реляционной базе (ROLAP-эффект).

    Система объединяет возможности MOLAP и ROLAP: основные подробные данные находятся в реляционной базе данных, а множество предварительно рассчитанных мер — в кубе. Система обеспечивает самые оптимальные и эффективные решения.

    **Преимущества HOLAP**
    - **Скорость и гибкость**: Быстрый доступ к агрегатам и возможность детализации, чего не могут обеспечить чистые MOLAP или ROLAP.
    - **Масштабируемость**: Экономит место по сравнению с чистым MOLAP, так как не дублирует все исходные данные в кубе.
    - **Реальное время**: Обеспечивает актуальность данных благодаря прямому доступу к RDBMS.

    **Когда использовать HOLAP**
    - Для кубов, требующих быстрого ответа на сводные запросы, но при этом нуждающихся в детализации до уровня исходных данных.
    - Когда необходимо сочетать производительность многомерных моделей с полнотой данных реляционных систем.

###### Средства ETL

<dfn title="ETL">ETL</dfn> (Extract, Transform, Load) — это средства извлечения, преобразования и загрузки данных, которые позволяют хранилищу взаимодействовать с окружающим миром. С их помощью данные попадают в хранилище, обрабатываются и конвертируются в нужный формат, выгружаются на сервер для аналитической обработки и возвращаются обратно для просмотра результатов.

###### Сервер
Ядром системы является сервер — именно он забирает данные из источников, преобразует их с помощью средств ETL, выполняет предварительные расчеты, обрабатывает запросы клиента и руководит работой всей инфраструктуры OLAP.

###### Аналитические инструменты
<dfn title="аналитические инструменты">Аналитические инструменты</dfn> — это программы и приложения, которые выступают в роли посредников между пользователем и хранилищем данных. С их помощью аналитики формируют запросы к базам данных и отчеты с наглядным представлением важной для бизнеса информации.

##### Требования к OLAP-системам
*[FASMI]: Fast Analysis of Shared Multidimensional Information

Вместе с понятием аналитической системы OLAP Эдгар Кодд предложил 12 критериев, по которым система признается таковой. Однако, по мнению некоторых специалистов, его подход оказался недостаточно конкретизированным, поэтому через некоторое время были предложены новые критерии, суть которых раскрывает аббревиатура FASMI — Fast Analysis of Shared Multidimensional Information, то есть быстрый анализ доступной многомерной информации. К аналитическим системам OLAP выдвигается пять основных требований:

1. **Скорость реакции** (*Fast*) — реакция системы должна быть быстрой: время между запросом и откликом не должно превышать пяти секунд. Это важно для оперативного представления информации в системах поддержки принятия решений, чтобы полученный результат строго соответствовал текущей ситуации.
2. **Аналитические возможности** (*Analysis*) — система должна выполнять любой логический, численный или статистический анализ, требуемый в рамках выполняемых задач, а также представлять и сохранять его результаты в доступном и наглядном виде.
3. **Доступность данных** (*Shared*) — данные в системе должны быть доступны множеству пользователей, но только в необходимом им объеме, который определяется механизмами разграничения прав или ролей.
4. **Многомерность представления** (*Multidimensional*) — данные должны быть представлены в многомерных иерархических структурах. Причем это должно отражаться не в физической структуре хранилища, а в логике пользовательских запросов. Это главное требование к системам OLAP.
5. **Релевантная информация** (*Information*) — система должна работать с нужными данными независимо от их расположения и объема. При этом наличие посторонних нерелевантных приложению данных может негативно сказаться на быстродействии и эффективности всей системы.

##### Чем отличаются технологии OLAP и OLTP

Технология | Описание | Назначение
-- | -- | --
**OLTP** | Технология обработки транзакций в реальном времени. Она обеспечивает непрерывное занесение данных в базу, их модификацию и извлечение. Данные размещаются в простых таблицах и, как следствие, занимают не так много места в хранилище. Но главное: OLTP-системы не предназначены для комплексного анализа данных, скорее, это инструмент их массового сбора и преобразования. | Пользователи приложений, где происходит обмен данными
**OLAP**| OLAP-системы хранят неизменные многомерные исторические данные и данные транзакций (зачастую собранные системами OLTP), позволяя получать по ним сложную аналитику. | Аналитики данных, которые собираются в приложениях

Во многих случаях эти технологии образуют тандем, который помогает бизнесу быстро анализировать происходящее и находить лучшие решения.[^olap]

##### Преимущества и недостатки
Преимущества технологии OLAP включают в себя: быстрое формирование отчетов, многомерный анализ данных, единое хранилище, гибкость для бизнес-пользователей и выявление скрытых зависимостей. Основные недостатки: высокая стоимость и сложность внедрения, потенциальные проблемы с масштабированием и «взрывом» данных, а также необходимость в специальных знаниях для нетехнических пользователей и сложности с неаддитивными агрегатами.

**Ключевые преимущества**
- **Прямой доступ к данным**: пользователь может извлечь и просмотреть любые существующие в базе данные без фильтрации и преобразования.
- **Единая платформа**: объединяяя данные из разных источников, OLAP позволяет создать централизованное хранилище для всех аналитических нужд. Поскольку все данные находятся в одном месте, OLAP позволяет реализовать общую платформу для поддержания всех аналитических процессов предприятия.
- **Неизменность данных**: огромные массивы данных в OLAP обработаны заранее и хранятся в неизменных специализированных архивах, что значительно ускоряет их комплексный анализ.
- **Гибкость и универсальность**: OLAP-системы отличаются особой гибкостью: конечные пользователи могут извлекать произвольные данные, формировать любые отчеты и выполнять самые разные аналитические операции. Бизнес-пользователи могут самостоятельно создавать отчеты и проводить анализ без постоянного привлечения IT-специалистов.
- **Быстрая детализация итоговых данных**: иерархическая многомерная структура хранения позволяет оперативно представлять результат анализа данных с нужной степенью детализации.
- **Скорость и производительность**: предварительно агрегированные и обработанные данные в многомерных кубах обеспечивают молниеносные ответы на сложные запросы и формирование отчетов. Все данные рассчитаны заранее, поэтому при составлении отчета нужно дождаться только вывода результатов в заданном виде.
- **Многомерный анализ**: предварительно агрегированные и обработанные данные в многомерных кубах обеспечивают молниеносные ответы на сложные запросы и формирование отчетов. Быстрый доступ к инсайтам помогает принимать более обоснованные и своевременные бизнес-решения.
- **Возможность выявления скрытых зависимостей**: многомерность данных помогает обнаружить неочевидное влияние одних производственных процессов на другие и выявлять неочевидные связи между различными бизнес-процессами.

**Недостатки OLAP**
- **Сложность и стоимость**: требует значительных начальных инвестиций в ПО и инфраструктуру, а также затрат на внедрение и обслуживание.
- **Сложность моделирования**: проектирование многомерных моделей данных может быть нетривиальной задачей, особенно при частых изменениях бизнес-логики.
- **Проблема «взрыва» данных**: хранение всех комбинаций данных может привести к экспоненциальному росту размера куба.
- **Ограниченная гибкость** (для некоторых реализаций): традиционные кубы могут быть негибкими при необходимости анализа новых, не предусмотренных заранее измерений.
- **Сложность для нетехнических пользователей**: несмотря на цели самообслуживания, освоение специализированных инструментов может быть барьером.
- **Проблемы с неаддитивными показателями**: сложности в корректной агрегации значений, смысл которых меняется при изменении уровня детализации (например, средняя зарплата).

##### Популярные системы и платформы
- **IBM Cognos**: комплексное решение для бизнес-аналитики (BI).
- **Oracle Essbase**: один из классических примеров многомерной (MOLAP) системы.
- **ClickHouse**, **Apache Doris**, **StarRocks**: современные Open-Source платформы с поддержкой федеративных запросов к данным в озёрах данных.
- **Vertica**: аналитическая база данных с функциями машинного обучения, часто используется в OLAP.
- **Greenplum**: масштабируемая база данных на основе PostgreSQL, применяемая для больших данных.

###### СУБД Greenplum
*[СУБД]: Система управления базами данных
*[MPP]: Massively Parallel Processing

После закрытия открытого доступа к Pivotal компаниям, работающим с большими объемами данных, необходимо искать новые решения для управления информацией. Одним из таких вариантов является Arenadata DB — платформа для работы с базами данных, предлагающая широкий функционал для оптимизации обработки данных и обеспечения их безопасности.

<dfn title="Greenplum Database">Greenplum Database</dfn> (Arenadata DB) — это массивно-параллельная СУБД (Система управления базами данных), предназначенная для обработки больших объемов данных и их анализа, превращая их в источник роста и конкурентных преимуществ СУБД Greenplum обеспечивает скорость, масштабируемость и точность обработки данных. Она оптимизирована для комплексных аналитических задач и эффективно управляет рабочими процессами с большими данными..[^greenplum]

Архитектура Greenplum Database основана на массивно-параллельной обработке данных (MPP), что позволяет распределять нагрузку между множеством узлов, обеспечивая высокую производительность. <dfn title="массово-параллельная обработка">Массово-параллельная обработка</dfn> (*Massively Parallel Processing*, MPP) — это архитектура вычислений, которая использует сотни или тысячи процессоров (узлов) для одновременного решения одной сложной задачи, разбивая ее на мелкие части, которые выполняются параллельно, что обеспечивает высокую скорость обработки больших объемов данных. Каждый узел в MPP системе имеет свою память и ресурсы, работает независимо, а обмен данными происходит через высокоскоростную сеть.

**Ключевые особенности MPP**:
- **Распределенная память**: каждый процессор имеет собственную память, а не разделяет ее с другими (в отличие от архитектур типа SMP).
- **Масштабируемость**: системы MPP легко масштабируются путем добавления новых узлов для увеличения производительности.
- **Параллельное выполнение**: сложные запросы или задачи разбиваются на подзадачи и выполняются одновременно на множестве узлов.
- **Независимость узлов**: узлы работают автономно, со своей ОС и устройствами ввода/вывода.

**Где используется MPP**:
- **Анализ Big Data** (Больших данных): обработка огромных датасетов.
- **Хранилища данных** (Data Warehousing): быстрое получение аналитических отчетов.
- **Научные исследования**: моделирование погоды, сложные вычисления.
- **Бизнес-аналитика** (BI): получение мгновенных инсайтов из больших объемов информации.

**Принцип работы** (на примере баз данных):
- **Разделение данных**: табличные данные разбиваются на блоки (сплиты).
- **Распределение**: блоки данных распределяются по узлам кластера.
- **Параллельная обработка**: каждый узел обрабатывает свой блок данных независимо.
- **Агрегация**: Результаты с узлов собираются и объединяются для получения окончательного ответа.

MPP архитектура — это фундамент современных платформ для работы с большими данными, обеспечивающий высокую производительность и отказоустойчивость, как в облачных сервисах, так и в локальных системах. 

![Greenplum database](../img/big-data-blockchain-technology-isometric-mobile-phone-data-visualization-1024x447.jpg.webp)

**Основные элементы архитектуры**:
1. **Мастер-узел**

      - Управляет запросами, распределяет задачи между сегментами.
      - Обрабатывает метаданные и возвращает результаты пользователю.

2. **Сегменты**

      - Независимые узлы, работающие параллельно.
      - Каждый сегмент хранит и обрабатывает выделенную часть данных, что минимизирует время выполнения запросов.

3. **Коммуникационная сеть**

      - Обеспечивает взаимодействие между мастер-узлом и сегментами.
      - Гарантирует синхронизацию данных и агрегацию результатов.

Такой подход позволяет обрабатывать терабайты данных быстрее и точнее, чем в традиционных системах.

![Архитектура Гринпалм](../img/arhitektura-grinpalm-min.png.webp)

**Greenplum Database** позволяет обрабатывать большие объемы данных и выполнять аналитические задачи, необходимых для принятия бизнес-решений. Применяются в таких отраслях как:

**Ритейл**:

![Ритейл](../img/image-101-min.png.webp)

1. Формирование персонализированных предложений для клиентов на основе анализа транзакционных данных.
2. Оптимизация цепочек поставок за счет прогнозирования узких мест и минимизации логистических издержек.
3. Построение моделей прогнозирования спроса с учетом исторических данных и сезонных колебаний.

**Финансовый сектор**:

![Финансовый сектор](../img/image-201-min.png.webp)

1. Оценка кредитных рисков и разработка моделей скоринга для снижения уровня дефолтов.
2. Выявление мошеннических транзакций на основе анализа клиентского поведения в реальном времени.
3. Разработка новых финансовых продуктов через сегментацию и прогнозирование потребностей клиентов.

**Производство**:

![Производство](../img/image-78-min.png.webp)

1. Анализ производственных данных для повышения эффективности технологических процессов.
2. Контроль качества продукции в режиме реального времени с использованием предиктивной аналитики.
3. Оптимизация логистических операций и управление цепочками поставок.

**Телекоммуникации**:

![Телекоммуникации](../img/image-70-min.png.webp)

1. Анализ сетевой нагрузки для предотвращения сбоев в периоды пикового использования.
2. Сегментация абонентов для персонализированных маркетинговых кампаний.
3. Прогнозирование спроса на услуги связи на основе анализа поведения клиентов.

**Greenplum Database** подходит для организаций, которые:

- Обрабатывают большие объемы  данных ежедневно.
- Требуют высокой производительности при выполнении аналитических запросов.
- Переходят от традиционных решений к построению масштабируемых Data Warehouse.

**Пример применения**: компании, работающие с большим количеством транзакционных данных, используют Greenplum для консолидации разрозненной информации и анализа данных.

![Применение Greenplum](../img/primenenie-greenplum-min.png)

**Greenplum Database** востребована в enterprise-секторе благодаря высокой производительности, масштабируемости и надежности. Основные сценарии использования Greenplum включают:

- **Системы предиктивной аналитики и отчетности**: обработка больших массивов данных для построения прогнозных моделей, включая анализ поведения клиентов, выявление трендов и формирование стратегических отчетов.
- **Построение озер данных** (Data Lake) и **корпоративных хранилищ** (DWH): консолидация данных из различных источников для их последующей обработки и анализа.
- **Разработка аналитических моделей**: создание прогнозных моделей, таких как анализ вероятности оттока клиентов (Churn Rate), сегментация аудитории и оценка рисков.

![Разработка на основе Greenplum](../img/razrabotka-na-osnove-greenplum-min-1024x447.png.webp)

Greenplum успешно применяется в компаниях, где требуется обработка больших данных с высокой скоростью. Среди ключевых пользователей:

- **Финансовый сектор**: NYSE, NASDAQ, Тинькофф Банк, Sberbank CIB используют Greenplum для анализа транзакций, кредитного скоринга и управления рисками.

- **Телекоммуникации**: Ростелеком, AT&T применяют Greenplum для анализа сетевой нагрузки, сегментации клиентов и управления качеством обслуживания.

- **Ритейл**: Sony и другие ритейлеры используют Greenplum для прогнозирования спроса, анализа покупательского поведения и персонализации маркетинговых кампаний.

**Greenplum Database** предоставляет надежную платформу для аналитики, способную адаптироваться под потребности бизнеса, работающего с большими объемами данных.

**Greenplum** и **ClickHouse** — это два разных инструмента для работы с данными, ориентированных на разные задачи.

- **Greenplum** оптимизирован для построения масштабируемых хранилищ данных и выполнения сложных аналитических запросов SQL. Подходит для компаний, которым требуется анализировать данные в терабайтных объемах и использовать сложные алгоритмы прогнозирования.
- **ClickHouse** ориентирован на выполнение запросов OLAP в режиме реального времени, но менее эффективен при создание комплексных аналитических витрин, включающих множество связанных таблиц..

Greenplum — это универсальный инструмент для бизнеса, требующего интеграции Big Data с глубоким аналитическим подходом.[^greenplum]

### Аналитика больших данных
*[BI]: Business Intelligence
*[OLAP]: On-Line Analytical Processing

Современные Big Data решения охватывают широкий спектр вопросов, связанных с корпоративными информационными ресурсами. Иногда разрабатываются комплексные системы, которые позволяют компаниям эффективно управлять данными и использовать их в интересах бизнеса.

#### Основная терминология
<dfn title="Business Intelligence">Business Intelligence</dfn> (BI) — технология аналитики, которая призвана предоставлять компании доступ к целостной картине деятельности в режиме реального времени. BI-системы отображают ключевые метрики, формируют детализированные отчёты и помогают ориентироваться в изменениях, оказывая поддержку при принятии стратегических решений.

<dfn title="OLAP">OLAP</dfn> (англ. *online analytical processing*, интерактивная аналитическая обработка) — подход к обработке данных, позволяющий оперативно получать в структурированном виде определённый срез из большого массива данных для их последующего анализа. Как правило основывается на подготовке агрегированной информации из больших массивов данных, структурированной по многомерному принципу. Реализации технологии OLAP являются компонентами программных решений класса Business Intelligence[^OLAP].

<dfn title="OLAP-кубы">OLAP-кубы</dfn> — структурированный инструмент, необходимый для многомерного анализа показателей. Благодаря ему специалисты выявляют скрытые закономерности, проводят глубокую обработку информации и прогнозируют динамику развития.

<dfn title="хранилище данных">Хранилище данных</dfn> (DataWarehouse, DWH) — единая среда хранения, рассчитанная на большой объем корпоративной информации. Грамотная архитектура DWH обеспечивает стабильную работу основных сервисов, а также сбор и консолидацию всех данных для дальнейших аналитических вычислений.

<dfn title="dashboarding">Dashboarding</dfn> — визуальное представление показателей, помогающее отслеживать изменения в режиме реального времени. Гибкие дашборды упрощают работа с ключевыми метриками и улучшают процессы управления, за счет чего повышается понимание текущих тенденций и возможностей компании.

<dfn title="data science">Data Science</dfn> (наука о данных) — методология, в рамках которой применяются машинное обучение и нейронные сети. Такой подход помогает создавать модели для прогнозирования спроса, удержания аудитории или оптимизации маркетинговых кампаний, задействуя весь доступный объем данных.[^resheniya-bi-big-data]

#### Бизнес-аналитика
Разработка и внедрение BI систем (Business Intelligence) – это не просто цифровизация отчетности, а стратегический инструмент для повышения прозрачности, управляемости и эффективности бизнеса. BI позволяет принимать решения, основанные на фактах, а не на интуиции.[^business-intelligence]

##### Преимущества внедрения бизнес-аналитики BI
Сегодня компании генерируют огромные объемы информации: продажи, финансы, маркетинг, логистика, клиентское поведение. Но без систематизированного подхода эти данные остаются неиспользованным активом. BI-системы позволяют трансформировать разрозненные данные в целостную картину бизнеса, выявлять тренды и отклонения, прогнозировать сценарии развития.

Business Intelligence – это не просто набор дашбордов. Это платформа, интегрирующая данные из внутренних и внешних источников (CRM, ERP, базы данных, облачные сервисы), очищающая их, структурирующая и визуализирующая в формате, понятном как для руководства, так и для аналитиков.

![Устройство современной BI-системы](../img/snimok-ekrana-2025-05-23-052705-1024x711.png.webp)

##### Какие задачи решают BI-системы для руководителей и аналитиков
- **Принятие обоснованных решений**. BI предоставляет полную картину происходящего в реальном времени, позволяя оперативно реагировать на изменения.

- **Финансовый контроль, оптимизация затрат и автоматизация отчетности**. Анализ прибыльности проектов, выявление точек потерь и неэффективных процессов.

- **Повышение прозрачности процессов и визуализация данных**. Вы видите, как работают отделы, где есть узкие места, и какие действия приносят результат.

- **Планирование и прогнозирование**. BI для бизнеса помогает строить сценарии развития, моделировать спрос и управлять рисками.

- **Улучшение клиентского сервиса**. Аналитика клиентского поведеня позволяет выстраивать персонализированные предложения и повышать лояльность.

![BI](../img/frame-2-1024x331.png.webp)

##### Примеры задач, решаемых с помощью BI
BI-системы эффективны в компаниях, где важна скорость принятия решений и управление на основе данных:

- **Ритейл**: анализ продаж, управление цепочками поставок, оценка эффективности рекламных кампаний.

- **Производственные предприятия**: контроль загрузки оборудования, минимизация простоев, управление себестоимостью.

- **Финансовый сектор**: детальный анализ доходов, расходов, рисков и рентабельности.

- **IT и цифровые сервисы**: поведенческая аналитика пользователей, мониторинг эффективности релизов, развитие продуктов.

- **Логистика и транспорт**: оптимизация маршрутов, мониторинг SLA, управление запасами.

##### BI и сквозная аналитика: в чем различие?

![BI](../img/decosystems.vzlet_.media_uslugi_resheniya-bi-big-data_-1-photoroom.png.webp)

**Business Intelligence** фокусируется на внутренней эффективности: анализ операционных показателей, производственных процессов, финансовой отчетности.

![Analytics](../img/decosystems.vzlet_.media_uslugi_resheniya-bi-big-data_-e1729072553552-photoroom.png.webp)

**Сквозная аналитика** ориентирована на внешние каналы привлечения и поведение клиентов: анализ маркетинговой воронки, атрибуция рекламных расходов, ROI по каналам.

Вместе они создают единую цифровую экосистему управления бизнесом: BI обеспечивает контроль внутренних метрик, а сквозная аналитика – оптимизацию внешней коммуникации и маркетинга.

##### Ключевые этапы внедрения BI
- **Постановка целей**: определение приоритетных задач и KPI.

- **Выбор BI-платформы**: анализ функционала, масштабируемости и совместимости с текущими системами.

- **Интеграция данных**: подключение к источникам, обеспечение качества и актуальности информации.

- **Настройка дашбордов**: визуализация ключевых метрик для разных ролей: от СЕО до операционного менеджера.

- **Обучение пользователей**: адаптация команды под работу с новой системой.

- **Мониторинг и оптимизация**: регулярное обновление данных и сценариев, адаптация под изменяющиеся цели бизнеса.

##### Примеры используемых систем
Стоит обратить внимание на такие платформы как Alpha BI, Glarus BI, Luxms BI и Analytics Workspace

- **Analytics Workspace**

    ![Analytics Workspace](../img/card-1.png)

    Это мощная BI-система, разработанная для оптимизации аналитики данных и повышения эффективности бизнеса. Она предлагает интуитивно понятный интерфейс, который облегчает визуализацию данных и создание отчетов даже для пользователей без технического бэкграунда. С возможностью интеграции с различными источниками данных, Analytics Workspace обеспечивает всесторонний анализ и глубокое понимание бизнес-процессов. Автоматизация отчетности и возможность настройки информационных панелей позволяют командам сосредоточиться на стратегическом развитии, принимая обоснованные решения на основе актуальной информации.

- **Alpha BI**

    ![Alpha BI](../img/card-2.png)

    Это интуитивно понятная BI-система, которая обеспечивает мгновенный доступ к вашим данным и мощные инструменты визуализации. С Alpha BI вы сможете легко настраивать отчеты и обзорные панели, что позволяет принимать взвешенные бизнес-решения на основе актуальной информации.

- **Glarus BI**

    ![Glarus BI](../img/card-3.png)

    Предлагает надежное решение для анализа данных, фокусирующее внимание на автоматизации отчетности. С этой системой вы сможете сократить время на сбор и обработку информации, позволяя вашему бизнесу сосредоточиться на стратегическом развитии и повышении эффективности.

- **Luxms BI**

    ![Luxms BI](../img/luxmbi-1.png.webp)

    Российская платформа бизнес-аналитики с мощными функциональными и визуальными возможностями для создания аналитических решений — от «готовых» дэшбордов до развитого функционала self-service. Платформа гибко настраивается под конкретные бизнес цели компании.

### Разработка систем Big Data
Опыт показывает, что внедрение любой инициативы в сфере Big Data обычно занимает от года до нескольких лет. Для успеха проекта важна проработка концепции управления данными, определение архитектуры и выбор технологического стека. Когда возникает нехватка конкретной экспертизы или времени, компании привлекают внешних интеграторов.

Распространённая практика — создавать отдел, отвечающий за анализ и обработку больших массивов данных. В такой команде работают архитекторы, аналитики и инженеры по качеству, благодаря чему почти любая масштабная задача выполняются быстрее. Если система разворачивается в облаке, зоны ответственности разделяются между компанией-заказчиком, интегратором и провайдером, что способствует прозрачности и гармоничной координации.

#### Специалисты для работы с Big Data
Можно выделить несколько групп специалистов, без которых вряд ли удастся организовать хранилище больших данных:

- **Инженеры** — создают инфраструктуру для сбора и хранения данных. К ним относятся также разработчики центров обработки данных и сотрудники облачных сервисов.
- **Аналитики** — помогают находить скрытые закономерности и вырабатывать решения по улучшению продукта. Это не только дата-сайентисты, но и маркетологи, дизайнеры интерфейсов, специалисты по обработке естественного языка и другие.
- **Специалисты по нейросетям и машинному обучению** — подключают к работе искусственный интеллект, упрощающий анализ массивов информации.[^bigdata]

#### Этапы разработки систем Big Data

1. **Анализ задачи**. Сбор исходной информации, изучение существующих процессов и подходящих СУБД. Важна точная формулировка целей и понимание того, как данные будут использоваться и какие результаты ожидаются.

2. **ТЗ на разработку Big Data**. На базе полученных сведений формируется техническое задание. В нём учитываются все требования к функционалу, детально описывается модель решения и определяются используемые инструменты.

3. **Проектирование и дизайн**. Архитектурный отдел продумывает логику модулей, а также внешний вид будущей системы. На этом этапе выбираются методы интеграции и технология, позволяющие добиться высокой производительности.

4. **Программная реализация**. Специалисты приступают к коду, выбирая гибкие методологии (Scrum, Agile) или каскадные (Waterfall), в зависимости от потребностей заказа. Мы можем разрабатывать модули параллельно с тестированием, чтобы поддерживать высокое качество.

5. **Тестирование**. После завершения основной части работ команда проверяет все компоненты на корректность обработка данных и устойчивость к нагрузкам. Результаты согласовываются со стейкхолдерами.

6. **Внедрение и сопровождение**. Готовое решение интегрируется в инфраструктуру компании. Далее специалисты контролируют стабильность системы, занимаются её обновлением и развитием. Правильно организованная поддержка позволяет сохранять эффективность в долгосрочной перспективе.

При грамотном подходе даже очень большой проект в области Big Data становится полезным инструментом для усиления позиций на рынке. Благодаря углубленной аналитике руководители получают информацию о ключевых тенденциях, а это помогает разрабатывать более точные стратегии и повышать конкурентоспособность. Именно такой подход DecoSystems стремится внедрять, реализуя решения любой сложности и масштаба.[^resheniya-bi-big-data]

### Направления работы в области Big Data

*[LTV]: Lifetime Value

1. **Разработка банковских антифрод-систем и сценариев поиска недобросовестных клиентов**

      - Автоматизированная оценка онлайн-операций
      - Выявление паттернов поведения пользователей
      - Учет множества критериев при сегментарном анализе действий пользователя

      ![Card](../img/card-1.jpg.webp)

2. **Увеличение среднего чека и минимизация оттока существующих клиентов**

      - Проведение клиентской аналитики
      - Сегментация клиентов с помощью алгоритмов ML
      - Настройка рекомендательных систем и расчет LTV клиентов

      <details>
      <summary>Что такое LTV</summary>

      LTV, или Lifetime Value, — пожизненная ценность клиента. Метрика показывает, сколько денег клиент принёс компании за всё время взаимодействия с ней. Это может быть и чистая прибыль от клиента, и доход от всех его заказов. LTV — одна из главных маркетинговых метрик. С её помощью можно оценить эффективность выбранной бизнес-модели и понять, окупаются ли на самом деле затраты на маркетинг. Метрику используют, чтобы понять, оправданы ли затраты на привлечение, вовлечение и удержание клиента. Другие названия показателя — CLV и CLTV: Customer Lifetime Value.[^chto-takoe-ltv-zachem-eye-schitat-i-kak-pravilno-eto-delat]

      </details>

      ![Card](../img/card-2.jpg.webp)

3. **Создание маркетинговых движков и реализация персонализированного real-time маркетинга**

      - Интеграция бэкофисных систем с метаданными из веб-источников и социальных сетей
      - Интеграция бэкофисных систем с геолокационными метаданными
      - Реализация обогащения данных через интеграционное взаимодействие с публичными источниками для улучшения качества базы данных

      <details>
      <summary>Что такое бэк-офис</summary>

      <dfn title="бэк-офис">Бэк-офис</dfn> (от англ. *back* — задняя часть) — внутренние отделы компании, которые выполняют обслуживающие и административные функции и не взаимодействуют напрямую с клиентами и заказчиками. Среди прочего в круг задач входит оптимизация и автоматизация рабочих процессов, упразднение неэффективных операций. Фактически большинство компаний имеют бэк-офис, но не всегда выделяют его как нечто обособленное. Как правило, к бэк-офису относятся:

      - юристы
      - бухгалтеры и экономисты
      - HR
      - IT-специалисты
      - аналитики

      Существует противоположное понятие — фронт-офис. В него входят сотрудники, взаимодействующие с клиентами. В небольших компаниях обычно нет территориального разделения на фронт- и бэк-офис: все сотрудники работают в одном месте.[^bek-ofis]

      </details>

      ![Card](../img/card-3.jpg.webp)

4. **Проведение аудита текущих систем отчетности и реализация дашбордов**

   - Формирование требований к системам отчетности
   - Обучение специалистов использованию систем визуализации данных
   - Обучение специалистов использованию отчетных движков (Qlik Sense, Stimulsoft, Tableau и другие)

      ![Card](../img/card-5.jpg.webp)

5. **Построение логической модели данных для различных сфер бизнеса**

      - Разработка DWH, Data Lake, Data Marts
      - Определение оптимальной инфраструктуры при использовании MPP систем и ETL-инструментов
      - Реализация алгоритмов пакетной и real time модели обновления слоев данных для надежной работы с большими объемами информации

      ![Card](../img/card-4.jpg.webp)

### Применение Big Data
Необходимость в хранилище больших сегодня возникает практически у каждой крупной компании. Рассмотрим примеры из разных отраслей, для которых может потребоваться внедрение хранилища больших данных.

- **Телекоммуникации**. Отрасль является абсолютным лидером по использованию Big Data: около 90% телекоммуникационных компаний уже собирают и анализируют большие данные, а остальные планируют начать в будущем. Например, такие онлайн-сервисы, как YouTube, VK и другие не смогли бы существовать без объемных хранилищ данных.

- **Продажи**. Немецкий производитель автомобилей BMW несколько лет назад решил собрать данные о своих продажах, продажах конкурентов, а также отследить, где автомобили этой марки пользуются наибольшим спросом. Анализ и визуализация информации помогли им выявить слабые и сильные места своего бизнеса и внести коррективы в стратегию компании.

- **Маркетинг**. Сложно представить, какое количество потребительских данных непрерывно поступает на серверы гиганта электронной коммерции Amazon. Благодаря анализу больших данных ритейлер в курсе интересов покупателей, а также предлагает собранную информацию другим компаниям, которые тоже используют ее в маркетинге.

- **Банкинг**. Такие банки, как СберБанк, Тинькофф и многие другие с помощью Big Data не только анализируют действия клиентов и предлагают им свои продукты, но и повышают безопасность. Например, биометрические данные клиентов помогают им бороться с мошенниками, а анализ доходов и затрат клиентов — оценивать их кредитоспособность.

- **Транспорт**. Получение больших объемов данных от водителей в режиме реального времени может сильно помочь сервисам такси. Так, Яндекс.Такси отслеживает спрос и количество водителей на территории, что позволяет предлагать клиентам актуальные цены.

- **Подбор персонала**. Рутинную работу по подбору кандидатов в крупных компаниях уже давно выполняют роботы, которые анализируют многочисленные резюме без помощи рекрутеров. Например, компания PepsiCo заполнила 10% своих вакансий с помощью робота. По словам представителей производителя, искусственный рекрутер может провести 1500 интервью за 9 часов, на что у HR-сотрудника ушли бы месяцы.

- **Автомобилестроение**. Наличие хранилища больших данных и умелых специалистов по их анализу иногда может даже спасти жизни. Например, компания Toyota с помощью Big Data выяснила, что большинство аварий происходит из-за того, что водители путают педали. Производитель начал разработку сервиса по определению манеры давления на педаль, который помешает водителю сделать ошибку во время стрессовой ситуации.[^bigdata]

*Области применения и примеры*

| Сфера | Примеры
-- | --
Рекомендательные системы | Netflix, YouTube, Amazon
финансовые технологии | Кредитный скоринг, обнаружение и борьба с мошенничеством
Медицина | Диагностика заболеваний по изображениям  или генетике, обработка генетических данных
Автомобильная промышленность | беспилотные автомобили
Социальные сети | Анализ и модерация контента


### Преимущества и недостатки Big Data
Резюмируя опыт использования больших данных, можно выделить следующие неоспоримые преимущества данной технологии:

- **Улучшение принятия решений**: анализ больших объемов данных позволяет выявлять точные корреляции и тренды, которые недоступны при работе с малыми наборами данных, что повышает качество стратегических и оперативных решений.

- **Персонализация и таргетинг**: использование больших данных позволяет предлагать клиентам более релевантный контент и продукты на основе их поведения, предпочтений и контекста в реальном времени.
​
- **Прогнозирование и оперативная оптимизация**: модели на основе больших данных улучшают точность прогнозов спроса, планирования запасов, обслуживания оборудования и др., что снижает издержки и увеличивает эффективность.
​
- **Инновации и новые бизнес-модели**: анализ больших данных стимулирует создание новых услуг, продуктов и цифровых экосистем, включая аналитические платформы, IoT-решения и автоматизацию процессов.
​
- **Эффективность процессов и автоматизация**: автоматизация сбора, обработки и анализа больших массивов данных ускоряет циклы принятия решений и сокращает время реакции на изменения рынка

Несмотря на очевидную пользу, пользователи больших данных сталкиваются с рядом трудностей:

- **Сложность**. Чтобы правильно собирать, фильтровать, обрабатывать и анализировать разрозненную и разнообразную информацию, требуется труд множества квалифицированных специалистов, а также вычислительные мощности и инфраструктура.

- **Затраты**. Специалисты по работе с данными очень востребованы, для хранения и обработки требуются серьезные вычислительные мощности, а многие инструменты являются платными.

- **Безопасность**. Чем больше у вас важной информации, тем выше риски, что она попадет не в те руки. Так, если взломать хранилище данных банка, то миллионы его клиентов могут остаться без денег. Чтобы не допустить этого, нужен основательный подход к сохранности данных и квалифицированные специалисты.[^bigdata]

### Наука о данных
<dfn title="наука о данных">Наука о данных</dfn> (*data science*) — это междисциплинарная область, которая использует научные методы, процессы, алгоритмы и системы для извлечения знаний и инсайтов из больших и структурированных или неструктурированных данных, объединяя математику, статистику, программирование и аналитику для решения сложных задач, прогнозирования и принятия решений. По сути, это превращение хаоса сырых данных в полезную информацию, модели и прогнозы, которые используются в бизнесе, науке, медицине и других сферах. 

Если говорить простыми словами, то <dfn title="data science">data science</dfn> (наука о данных) — это дисциплина, которая позволяет сделать данные полезными. Если найти пересечение различных определений что же такое Data Science, то им будет лишь одно слово — данные. Всё это говорит о том, что широта применения Data Science огромна. К сожалению, эта широта не дает никакой информации о потенциальной деятельности человека, решившего освоить это направление. Ведь с данными можно делать всё, что угодно. Можно строить сложные отчеты или «шатать» таблички с помощью SQL. Можно предсказывать спрос на такси константой или строить сложные математические модели динамического ценообразования. А еще можно настроить поточную обработку данных для высоконагруженных сервисов, работающих в режиме реального времени. 

А вообще, причем здесь слово «наука»? Безусловно, под капотом у Data Science серьезнейший математический аппарат: теория оптимизации, линейная алгебра, математическая статистика и другие области математики. Но настоящим академическим трудом занимаются единицы. Бизнесу нужны не научные труды, а решение проблем. Лишь гиганты могут позволить себе штат сотрудников, которые будут только и делать, что изучать и писать научные труды, придумывать новые и улучшать текущие алгоритмы и методы машинного обучения.

К сожалению, многие эксперты в этой области на разных мероприятиях зачастую связывают Data Science в первую очередь с построением моделей с помощью алгоритмов машинного обучения и довольно редко рассказывают самое важное, по-моему, — откуда возникла потребность в той или иной задаче, как она была сформулирована на «математическом языке», как это всё реализовано в эксплуатации, как провести честный эксперимент, чтобы правильно оценить бизнес-эффект.[^557416]

![Data scientist](../img/9b389ab7909bffed8bf8eb3653fb23bf.png)

Для того, чтобы лучше понять суть data science, рассмотрим следующую картинку с [распределением Дирихле](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%94%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5), то есть с вероятностью вероятностей:

![data science](../img/e2f56de4745c2eadd0fa550b12fb6271.png)

Предположим, что в Data Science существуют три основные компетенции:

1. **Математика**. Теоретические знания алгоритмов машинного обучения, и математическая статистика для проверки разных статистических гипотез и обработки результатов, а также любые другие фундаментальные знания, которые будут важны в вашей предметной области.

2. **Разработка**. Всё, что связано с разработкой, инженерными составляющими проекта, DevOps, SysOps, SRE, и прочее.

3. **Предметная область**. Навыки коммуникации с коллегами и бизнесом, чтобы понимать, какую проблему они хотят решить, на какие вопросы ответить.

#### Направления и специальности
*[DevOps]: Development & Operations
*[ML]: Machine Learning
*[R&D]: Research & Development
*[ИИ]: искусственный интеллект

И Data Scientist в этой парадигме — это некоторое наблюдение из нашего распределения Дирихле. Но с помощью этого распределения можно ввести несколько новых должностей, которые будут давать более ясное представление о вашей потенциальной деятельности. Рассмотрим несколько из них.

![Data science branches](../img/181c4de21df8ba4b94e4ea5e29f9bc2b.png)

- <dfn title="инженер машинного обучения">Инженер машинного обучения</dfn> (ML-инженер, *Machine Learning Engineer*) — специалист, который создает, обучает и внедряет модели искусственного интеллекта (ИИ), позволяя компьютерам решать задачи, анализируя большие объемы данных без явного программирования каждого шага. Он трансформирует идеи в рабочие системы, интегрируя их в реальные бизнес-процессы, оптимизируя производительность и обеспечивая стабильную работу в продакшне, занимется введением в эксплуатацию моделей машинного обучения и поддерживает их в актуальном состоянии. Для этого требуются навыки и знания в области алгоритмов машинного обучения, ну и, конечно, разработки.

- <dfn title="аналитик данных">Аналитик данных</dfn> (*data analyst*) — специалист, который собирает, обрабатывает и интерпретирует большие объемы данных, чтобы помочь компаниям принимать обоснованные решения, выявлять закономерности и тренды, оптимизировать процессы и снижать риски. По сути, он "переводит" хаотичные цифры в понятные выводы и рекомендации для бизнеса, используя статистические методы и инструменты визуализации. Данный специалист занимается проверкой статистических гипотез, проектирует и проводит эксперименты. Для этого требуются фундаментальные знания математической статистики, а также необходимо держать руку на пульсе бизнеса.

- <dfn title="инженер данных">Инженер данных</dfn> (дата-инженер, *data engineer*) — это IT-специалист, который строит и обслуживает инфраструктуру для сбора, хранения, обработки и перемещения больших объемов данных, делая их доступными и готовыми для аналитиков и ученых по данным (*Data Scientists*). Это тот человек, который занимается ETL-процессами, архитектурой хранилища, составляет витрины и поддерживает их, организовывает потоковую обработку данных. По сути, он «архитектор и строитель» мира данных, который обеспечивает надежные потоки информации, чтобы другие специалисты могли извлекать из них ценные инсайты.

- <dfn title="исследователь машинного обучения">Исследователь машинного обучения</dfn> (*machine learning researcher*) — исследователь, ориентированный на разработку новых методов и архитектур, проведение экспериментов и публикацию научных статей. Часто работает в академическом или R&D-контексте. Данный специалист в основном занимается исследовательской работой. Пишет и изучает статьи, придумывает новые математические методы. Таких позиций в России довольно мало, да и встречаются они, как правило, в крупных компаниях, которые могут себе это позволить.

- <dfn title="аналитик">Аналитик</dfn> (*analyst*) — специалист, который собирает, обрабатывает и интерпретирует данные, превращая их в понятные выводы и рекомендации для принятия обоснованных решений в бизнесе, IT или других сферах. <dfn title="бизнес-аналитик">Бизнес-аналитик</dfn> (*business analyst*) — это специалист, который изучает бизнес-процессы компании, выявляет проблемы и возможности для повышения эффективности, а затем переводит потребности бизнеса на понятный язык для команд разработки, предлагая и помогая внедрять решения для достижения целей компании. Это человек, который отвечает на вопросы бизнеса, и его плотность вероятности приходится на предметную область. Он выступает связующим звеном между бизнесом и IT-специалистами, работая над оптимизацией процессов, созданием новых продуктов и принятием управленческих решений, основанных на фактах. 

- <dfn title="оператор машинного обучения">Оператор машинного обучения</dfn> (*MLOps-инженер*) — это специалист на стыке Data Science и DevOps, который автоматизирует и управляет всем жизненным циклом моделей машинного обучения (ML) — от экспериментов и разработки до развертывания (deployment), мониторинга и поддержки в промышленной эксплуатации, делая ML-системы надежными, масштабируемыми и стабильными. Он создает конвейеры (pipelines) CI/CD для моделей, контейнеризирует их, настраивает мониторинг и обеспечивает интеграцию с IT-инфраструктурой, превращая прототипы в реальные бизнес-решения. Этот человек максимально сосредоточен на разработке и развёртывании кода в продакшене.

#### Уровни компетенций

Попробуем коротко сформулировать профиль человека, который будет находиться на каждом из грейдов в мире Data Science. Не стоит забывать, что от компании к компании уровень компетенций для каждого из грейдов может довольно сильно отличаться.[^557416]

1. **Junior Data Scientist**

   - Умеет реализовать полный DS-пайплайн: «приготовить» данные, обучить модель, измерить ее качество.
   - Делает только то, что ему сказали.
   - Нуждается в постоянной опеке и контроле.

2. **Middle Data Scientist**

   - Имеет подтвержденный на практике результат, например, построил и внедрил модель оттока клиентов, которая экономит компании N млн. руб в год.
   - Может обсуждать бизнес-постановку задачи.
   - В меру самостоятельный.
   - Редко ошибается.

3. **Senior Data Scientist**

   - Имеет более обширный опыт по сравнению с мидлом.
   - Может самостоятельно формулировать и решать задачи.
   - Имеет опыт наставничества или готов быть ментором.
   - Обладает высоким уровнем эмоционального интеллекта.
   - Уровень технических компетенций выше мидла.

   Если у middle ребят возникают проблемы с ростом и развитием, то зачастую это связано с

   - отсутствием проактивности;
   - неготовностью брать ответственность и инициативу на себя и доводить дело до конца;
   - неумением находить общий язык с бизнес заказчиками и смежниками;
   - синдромом самозванца;
   - недостаточным уровнем эмоционального интеллекта и/или отсутствия понимания его важности в рабочей деятельности

   А дальше уже сложнее, потому что тимлид может руководить как командой из 2-3 человек, так и несколькими отделами.

4. **Teamlead**

   - Эксперт, который отвечает за конкретные участки DS-пайплайна. Работает в соответствие с поставленными перед ним задачами. Координирует работу нескольких младших коллег.
   - Ставит задачи экспертам в соответствии с заданным планом и координирует их работу. Несет ответственность за конкретное направление DS в компании.
   - Отвечает за продукт/проект/направление, имеющие большое значение для крупной компании. Определяет требования к команде и составляет планы в соответствии с заданным направлением действий.
   - Отвечает за стратегически важный продукт/проект/направление в крупной компании. Руководит большой командой data scientist’ов и аналитиков. Задает команде направление действий, оценивает сроки и затраты, отвечает за результаты проектов.

    Чем выше уровень специалиста, тем больше ответственности и тем сложнее направление R&D. А значит, и больше зарплата.

    Но всё же можно выделить характерные отличия тимлида. Безусловно, этот человек должен обладать техническими навыками (hard skills): он знает, как сделать так, чтобы «всё заработало», может ответить на специфичные для продукта вопросы, знает, как работает продукт. А еще тимлид планирует и формулирует задачи (впоследствии «продаёт»), раскладывает их на составляющие, напрямую общается с бизнесом, работает с командой, занимается развитием и ростом своих ребят. Для тимлида важно думать и жить в терминах продукта и бизнеса, быть проактивным и доводить дело до конца.[^557416]

### Взаимосвязь машинного обучения и больших данных
На пересечении двух революционных технологий — машинного обучения и больших данных — рождается симбиоз, который кардинально меняет парадигмы современного мира. Осознание глубины и значения этой связи имеет ключевое значение для понимания будущего IT-сферы. Для начала, рассмотрим суть обеих дисциплин. Большие данные представляют собой огромные объемы информации, которые сложно обработать с помощью традиционных методов. Машинное обучение же — это способность машин адаптироваться и «учиться» на основе данных без явного программирования. Казалось бы, где здесь переплетение?

Для качественного МО нужны большие объёмы данных — больших данных. Большие данные — источник информации для обучения моделей машинного обучения. Чем больше и качественнее данные, тем лучше можно обучить модель. Современные алгоритмы глубокого обучения требуют огромных объёмов данных и вычислительных ресурсов. Глубокие нейросети требуют масштабных данных и ресурсов. В условиях больших данных необходимость в эффективной автоматизации анализа и построения моделей огромна. Совместное использование позволяет строить точные и сложные модели, работать с реальным временем и масштабными задачами (например, рекомендации, анализ видео).

Таким образом, тезисно связь между большими данными и машинным обучением можно выразить следующим образом.

- **Топливо для машин**: Большие данные становятся «топливом» для систем машинного обучения. Благодаря такому объему информации, машины могут обучаться гораздо эффективнее, распознавая закономерности и шаблоны в данных, которые раньше оставались незамеченными.

- **Высокая скорость принятия решений**: Машинное обучение позволяет обработать массивы больших данных в реальном времени, предоставляя компаниям моментальные аналитические выводы и прогнозы.

- **Персонализация и оптимизация**: Используя большие данные, системы машинного обучения могут предоставлять высоко персонализированные решения для пользователей, учитывая их индивидуальные предпочтения и поведение.

- **Автоматизация процессов**: На основе анализа больших данных, машины могут автоматизировать рутинные процессы, выполняя задачи, которые ранее требовали человеческого вмешательства.

Большие данные предоставляют необходимую основу для обучения машин, в то время как машинное обучение обеспечивает инструменты для извлечения ценной информации из этих данных. Вместе они становятся мощным катализатором инноваций и прогресса в современном цифровом мире.[^mashinnoe-obuchenie-i-big-data]

Таким образом МО и большие данные — ключевые технологии будущего и важные тренды. Следующие шаги в обучении: изучение Python, библиотек (scikit-learn, TensorFlow), баз данных и распределённых систем. Для старта — изучить Python, библиотеку scikit-learn, основы Hadoop/Spark.

### Интернет вещей

<dfn title="интернет вещей">Интернет вещей</dfn> (*Internet of Things* — IoT) — это глобальная сеть физических устройств («вещей»), оснащенных датчиками и программным обеспечением, которые позволяют им подключаться к интернету и обмениваться данными друг с другом и с другими системами без участия человека, что создает «умные» объекты, автоматизирует процессы и делает нашу жизнь удобнее и эффективнее. Эти «вещи» могут быть бытовой техникой, автомобилями, промышленным оборудованием, носимыми устройствами и многим другим, собирая и передавая данные о температуре, влажности, движении и других параметрах.

![Internet of Things](../img/aa0.png)

**Как это работает**
- **Сбор данных**: датчики в устройствах собирают информацию из окружающей среды.
Передача данных: Информация отправляется в облако или на сервер через беспроводные сети (Wi-Fi, Bluetooth, сотовая связь).
- **Обработка данных**: облачные сервисы анализируют данные с помощью алгоритмов и ИИ.
- **Действие/Реакция**: система либо автоматически выполняет действия (например, регулирует температуру), либо отправляет уведомление пользователю.

**Примеры использования**
- **Умный дом**: термостаты, освещение, замки, управляемые со смартфона.
- **Носимые устройства**: фитнес-трекеры, смарт-часы, отслеживающие активность и здоровье.
- **Промышленность (IIoT)**: мониторинг оборудования, предсказание поломок, оптимизация производства.
- **Умные города**: управление освещением, парковками, мониторинг трафика.
- **Автомобили**: системы помощи водителю, удаленная диагностика.

<dfn title="промышленный интернет вещей">Промышленный интернет вещей</dfn> (*Industrial Internet of Things* — IIoT) — это система объединённых компьютерных сетей и подключённых к ним промышленных (производственных) объектов со встроенными датчиками и программным обеспечением для сбора и обмена данными, с возможностью удалённого контроля и управления в автоматизированном режиме, без участия человека. Это концепция, объединяющая датчики, программное обеспечение и оборудование в единую сеть для сбора и обмена данными в промышленных условиях (заводы, энергетика) с целью автоматизации, повышения эффективности и оптимизации процессов, являясь ключевым элементом Индустрии 4.0.

**Отличие IIoT от IoT** (Интернет вещей):
- IoT – широкий термин для подключенных устройств в быту и повседневной жизни (умные дома, часы).
- IIoT – узкоспециализированное применение IoT для промышленных нужд, где важны надежность, безопасность и масштабируемость для критически важных систем, делая его основой «умных заводов».

Применение Интернета вещей в промышленности создаёт новые возможности для развития производства и решает ряд важнейших задач: повышение производительности оборудования, снижение материальных и энергетических затрат, повышение качества, оптимизация и улучшение условий труда сотрудников компании, рост рентабельности производства и конкурентоспособности на мировом рынке. IIoT трансформирует производство через предиктивное обслуживание, удаленный мониторинг и улучшение контроля, в отличие от потребительского IoT, фокусируясь на сложных операциях.

**Основные компоненты и функции IIoT**:
- **Датчики и устройства**: интеллектуальные датчики и контроллеры, установленные на оборудовании, собирают данные в реальном времени (температура, вибрация, влажность, состояние механизмов).
- **Сеть и передача данных**: данные передаются через IP-сети по стандартизированным протоколам (например, MQTT) для обеспечения универсальной связи.
- **Анализ данных**: полученная информация анализируется для выявления закономерностей, прогнозирования отказов и принятия решений.
- **Автоматизация и управление**: система позволяет удаленно управлять процессами и оборудованием, устраняя необходимость ручного вмешательства.

Промышленный Интернет вещей является ключевым элементом Четвёртой промышленной революции (Индустрия 4.0). Промышленный Интернет вещей применяется не только на производстве, заводах и в тяжелой промышленности, такой как добыча полезных ископаемых, электростанции, авиация и т.д., он также используется в таких отраслях и сферах, как сельское хозяйство, логистика, финансы, государственные услуги, а также в межотраслевых решениях.[^iiot]

**Примеры применения**:
- **Предиктивное обслуживание**: датчики на станках предсказывают поломки до их возникновения, снижая простои.
- **Оптимизация производства**: системы климат-контроля автоматически регулируют условия в цехах.
- **Управление цепочками поставок**: отслеживание грузов и продукции в реальном времени.
- **Энергоэффективность**: мониторинг и оптимизация энергопотребления.

**Преимущества**
- Удобство и автоматизация повседневной жизни.
- Повышение эффективности и экономия ресурсов в бизнесе.
- Улучшение качества жизни и безопасности.

**Недостатки**
- Проблемы безопасности и защиты данных.

#### Архитектура IoT-систем

Типовая архитектура IIoT-систем состоит из следующих 3-х уровней:

1. **Конечные устройства** (**вещи**, *things*) – датчики, сенсоры, контроллеры и прочее периферийное оборудование для измерения необходимых показателей и передачи этих данных в сеть по проводным или беспроводным протоколам (Serial, RS-485, MODBUS, CAN bus, OPC UA, BLE, WiFi, Bluetooth, [6LoRaWAN](https://bigdataschool.ru/blog/iot-ipv6-crypto-big-data/), Sigfox и пр.). Поскольку каждая «порция» этой информации невелика по объему, такие данные называют малыми (Little Data).

2. **Сетевые шлюзы и хабы** (*network*) – роутеры, которые объединяют и подключают конечные устройства к облаку.

3. <dfn title="облако">Облако</dfn> (*cloud*) – удаленный сервер в датацентре, обрабатывающий, анализирующий и надежно хранящий информацию. Именно здесь малые данные превращаются в Big Data, когда консолидируется множество информационных потоков с различных устройств. Так интернет вещей становится «интеллектуальным», поскольку подключаются средства анализа данных, в т.ч. с использованием методов машинного обучения (Machine Learning). Это позволяет эффективно и удаленно управлять техникой, на которой установлены конечные устройства. Например, если датчики уровня вибрации оборудования показывают превышение допустимых значений, можно заранее спланировать профилактический ремонт и избежать поломки дорогостоящих инструментов.[^iot-architecture-big-data]

![IoT, Internet of Things, архитектура](../img/aa1.png)

*3 уровня архитектуры IoT-систем*

#### Как интернет вещей собирает малые данные
**Датчики и сенсоры** измеряют необходимые параметры (температуру, давление, уровень, вибрацию и т.д.), регистрируя изменение окружающей среды, а не ее статическое состояние. Стоимость реализации и использования такого оборудования быстро падает, что позволяет собирать все больше данных при сокращении затрат. Раньше, подключая датчики к системам контроля и управления, можно было работать только с токами потребления в пределах 4–20 мА, протоколом HART или промышленными шинами, а также специализированным ПО. Сегодня возможно использовать самые разные типы проводных и беспроводных сетей для сбора данных, и поэтому даже в пределах одного производства используется сразу несколько типов сетевых подключений.[^big_data_iot]

**Выбор протоколов передачи данных** зависит от следующих факторов:

- **скорость** передачи информации — объем данных, передаваемых за единицу времени;
- **энергопотребление** – сколько времени электроника конечных устройств может работать без подзарядки;
- **дальность** – максимальное расстояние, на которое нужно передать данные;
- **частота** передачи информации (измеряемая в Гц), доступная для использования.

**Выделяют 2 категории датчиков**:

- **активные** – излучают сигналы сами и принимают их отражения, требуют больше энергии;
- **пассивные** – лишь принимают сигналы, что снижает их энергопотребление.

Большинство датчиков основано на волновом принципе — приеме звуковых, ультразвуковых, световых и тепловых волн. Но существуют устройства, измеряющие физические характеристики (индуктивность, емкость, давление и пр.). Комбинируя различные типы датчиков, можно значительно повысить качество и «уровень интеллектуальности» IoT-системы.

![IoT, Internet of Things, архитектура](../img/aa2.png)

*Датчики и сенсоры — источники большого количества малых данных для IoT-систем*

#### Как работает интернет вещей с Big Data
Как правило, в промышленном IoT отсутствует прямой доступ к конечным устройствам, поэтому для соединения уровней технологического оборудования и интеллектуальных систем обработки и хранения информации используются шлюзы.[^415933]

**Конечные устройства являются источниками данных** с низкой вычислительной мощностью, которые непрерывно передают на шлюз множество информации различного формата. Датчик конечного устройства формирует аналоговый сигнал, который преобразуется в цифровое (дискретное) значение с помощью АЦП – аналого-цифрового преобразователя. Это значение маркируется меткой времени и классифицируется (тегируется) локальным процессором конечного устройства. Теги могут быть простыми, например, обнаружено движение, или сложными, из нескольких параметров (движение + скорость, движение + скорость + автомобиль и пр.). Чем сложнее тег, тем более мощным должен быть периферийный процессор и энергопотребление конечного устройства. Однако, более информативные теги позволяют сократить количество передаваемых данных в облако и полосу пропускания информации, а это, в свою очередь, увеличивает скорость реакции на событие.[^420173]

**Шлюз**, в свою очередь, отправляет данные в облачный кластер, где развернута программная IoT-платформа на базе средств Big Data для обработки и интеллектуального анализа информации.

**На облачном сервере** данные от различных периферийных устройств интегрируются (суммируются по тегам), систематизируются и анализируются с применением Machine Learning и других методов искусственного интеллекта. Результаты интеллектуального анализа данных визуализируются в виде графиков, диаграмм и пр., отображаясь в витринах (дэшбордах) пользовательского интерфейса IoT-платформы.[^420173]

![облака, Big Data, Большие данные, интернет вещей, IoT, Internet of Things, архитектура](../img/aa3.png)

*Передача информации с конечного устройства в облако*

Однако, интернет вещей предполагает не только передачу информации с технологических объектов, но и удаленное управление ими. Поэтому реализуется обратная связь от облачной IoT-платформы к периферийному устройству управления необходимым объектом, например, задвижкой на трубе и пр. Для этого в облаке реализуется виртуальное представление периферийного устройства, куда записывается необходимая информация по изменению его состояния, а затем передается на исполнительное устройство конечного оборудования. При этом периферийный процессор выполняет распознавание тегов и ЦАП, т.е. обратное цифро-аналоговое преобразование – из дискретного значения в аналоговую форму.

![облака, Big Data, Большие данные, интернет вещей, IoT, Internet of Things, архитектура](../img/aa4.png)

*Передача данных с IoT-платформы на конечное устройство*

Вся IoT-система является распределенной и масштабируемой, однако, связанной недостаточно надежными каналами передачи данных. Поэтому используются механизмы гарантированной доставки информации. В частности, если не удается передать данные от конечного устройства в облако или наоборот, осуществляются повторные попытки передачи.[^420173] Для обмена сигналами между компонентами распределенной системы используются специальные решения – брокеры сообщений, которые гарантируют доставку нужных данных одному или нескольким получателям через управляемую очередь.[^очередь-сообщений]

Наиболее популярными брокерами сообщения считаются RabbitMQ, Apache Qpid, Apache ActiveMQ. Также для этих целей используется распределенный реплицированный журнал фиксации изменений [Apache Kafka](https://bigdataschool.ru/wiki/kafka/), который отлично масштабируется, обеспечивая наращивание пропускной способности при росте числа и нагрузки со стороны источников данных, а также количества приложений по их обработке (подписчиков).[^416629] Для быстрой загрузки данных с конечных устройств часто используется платформа обработки событий (сообщений) Apache NiFi или ее упрощенная модификация Apache [MiNiFi](https://bigdataschool.ru/wiki/nifi/). Подробнее про средства обработки данных в облаке, в рамках IoT-платформы повествует следующий раздел.

![облака, Big Data, Большие данные, интернет вещей, IoT, Internet of Things](../img/aa5.png)

*Интернет вещей — это целая экосистема программных и аппаратных компонентов*

#### Роль Big Data и ML в IoT
Интернет вещей (IoT) дает компаниям и организациям возможность контролировать удаленно расположенные «дешевые» вещи /объекты  получая с них информацию и выполняя мониторинг удаленно исполняемых операций. Учитывая огромное количество «умных» устройств в промышленности и повседневной жизни мы получаем колоссальную базу источников поступления информации (больших данных) в реальном времени.[^internet-of-things]

Internet of Things (IoT) имеет тесную связь с Big Data, поскольку IoT генерирует огромные объемы данных из множества подключенных устройств, которые требуют хранения, обработки и анализа в рамках Big Data. IoT-устройства, такие как датчики и умные приборы, производят потоковую информацию в реальном времени, которая консолидируется в Big Data для систематизации и масштабирования. Это превращает разрозненные данные в ценный ресурс для мониторинга и управления. Без Big Data инфраструктуры IoT-системы не справляются с объемами информации.

Machine Learning применяется поверх Big Data для анализа собранных данных, выявления паттернов и предиктивной аналитики, делая IoT "умным". Например, ML предсказывает сбои оборудования на основе IoT-данных. Однако ML — это инструмент обработки Big Data, а не основа IoT.

*Сравнение зависимостей*

Аспект |	Big Data в IoT |	Machine Learning в IoT
-- | -- | --
Основная функция |	Хранение и обработка потоков данных |	Анализ данных для прогнозов и автоматизации​
Необходимость |	Фундаментальна для масштаба |	Дополняет для интеллекта
​Примеры |	Агрегация данных с датчиков | 	Прогноз вибрации оборудования

### Представление данных для машинного обучения. Признаки объектов
В каком виде данные подаются машине?

Классический способ — представление данных в виде таблицы.

| Объекты | Признак 1 | Признак 2 | $\dots$ | Признак $m$
-- | -- | -- | -- | --
$A_1$ | $P_{11}$ | $P_{12}$ | $\dots$ | $P_{1m}$
$A_2$ | $P_{21}$ | $P_{22}$ | $\dots$ | $P_{2m}$
$\dots$ | $\dots$ | $\dots$ | $\dots$ | $\dots$
$A_n$ | $P_{n1}$ | $P_{n2}$ | $\dots$ | $P_{nm}$

- Здесь строки — это объекты, а столбцы — признаки. Таким образом, у каждого объекта есть несколько признаков.

- Соответственно, в данной выборке $n$ объектов, у каждого из которых имеется $m$ признаков.

- Число $n$ называется <dfn title="объем выборки">объёмом выборки</dfn>.

С помощью $P(A)$ будем обозначать значение признака $P$ для объекта $A$.

##### Типы признаков
Признаки могут очень сильно отличаться по своей природе.

- <dfn title="количественный признак">Количественный</dfn> (<dfn title="числовой признак">числовой</dfn>): признак, выражаемый по своему физическому смыслу в виде некоторого вещественного (действительного) числа. Область его значений (см. область отправления и прибытия, область определения и значений) — вещественные числа, и сам признак имеет числовую природу.

- <dfn title="порядковый признак">Порядковый</dfn>: не имеет числовой природы, он всего лишь задает порядок (см. отношение строгого порядка) на некотором множестве объектов. Порядковые признаки иначе называют <dfn title="псевдоколичественный признак">псевдоколичественными</dfn>, поскольку хотя они и имеют представление в виде числа, но не выражают количественных характеристик. Дело в том, что есть числовые признаки, а есть числа, но числовыми признаками, как ни странно, не являющиеся. Отличие первых от вторых в том, что над первыми можно производить операции, свойственным числам (арифметические), а над вторыми нет, ибо смысла они не имеют. Порядковые признаки поддерживают только операции сравнения — проверки на равенство (равно / не равно) и сравнения между собой (больше / меньше и т.д.). Также псевдоколичественные признаки позволяют работать с модой и медианой, но никак не со средними!

- <dfn title="номинальный признак">Номинальный</dfn> (<dfn title="категориальный признак">категориальный</dfn>): признак не имеет числовой природы и, как правило, число его возможных значений конечно (вспомнить про перечисления в ЯП — набор именованных констант). В частности, <dfn title="бинарный признак">бинарный признак</dfn> — это номинальный признак с двумя возможными значениями.

*Пример*

| Студент | Пол | Рост | Вес | Группа крови | Место на олимпиаде
-- | -- | -- | -- | -- | --
Иванов | 1 | 172 | 107 | 1 | 3
Запеканка | 1 | 185 | 64 | 2 | 4
Ватрушкина | 0 | 168 | 61 | 3 | 2
Ололоева | 0 | 201 | 85 | 4 | 1

- Рост и вес — это вещественные числа (**количественный** признак).
- Пол — это **бинарный** признак (в классическом понимании). Числа 0 и 1 не обозначают возможности совершения арифметических действий с признаками (например, сложение мужчины с женщиной не дает мужчину, равно как сложение женщин не дает женщину) или некоторую важность (приоритет, превосходство) одного над другим. Другими словами, это просто метки классов и не более того.
- Группа крови — это **номинальный** (но не бинарный) признак. Хотя исторически она и выражается в числах, тем не менее по факту она не имеет числовой природы, поскольку с группами крови невозможно выполнение арифметических операций и операци сравнения.
- Место на олимпиаде — это **порядковый** признак. Хотя места на олимпиаде и выражаются числами, но их нельзя складывать, умножать и т.д., т.к. это по сути не числа, а **ранги**, выражающие порядок.

!!! tip Упражнение

    - Какие еще числовые, порядковые, бинарные признаки существуют у человека?

      <details>
      <summary><em>Признаки человека</em></summary>

      Числовые признаки:

        - возраст;

        - индекс массы тела (ИМТ);

        - давление крови (систолическое и диастолическое);

        - частота пульса и дыхания;

        - уровень гормонов (например, количество тестостерона, эстрогена);

        - количество гемоглобина и других клеток крови.

        Порядковые признаки:

        - уровень образования;

        - степень физической активности;

        - категория состояния здоровья по шкале (например, нормальное, предболезненное, болезненное);

        - порядок рождения в семье (первый ребенок, второй и т.д.).

      Бинарные признаки:

        - резус-фактор крови (положительный или отрицательный);

        - наличие определенных заболеваний (например, диабет: есть/нет);

        - курение (курит/не курит);

        - способность сворачивать язык трубочкой (да/нет);

        - левша или правша.

      Таким образом, кроме основных физиологических и биометрических характеристик, у человека много других признаков разного типа, которые используются в медицине, генетике, социологии и других областях для более полного описания и анализа личности и состояния здоровья человека

      </details>

    - А слабо найти номинальный, но не бинарный признак у человека?

      <details>
      <summary><em>Номинальные признаки человека</em></summary>

      Пример номинального, но не бинарного признака у человека — это цвет глаз. Это качественный признак, который имеет несколько категорий (например, синий, карий, зеленый, серый), при этом между категориями нет упорядочивания или ранжирования, то есть нельзя сказать, что один цвет глаз "больше" или "лучше" другого. Такой признак является номинальным, так как это просто разные категории без внутреннего порядка.

      Другие примеры номинальных немонопольных признаков: национальность, род занятий, цвет волос, семейное положение (холост, женат, разведен, овдовевший) — все они представляют разные категории без упорядочивания, и не сводятся к бинарному разрезу.

      </details>

А теперь представьте, что у вас есть объекты более сложной природы. Как представить в виде строк таблицы набор картинок, аудио-файлов, текстов и т.п.?

Допустим, есть черно-белые рисунки, состоящие из 9 пикселов. Для простоты представим, что они имеют один и тот же размер (3 &times; 3 = 9 пиксейлей), причем все пиксели монохромные.

![Image set](../img/img-set.png)

Самое простое — это ввести 9 бинарных признаков. Нумеруем пиксели. Получаем 9 бинарных (так как рисунки черно-белые) признаков, каждый из которых будет отвечать за цвет того или иного пикселя.

![Numbered box](../img/numbered_box.png)

!!! tip Упражнение

    Сколько вариантов квадратов 3×3 с монохромными пикселями всего возможно?

    <details>
    <summary><em>Ответ</em></summary>

    В квадрате 3×3 всего 9 пикселей. Если каждый пиксель монохромный, то у него может быть 2 состояния: черный или белый (0 или 1).

    Количество всех возможных комбинаций расположения таких пикселей вычисляется как $2^9$, так как каждый из 9 пикселей может принимать 2 состояния независимо друг от друга.

    С точки зрения комбинаторики это относится к числу сочетаний с повторениями, где каждый элемент массива (пиксель) имеет 2 возможных состояния, и они могут повторяться независимо. Это можно рассматривать как вариант размещения с повторениями — выбор из двух вариантов для каждого из 9 мест без ограничений на количество повторений.

    Формула для подсчёта количества всех возможных комбинаций для $n$ и $k$ вариантов значений каждой позиции:

    $$ k^n $$

    В данном случае: $k=2$ (черный или белый), $n=9$ (пиксели в квадрате 3×3), значит

    $$ 2^9 = 512 $$

    Итого, всего возможно 512 различных комбинаций расположения монохромных пикселей в квадрате 3×3. Это не перестановка или классическое сочетание, а именно размещение с повторениями, так как порядок в 3×3 фиксирован (позиции разные) и каждая позиция выбирает значение независимо.

    </details>


*Превращение рисунка в строку из таблицы*

| Рисунок | пр1 | пр2 | пр3 | пр4 | пр5 | пр6 | пр7 | пр8 | пр9 |
-- | -- | -- | -- | -- | -- | -- | -- | -- | --
Рис1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1
Рис2 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1
Рис3 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0

Аналогичным образом можно закодировать любые другие монохромные картинки (разных размеров).

!!! question Какие недостатки имеются у такого представления рисунков?

    Главный недостаток такого представления — это то, что в процессе перехода от рисунка к строке теряется информация о смежности пикселей, т.е. утрачивается информацию о том, какие пиксели были расположены рядом друг с другом. Это главная проблема подобного представления рисунков.

##### Ограничения табличного представления объектов
Таблично представление объектов накладывает различные ограничения. Самое главное ограничение — это то, что все объекты должны иметь одинаковое количество признаков (чтобы их можно было поместить в одну таблицу). Выполнить это требование не всегда легко:
1. Если объект $A$ имеет более сложную структуру (более богатую историю) чем $B$.
2. Если объекты $A$ и $B$ – картинки, то они должны иметь одинаковый формат и размер.
3. Что делать если объекты имеют совершенно разную структуру (форму)? Например, это актуально для земельных участков.


### Заключение
В заключение, большие данные – это не просто термин или модный тренд. Это катализатор технологического прогресса и новое направление в развитии информационных технологий, которое уже сейчас определяет будущее нашего цифрового мира.[^mashinnoe-obuchenie-i-big-data]

##### Выводы
- Данные машине подаются в виде таблиц.
- Каждый объект в таблице описывается с помощью набора признаков.
- Признаки бывают разного типа.

### Практическая работа. Представление признаков в табличной форме

#### Задание
Сформировать таблицу с выборкой, содержащей признаки рисунков в соответствии со своим индивидуальным вариантом. Образец (вариант 0) представлен в прилагаемом файле *task_01.xlsx*, содержащем исходные данные по вариантам, в том числе индивидуально для каждого студента по группам (на соответствующих листах).

Задание представить в виде заполненного файла (свой фрагмент) в соответствии с представленным образцом.

### Глоссарий

ACID (Atomicity, Consistency, Isolation, Durability)
: набор из четырех требований, которые обеспечивают надежную работу транзакционных систем, как описано в этой статье. **Atomicity** (Атомарность): транзакция выполняется полностью или не выполняется вообще, без промежуточных состояний, как в примере с переводом денег. **Consistency** (Согласованность): транзакция приводит базу данных из одного согласованного состояния в другое, соблюдая все правила целостности. **Isolation** (Изоляция): параллельные транзакции не влияют друг на друга, каждая "видит" только завершенные изменения. **Durability** (Надежность): после успешного завершения транзакции (фиксации) ее изменения сохраняются навсегда, даже при сбоях.

Big Data
: крупные массивы разнообразной информации и стек специальных технологий для работы с ней. Термин применяется к таким объемам данных, с которыми пользовательский компьютер и офисные программы не справятся. С помощью анализа больших данных бизнес может получить возможность принимать решения по развитию продукта и завоевывать конкурентное преимущество.

Business Intelligence (BI)
: технология аналитики, которая призвана предоставлять компании доступ к целостной картине деятельности в режиме реального времени. BI-системы отображают ключевые метрики, формируют детализированные отчёты и помогают ориентироваться в изменениях, оказывая поддержку при принятии стратегических решений.

Dashboarding
: визуальное представление показателей, помогающее отслеживать изменения в режиме реального времени. Гибкие дашборды упрощают работа с ключевыми метриками и улучшают процессы управления, за счет чего повышается понимание текущих тенденций и возможностей компании.

Data Lake (озеро данных)
: универсальный репозиторий для хранения данных в формате «как есть» — в структурированном, полуструктурированном и неструктурированном виде. Данные в хранилище поступают непрерывно в неструктурированном или, наоборот, структурированном или слабоструктурированном виде. Такой подход упрощает сбор и первичную агрегацию данных из разных источников, сохраняя их неизменными до момента необходимого анализа. Озеро данных используется для сбора данных из разных источников в режиме реального времени.

Data Mart (витрина данных)
: хранилище данных, предназначенных для повседневного использования. Это локальное или специализированное подручное хранилище данных, ориентированное на определённый бизнес-подразделение или процесс, с более быстрым доступом и меньшим объёмом данных по сравнению с DWH. Поступающую информацию необходимо тщательно обрабатывать, но после этого к ней проще регулярно обращаться.

Data Science (наука о данных)
: методология, в рамках которой применяются машинное обучение и нейронные сети. Такой подход помогает создавать модели для прогнозирования спроса, удержания аудитории или оптимизации маркетинговых кампаний, задействуя весь доступный объем данных.

Data Vault (свод данных)
: одна из моделей хранилища Data Warehouse с временными отметками размещения данных, которые позволяют проследить изменение хранимой информации во времени.

Data Warehouse (DWH, хранилище данных)
: централизованное хранилище структурированных данных, оптимизированное под единый путь анализа, бизнес-отчётности и сложные запросы. Часто применяется после этапов очистки и трансформации данных.

DAS (англ. *Direct-attached storage* — система хранения данных с прямым подключением, дисковое хранилище)
: запоминающее устройство, непосредственно подключённое к серверу или рабочей станции, без помощи сети хранения данных. Это ретроним, используемый в основном для отличия несетевых устройств хранения от SAN и NAS. Де-факто DAS — это быстрое (если интерфейс быстрый) локальное хранилище, доступное только тому устройству, к которому оно подключено. Жёсткий диск внутри ПК тоже своего рода DAS. DAS часто называют «островами информации».

Data Lake (*озеро данных*)
: технология для получения и управления данными в разных форматах: в необработанном, неупорядоченном или, наоборот, структурированном или слабоструктурированном виде, в едином репозитории.

Data Warehouse (хранилище данных)
: единая среда хранения, рассчитанная на большой объем корпоративной информации. Представляет собой единое корпоративное хранилище с обработанной и структурированной информацией. Хранилище упрощает анализ полученных данных, но требует структурированности. Грамотная архитектура DWH обеспечивает стабильную работу основных сервисов, а также сбор и консолидацию всех данных для дальнейших аналитических вычислений.

ELT (Extract, Load, Transform — Извлечение, Загрузка, Преобразование)
: современный метод интеграции данных, противоположный ETL, где данные сначала загружаются в хранилище, а затем преобразуются по мере необходимости, используя мощь облачных хранилищ для гибкого анализа больших объемов.

ERP-система
: конкретный программный пакет, реализующий стратегию ERP.

ETL (*Extract, Transform, Load* — «Извлечение, Преобразование, Загрузка»)
: процесс интеграции данных, который собирает информацию из разных источников, очищает, преобразует её в единый формат и загружает в централизованное хранилище (например, «склад данных» или DWH) для дальнейшего анализа, отчетности и поддержки бизнес-решений. Он позволяет объединять разнородные данные, делая их согласованными и пригодными для использования. Это средства извлечения, преобразования и загрузки данных, которые позволяют хранилищу взаимодействовать с окружающим миром. С их помощью данные попадают в хранилище, обрабатываются и конвертируются в нужный формат, выгружаются на сервер для аналитической обработки и возвращаются обратно для просмотра результатов.

Greenplum Database (Arenadata DB)
: массивно-параллельная СУБД (Система управления базами данных), предназначенная для обработки больших объемов данных и их анализа, превращая их в источник роста и конкурентных преимуществ.

HMI (*Human-Machine Interface*, *человеко-машинный интерфейс*)
: устройство или программный комплекс, обеспечивающий взаимодействие человека с машиной, компьютером или процессом, чаще всего в промышленности, позволяя оператору наблюдать, контролировать и управлять оборудованием через визуальные экраны, кнопки и сенсорные панели для мониторинга данных и команд в реальном времени. HMI-панели не требуют подключения к ПК и напрямую связываются с PLC (программируемыми логическими контроллерами) для визуализации процессов, сбора данных и управления производственными операциями, повышая эффективность автоматизации. В итоге, HMI выступает как "лицо" промышленной системы, делая сложный технологический процесс доступным для оператора.

HOLAP (*Hybrid Online Analytical Processing*)
: технология гибридной аналитической обработки данных, которая сочетает лучшие черты MOLAP (многомерного OLAP) и ROLAP (реляционного OLAP), храня агрегированные данные в многомерных кубах для быстрого анализа, а исходные данные — в реляционных базах, что обеспечивает гибкость и масштабируемость для сложных запросов и детализации (drill-down) до первичного уровня. Эта модель позволяет быстро получать сводные отчеты (как MOLAP) и одновременно обращаться к детальным данным (как ROLAP), что критически важно для бизнес-аналитики.

MOLAP (*Multidimensional Online Analytical Processing*)
: технология анализа данных, которая хранит и обрабатывает данные в виде многомерных кубов (массивов) для очень быстрого выполнения сложных аналитических запросов, агрегируя и оптимизируя их заранее, что обеспечивает высокую производительность для бизнес-аналитики, но требует больше ресурсов и сложнее обновляется при больших изменениях данных.

NAS (англ. *Network Attached Storage*, сетевое хранилище)
: сервер для хранения данных на файловом уровне. По сути представляет собой компьютер с некоторым дисковым массивом, подключённый к сети (обычно локальной) и поддерживающий работу по принятым в ней протоколам. Несколько таких компьютеров могут быть объединены в одну систему.

OLAP (*OnLine Analytical Processing*)
: оперативная аналитическая обработка данных или анализ данных в реальном времени. Термин впервые использовал создатель теории реляционных баз данных Эдгар Кодд в 1993 г. в статье «OLAP для пользователей-аналитиков: каким он должен быть».

OLAP-кубы
: структурированный инструмент, необходимый для многомерного анализа показателей. Благодаря ему специалисты выявляют скрытые закономерности, проводят глубокую обработку информации и прогнозируют динамику развития.

OLTP (*OnLine Transaction Processing*)
: технология и система обработки транзакций в реальном времени. Она обеспечивает непрерывное занесение данных в базу, их модификацию и извлечение. Данные размещаются в простых таблицах и, как следствие, занимают не так много места в хранилище. Но главное: OLTP-системы не предназначены для комплексного анализа данных, скорее, это инструмент их массового сбора и преобразования.

ROLAP (*Relational Online Analytical Processing*)
: подход к аналитической обработке данных, который использует традиционные реляционные базы данных (РБД) и язык SQL для многомерного анализа больших объёмов информации, в отличие от MOLAP, где данные хранятся в многомерных кубах. Он позволяет пользователям выполнять сложные запросы к данным в реальном времени, извлекая срезы данных напрямую из хранилищ (часто в схемах "звезда" или "снежинка"), что обеспечивает гибкость и масштабируемость.

SAN (англ. *Storage Area Network*, сеть хранения данных)
: архитектурное решение для подключения внешних устройств хранения данных, таких как дисковые массивы, ленточные библиотеки, оптические приводы к серверам таким образом, чтобы операционная система распознала подключённые ресурсы как локальные.

SCADA (*Supervisory Control and Data Acquisition*)
: программно-аппаратный комплекс для **диспетчерского управления и сбора данных** в реальном времени, используемый для мониторинга, контроля и автоматизации технологических процессов на промышленных объектах, от энергетики и ЖКХ до производства. Системы SCADA собирают данные с датчиков, отображают их в понятном виде (человеко-машинный интерфейс), позволяют оператору управлять оборудованием удаленно, сигнализируют о неполадках, повышают точность процессов и снижают брак.

Агрегация данных (*data aggregation*)
: процесс обобщения информации в сводную форму для анализа. Агрегация в пайплайне следует после интеграции: сначала сливаются источники, затем данные суммируются. Уменьшает объём данных, ускоряя обработку без потери ключевых инсайтов. Агрегация снижает нагрузку на хранение, интеграция обеспечивает полноту. Результат — компактный датасет с метриками, удобный для отчётов и визуализации.

Аналитик (*analyst*)
: специалист, который собирает, обрабатывает и интерпретирует данные, превращая их в понятные выводы и рекомендации для принятия обоснованных решений в бизнесе, IT или других сферах.

Аналитик данных (*data analyst*)
: специалист, который собирает, обрабатывает и интерпретирует большие объемы данных, чтобы помочь компаниям принимать обоснованные решения, выявлять закономерности и тренды, оптимизировать процессы и снижать риски. По сути, он "переводит" хаотичные цифры в понятные выводы и рекомендации для бизнеса, используя статистические методы и инструменты визуализации. Данный специалист занимается проверкой статистических гипотез, проектирует и проводит эксперименты. Для этого требуются фундаментальные знания математической статистики, а также необходимо держать руку на пульсе бизнеса.

Аналитика настроений (*sentiment analysis*)
: изучение отзывов клиентов и обсуждений продукта в соцсетях, чтобы выявить слабые стороны продукта и уровень удовлетворенности потребителей.

Аналитические инструменты
: программы и приложения, которые выступают в роли посредников между пользователем и хранилищем данных. С их помощью аналитики формируют запросы к базам данных и отчеты с наглядным представлением важной для бизнеса информации.

Бизнес-аналитик (*business analyst*)
: специалист, который изучает бизнес-процессы компании, выявляет проблемы и возможности для повышения эффективности, а затем переводит потребности бизнеса на понятный язык для команд разработки, предлагая и помогая внедрять решения для достижения целей компании. Это человек, который отвечает на вопросы бизнеса, и его плотность вероятности приходится на предметную область. Он выступает связующим звеном между бизнесом и IT-специалистами, работая над оптимизацией процессов, созданием новых продуктов и принятием управленческих решений, основанных на фактах.

Бэк-офис (от англ. *back* — задняя часть)
: внутренние отделы компании, которые выполняют обслуживающие и административные функции и не взаимодействуют напрямую с клиентами и заказчиками. Среди прочего в круг задач входит оптимизация и автоматизация рабочих процессов, упразднение неэффективных операций. Фактически большинство компаний имеют бэк-офис, но не всегда выделяют его как нечто обособленное. Как правило, к бэк-офису относятся: юристы, бухгалтеры и экономисты, HR, IT-специалисты, аналитики. Существует противоположное понятие — фронт-офис. В него входят сотрудники, взаимодействующие с клиентами. В небольших компаниях обычно нет территориального разделения на фронт- и бэк-офис: все сотрудники работают в одном месте.

Бинарный признак (*binary feature*)
: номинальный признак с двумя возможными значениями.

Большие данные (*big data*)
: наборы данных настолько объёмные или сложные, что традиционные методы обработки не справляются с ними эффективно.

Данные (*data*)
: необработанные, сырые факты и цифры, не имеющие самостоятельного смысла

Инженер данных (дата-инженер, *data engineer*)
: IT-специалист, который строит и обслуживает инфраструктуру для сбора, хранения, обработки и перемещения больших объемов данных, делая их доступными и готовыми для аналитиков и ученых по данным (*Data Scientists*). Это тот человек, который занимается ETL-процессами, архитектурой хранилища, составляет витрины и поддерживает их, организовывает потоковую обработку данных. По сути, он «архитектор и строитель» мира данных, который обеспечивает надежные потоки информации, чтобы другие специалисты могли извлекать из них ценные инсайты.

Инженер машинного обучения (ML-инженер, *Machine Learning Engineer*)
: специалист, который создает, обучает и внедряет модели искусственного интеллекта (ИИ), позволяя компьютерам решать задачи, анализируя большие объемы данных без явного программирования каждого шага. Он трансформирует идеи в рабочие системы, интегрируя их в реальные бизнес-процессы, оптимизируя производительность и обеспечивая стабильную работу в продакшне, занимется введением в эксплуатацию моделей машинного обучения и поддерживает их в актуальном состоянии. Для этого требуются навыки и знания в области алгоритмов машинного обучения, ну и, конечно, разработки.

Инсайт (*insight*)
: глубокое, неожиданное понимание или ценное открытие, полученное из данных, которое приводит к практическим рекомендациям. Инсайт отличается от обычных фактов тем, что объясняет "почему" и "как использовать": например, не просто "продажи упали на 20%", а "отток вырос из-за задержек доставки в регионы X и Y". Он рождается на стыке данных, гипотез и контекста.

Интеграция даных (*data integration*)
: объединение данных из разных источников в единый набор данных (*dataset*).

Интеллектуальный анализ данных (*data mining*)
: подмножество анализа для поиска скрытых зависимостей, процесс «просеивания» больших массивов данных с целью извлечь из них ценную информацию для конкретного применения. Он является неотъемлемой частью науки о данных и бизнес-аналитики и направлен в первую очередь на поиск закономерностей.

Интернет вещей (*Internet of Things* — IoT)
: глобальная сеть физических устройств («вещей»), оснащенных датчиками и программным обеспечением, которые позволяют им подключаться к интернету и обмениваться данными друг с другом и с другими системами без участия человека, что создает «умные» объекты, автоматизирует процессы и делает нашу жизнь удобнее и эффективнее. Эти «вещи» могут быть бытовой техникой, автомобилями, промышленным оборудованием, носимыми устройствами и многим другим, собирая и передавая данные о температуре, влажности, движении и других параметрах.

Информация (*information*)
: обработанные, структурированные и контекстуализированные данные, которые приобретают значение и цель для пользователя.

Исследователь машинного обучения (*machine learning researcher*)
: исследователь, ориентированный на разработку новых методов и архитектур, проведение экспериментов и публикацию научных статей. Часто работает в академическом или R&D-контексте. Данный специалист в основном занимается исследовательской работой. Пишет и изучает статьи, придумывает новые математические методы.

Количественный (числовой) признак
: признак, выражаемый по своему физическому смыслу в виде некоторого вещественного (действительного) числа. Область его значений (см. область отправления и прибытия, область определения и значений) — вещественные числа, и сам признак имеет числовую природу.

Корпроративное хранилище данных (*Data Warehouse*, DWH)
: централизованная система, где собираются данные из разных источников: например из CRM, с сайта, из мобильного приложения и ERP. В отличие от обычной базы, которая нужна для оперативной работы приложений, например чтобы быстро оформлять заказы или собирать данные пользователей, хранилище создаётся именно для аналитики. В нём данные очищаются, приводятся к единому формату и структурируются так, чтобы их было удобно анализировать.

Локальное развертывание (*on-premise* или *on-premises*)
: модель развертывания программного обеспечения и инфраструктуры, когда все ресурсы, включая оборудование и данные, находятся внутри компании, на её собственных серверах, а не в облаке. Локальное решение относится к программной или технологической инфраструктуре, которая устанавливается и обслуживается на собственных серверах или оборудовании компании. Локальные решения считаются более безопасными, поскольку компания имеет полный контроль над инфраструктурой, но они также могут быть более дорогостоящими и трудоемкими в управлении по сравнению с облачными или веб-решениями.

Маркетинговая аналитика (*marketing analytics*)
: изучение данных о клиентах, чтобы улучшить маркетинговые компании и разработать бизнес-инициативы.

Массово-параллельная обработка (*Massively Parallel Processing*, MPP)
: архитектура вычислений, которая использует сотни или тысячи процессоров (узлов) для одновременного решения одной сложной задачи, разбивая ее на мелкие части, которые выполняются параллельно, что обеспечивает высокую скорость обработки больших объемов данных. Каждый узел в MPP системе имеет свою память и ресурсы, работает независимо, а обмен данными происходит через высокоскоростную сеть.

Наука о данных (*data science*)
: междисциплинарная область, которая использует научные методы, процессы, алгоритмы и системы для извлечения знаний и инсайтов из больших и структурированных или неструктурированных данных, объединяя математику, статистику, программирование и аналитику для решения сложных задач, прогнозирования и принятия решений. По сути, это превращение хаоса сырых данных в полезную информацию, модели и прогнозы, которые используются в бизнесе, науке, медицине и других сферах. 

Номинальный (категориальный) признак
: признак, не имеющий числовой природы и, как правило, число его возможных значений конечно.

Облако (*cloud*) – удаленный сервер в датацентре, обрабатывающий, анализирующий и надежно хранящий информацию.

Облачное решение (*on-cloud*)
: Программное обеспечение или услуга, предоставляемая через интернет или сеть удаленных серверов. Подобное решение позволяет компаниям получать доступ и использовать программное обеспечение или сервис из любого места, где есть подключение к интернету, поскольку данные и приложения размещаются на удаленных серверах, а не на собственном компьютере пользователя или на локальных серверах. Облачные решения часто обеспечивают такие преимущества, как масштабируемость, доступность и экономичность, поскольку они не требуют поддержки аппаратной или программной инфраструктуры.

Обогащение данных (*data enrichment*)
: добавления дополнительной ценной информации к существующим наборам (например, дополняет записи новыми атрибутами из внешних источников). Отличается от агрегации расширением (добавление), а не сжатием данных.

Озеро данных (*Data Lake*)
: логическая совокупность репозиториев данных, предназначенных для хранения и анализа больших данных в их исходном формате. В отличие от традиционного понимания централизованного хранилища, Data Lake может быть распределенным по множеству физических местоположений, включая облачные платформы, on-premises инфраструктуру или гибридные среды. Концепция озера данных представляет собой эволюционный подход к управлению корпоративными данными, обеспечивающий гибкость, масштабируемость и экономическую эффективность в условиях постоянно растущих объемов информации.

Оператор машинного обучения (MLOps-инженер)
: специалист на стыке Data Science и DevOps, который автоматизирует и управляет всем жизненным циклом моделей машинного обучения (ML) — от экспериментов и разработки до развертывания (deployment), мониторинга и поддержки в промышленной эксплуатации, делая ML-системы надежными, масштабируемыми и стабильными. Он создает конвейеры (pipelines) CI/CD для моделей, контейнеризирует их, настраивает мониторинг и обеспечивает интеграцию с IT-инфраструктурой, превращая прототипы в реальные бизнес-решения. Этот человек максимально сосредоточен на разработке и развёртывании кода в продакшене.

Очистка данных (*data cleaning*)
: процесс выявления и устранения ошибок, несоответствий и "грязи" в датасетах для повышения их качества. Очистка обеспечивает точность ML-моделей, снижая шум и предвзятость. Включает следующие методы: удаление дублей, пропусков, выбросов и исправление ошибок.

Планирование ресурсов предприятия (Enterprise Resource Planning, ERP)
: организационная стратегия интеграции производства и операций, управления трудовыми ресурсами, финансового менеджмента и управления активами, ориентированная на непрерывную балансировку и оптимизацию ресурсов предприятия посредством специализированного интегрированного пакета прикладного программного обеспечения, обеспечивающего общую модель данных и процессов для всех сфер деятельности.

Пожизенная ценность клиента (*Lifetime Value*, LTV)
: метрика, показывающая, сколько денег клиент принёс компании за всё время взаимодействия с ней. Это может быть и чистая прибыль от клиента, и доход от всех его заказов. Метрику используют, чтобы понять, оправданы ли затраты на привлечение, вовлечение и удержание клиента. Другие названия показателя — CLV и CLTV: Customer Lifetime Value.

Порядковый (псевдоколичественный) признак
: не имеет числовой природы, он всего лишь задает порядок (см. отношение строгого порядка) на некотором множестве объектов.

Программируемый логический контроллер (ПЛК)
: специализированный промышленный микрокомпьютер для автоматизации процессов, надежно работающий в жестких условиях. Читает данные с датчиков, анализирует их по программе и управляет исполнительными механизмами (двигатели, насосы, клапаны). Создан для работы в режиме реального времени в суровых промышленных условиях, в отличие от обычных компьютеров, ориентированных на пользователя. Используется для управления станками, производственными линиями, системами вентиляции, освещением, насосами. Интегрируются в IIoT (промышленный интернет вещей) для сбора данных и предиктивного обслуживания оборудования.

Промышленный интернет вещей (*Industrial Internet of Things* — IIoT)
: система объединённых компьютерных сетей и подключённых к ним промышленных (производственных) объектов со встроенными датчиками и программным обеспечением для сбора и обмена данными, с возможностью удалённого контроля и управления в автоматизированном режиме, без участия человека. Это концепция, объединяющая датчики, программное обеспечение и оборудование в единую сеть для сбора и обмена данными в промышленных условиях (заводы, энергетика) с целью автоматизации, повышения эффективности и оптимизации процессов, являясь ключевым элементом Индустрии 4.0. IIoT трансформирует производство через предиктивное обслуживание, удаленный мониторинг и улучшение контроля, в отличие от потребительского IoT, фокусируясь на сложных операциях.

Реальное время
: режим работы системы, когда важны не только правильность обработки и предоставления информации, но и своевременность ее обработки. Информация в системах реального времени (СРВ) должна быть обработана за определенное фиксированное время, либо до наступления какого-либо контрольного события.

Реляционные базы данных (*relational databases*)
: классические системные решения для хранения структурированной информации с поддержкой транзакционности, целостности данных и SQL-запросов.

Решение на основе данных (*data-driven approach* — управляемый данными подход)
: стратегия принятия решений в бизнесе и управлении, основанная не на интуиции или опыте, а на глубоком анализе фактических данных и метрик для повышения точности, снижения рисков и поиска точек роста. Он включает сбор, обработку, анализ данных и их использование для формирования планов действий, от маркетинга до управления продуктом.

Система управления взаимоотношениями с клиентами (CRM, CRM-система, сокращение от англ. *customer relationship management*)
: прикладное программное обеспечение для организаций, предназначенное для автоматизации стратегий взаимодействия с заказчиками (клиентами), в частности, для повышения уровня продаж, оптимизации маркетинга и улучшения обслуживания клиентов путём сохранения информации о клиентах и истории взаимоотношений с ними, установления и улучшения бизнес-процессов и последующего анализа результатов. CRM — модель взаимодействия, основанная на теории, что центром всей философии бизнеса является клиент, а главными направлениями деятельности компании являются меры по обеспечению эффективного маркетинга, продаж и обслуживания клиентов. Поддержка этих бизнес-целей включает сбор, хранение и анализ информации о потребителях, поставщиках, партнёрах, а также о внутренних процессах компании. Функции для поддержки этих бизнес-целей включают продажи, маркетинг, поддержку потребителей.

Сбор данных (*data collection*)
: процессы получения информации из различных источников для последующего анализа и использования.

Сравнительная аналитика (*competitive analytics*)
: изучение поведения потребителей и их вовлеченность в режиме реального времени, чтобы сравнить продукт компании с продуктами конкурентов.

Транзакция (*transaction*)
: набор запросов, который выполняется в единой очереди. При неудачном или ошибочном завершении любого из запросов отменяется результат работы всего набора запросов.

Трансформация данных (*data transformation*)
: процесс преобразования сырых данных в удобный для анализа или хранения формат. Включает следующие методы: нормализация, кодирование категориальных переменных, масштабирование признаков.

Хранение данных (*data retention*, *data storage*)
: процессы сохранения, организации и обеспечения доступа к информации на цифровых носителях.

### Источники информации
[^38087337]: [Подскажите пожалуйста в чем заключается отличие информации от данных,и приведите примеры этих отличий](https://otvet.mail.ru/question/38087337)
[^resheniya-bi-big-data]: [Решения на основе Big Data](https://www.decosystems.ru/uslugi/resheniya-bi-big-data/)
[^chto-takoe-ltv-zachem-eye-schitat-i-kak-pravilno-eto-delat]: [Что такое LTV, зачем её считать и как правильно это делать](https://skillbox.ru/media/marketing/chto-takoe-ltv-zachem-eye-schitat-i-kak-pravilno-eto-delat/)
[^bek-ofis]: [Бэк-офис](https://netology.ru/glossariy/bek-ofis)
[^data-lake]: [Data Lake](https://bigdataschool.ru/wiki/data-lake/)
[^crm]: [Система управления взаимоотношениями с клиентами](https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0_%D1%83%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D0%B2%D0%B7%D0%B0%D0%B8%D0%BC%D0%BE%D0%BE%D1%82%D0%BD%D0%BE%D1%88%D0%B5%D0%BD%D0%B8%D1%8F%D0%BC%D0%B8_%D1%81_%D0%BA%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D0%B0%D0%BC%D0%B8)
[^810741]: [Что такое ERP?](https://habr.com/ru/articles/810741/)
[^erp]: [ERP](https://ru.wikipedia.org/wiki/ERP)
[^chto-znachit-on-premise]: [On-premise: что это, чем отличается от облака и кому подходит](https://www.fromtech.ru/blog/chto-znachit-on-premise)
[^on-premise-i-on-cloud-v-chem-raznitsa-i-chto-vybrat]: [On-premise и on-cloud: в чем разница и что выбрать](https://express.ms/blog/tekhnologii/on-premise-i-on-cloud-v-chem-raznitsa-i-chto-vybrat/)
[^datamart]: [Data Mart](https://yandex.cloud/ru/docs/glossary/datamart)
[^mashinnoe-obuchenie-i-big-data]: [Машинное обучение и большие данные](https://www.decosystems.ru/mashinnoe-obuchenie-i-big-data/)
[^OLAP]: [OLAP](https://ru.wikipedia.org/wiki/OLAP)
[^bigdata]: [Big Data](https://yandex.cloud/ru/docs/glossary/bigdata)
[^DAS]: [DAS](https://ru.wikipedia.org/wiki/DAS)
[^SAN]: [Сеть хранения данных](https://ru.wikipedia.org/wiki/%D0%A1%D0%B5%D1%82%D1%8C_%D1%85%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)
[^NAS]: [NAS](https://ru.wikipedia.org/wiki/NAS_(%D1%81%D0%B5%D1%80%D0%B2%D0%B5%D1%80))
[^datalake]: [Data Lake](https://yandex.cloud/ru/docs/glossary/datalake)
[^business-intelligence]: [Разработка и внедрение систем бизнес-аналитики BI](https://www.decosystems.ru/uslugi/business-intelligence/)
[^557416]: [Все что вы (не) хотели знать о Data Science](https://habr.com/ru/companies/citymobil/articles/557416/)
[^postroenie-hranilishh-dannyh]: [Создание корпоративных хранилищ данных (КХД)](https://www.decosystems.ru/uslugi/postroenie-hranilishh-dannyh/)
[^olap]: [OLAP](https://yandex.cloud/ru/docs/glossary/olap)
[^greenplum]: [СУБД Greenplum](https://www.decosystems.ru/uslugi/greenplum/)
[^oltp]: [OLTP](https://yandex.cloud/ru/docs/glossary/oltp)
[^iiot]: [Промышленный интернет вещей](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%BE%D0%BC%D1%8B%D1%88%D0%BB%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82_%D0%B2%D0%B5%D1%89%D0%B5%D0%B9)
[^iot-architecture-big-data]: [Как интернет вещей использует Big Data: архитектура IoT-систем](https://bigdataschool.ru/blog/iot-architecture-big-data)
[^internet-of-things]: [Internet of Things](https://bigdataschool.ru/wiki/internet-of-things/)
[^big_data_iot]: [Использование IIoT для увеличения эффективности](https://controlengrussia.com/internet-veshhej/big_data_iot/)
[^415933]: [Как построить IIoT архитектуру своими руками](https://habr.com/ru/companies/itsumma/articles/415933/)
[^420173]: [IoT архитектура — первый взгляд под капот](https://habr.com/ru/articles/420173/)
[^очередь-сообщений]: [Очередь сообщений](https://ru.wikipedia.org/wiki/%D0%9E%D1%87%D0%B5%D1%80%D0%B5%D0%B4%D1%8C_%D1%81%D0%BE%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%B8%D0%B9)
[^416629]: [RabbitMQ против Kafka: два разных подхода к обмену сообщениями](https://habr.com/ru/companies/itsumma/articles/416629/)
