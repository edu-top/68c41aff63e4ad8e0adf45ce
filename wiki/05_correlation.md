<link href="../styles.css" rel="stylesheet" />

## 05. Корреляционный анализ

- [05. Корреляционный анализ](#05-корреляционный-анализ)
  - [Цели и задачи](#цели-и-задачи)
    - [Цели модуля](#цели-модуля)
    - [Задачи модуля](#задачи-модуля)
    - [Результаты обучения](#результаты-обучения)
  - [Степень зависимости признаков](#степень-зависимости-признаков)
  - [Корреляция: определение, основные характеристики](#корреляция-определение-основные-характеристики)
    - [Что показывает корреляция](#что-показывает-корреляция)
    - [Характер связи между переменными](#характер-связи-между-переменными)
    - [Виды связи между переменными](#виды-связи-между-переменными)
    - [Графическое представление корреляции](#графическое-представление-корреляции)
    - [Направление и сила корреляции](#направление-и-сила-корреляции)
    - [Примеры корреляций](#примеры-корреляций)
    - [Ложная и частная корреляция](#ложная-и-частная-корреляция)
    - [Кто работает с понятием корреляции](#кто-работает-с-понятием-корреляции)
  - [Коэффициент корреляции](#коэффициент-корреляции)
    - [Виды коэффициентов корелляции](#виды-коэффициентов-корелляции)
    - [Выбор коэффициента корреляции](#выбор-коэффициента-корреляции)
  - [Проверка статистических гипотез](#проверка-статистических-гипотез)
    - [Уровень значимости и типы ошибок](#уровень-значимости-и-типы-ошибок)
    - [Тестирование статистической значимости](#тестирование-статистической-значимости)
    - [Статистические критерии](#статистические-критерии)
    - [Степени свободы](#степени-свободы)
    - [Алгоритм проверки статистических гипотез](#алгоритм-проверки-статистических-гипотез)
  - [Ковариация](#ковариация)
  - [Парные статистические связи](#парные-статистические-связи)
    - [Шкалы измерения](#шкалы-измерения)
    - [Коэффициент корреляции Пирсона (метрические шкалы)](#коэффициент-корреляции-пирсона-метрические-шкалы)
      - [Свойства коэффициента корреляции](#свойства-коэффициента-корреляции)
      - [Расчет коэффициента корреляции](#расчет-коэффициента-корреляции)
      - [Интерпретация результатов](#интерпретация-результатов)
      - [Оценка статистической значимости коэффициента корреляции](#оценка-статистической-значимости-коэффициента-корреляции)
      - [Варианты расчета](#варианты-расчета)
      - [Особенности коэффициента корреляции Пирсона](#особенности-коэффициента-корреляции-пирсона)
    - [Бисериальный коэффициент корреляции](#бисериальный-коэффициент-корреляции)
      - [Алгоритм расчета](#алгоритм-расчета)
  - [Практическая работа. Исследование меры линейной связи между количественными признаками для метрических шкал](#практическая-работа-исследование-меры-линейной-связи-между-количественными-признаками-для-метрических-шкал)
    - [Цель](#цель)
    - [Задачи](#задачи)
    - [Комментарий к заданию](#комментарий-к-заданию)
    - [Пример расчета](#пример-расчета)
  - [Практическая работа. Исследование парных связей между количественными и бинарными признаками](#практическая-работа-исследование-парных-связей-между-количественными-и-бинарными-признаками)
    - [Задачи](#задачи-1)
    - [Пример расчета](#пример-расчета-1)
    - [Комментарии к заданию](#комментарии-к-заданию)
  - [Заключение](#заключение)
  - [Источники информации](#источники-информации)

*[КК]: коэффициент корреляции

### Цели и задачи

#### Цели модуля

#### Задачи модуля

#### Результаты обучения
Научилиться оценивать зависимости между значениями признаков. Это делается с помощью коэффициента корреляции.

### Степень зависимости признаков
А теперь попробуем исследовать несколько разных связанных признаков.

*Пример зависимости между столбцами*

Р1 | Р2 | Р3 | Р4 | Р5
-- | -- | -- | -- | --
0 | 1 | 0 | 10 | 4
1 | 0 | 100 | 11 | 3
2 | 3 | 200 | 12 | 2
3 | 2 | 300 | 13 | 1

Выше приведена тренировочная таблица с несколькими признаками. Наша задача — установить, какие признаки наиболее сильно влияют на признак $P1$. Возьмем первую пару признаков $P1 - P2$ и подумаем о том, можно ли по значениям $P1$ вычислить значения для $P2$. На первый взгляд это не вполне очевидно, поэтому возьмем другую пару признаков ($P1 - P3$). Для данной пары признаков значение $P3$ легко установить, зная значения $P1$. После умножения значения $P1$ на 100 мы получим равенство значений во всех ячейках данной пары. Зависимость между этими двумя признаками формально можно выразить следующим образом:

$$ P3 = P1  \cdot 100 $$

Для пары $P1 - P4$ установить значение $P4$ можно без особого труда:

$$ P4 = P1 + 10 $$

Признак $P5$ тоже легко предсказать, располагая значением признака $P1$:

$$ P5 = 4 - P1 $$

Таким образом, можно сделать вывод о том, что в данной тренировочной таблице прослеживается сильная зависимость между признаком $P1$ и признаками $P3$, $P4$, $P5$. Эту зависимость можно выразить геометрически, для чего пары значений признаков можно представить в виде точек на координатной плоскости.

![Correlation coefficient](../img/cc-graph.png)

Если пары значений ложатся на одну прямую, то это свидетельствует о сильной зависимости между признаками. Если точки расположены хаотично и при любом способе проведения прямой остаются точки, отстоящие далеко от нее, то это свидетельствует об отсутствии очевидной зависимости между признаками.

> Тут может возникнуть следующий вопрос: а каким образом провести прямую через точки? Прямая должна быть такой, чтобы сумма квадратов отклонений точек через которые она "проводится" была бы минимальной. Тут главное не провести линию, а посмотреть как лежат сами точки. Проведите две параллельные прямые, которые расположены близко друг другу. Если все точки будут лежат внутри этих прямых, то значит признаки кореллируют.

!!! todo Задание

    Построить графики всех зависимостей из таблицы выше и сделать вывод о наличии зависимостей между признаками.

В данном случае можно заметить, что пары значений наших признаков ложатся на одну прямую, что говорит о большой зависимости между признаками.

Для оценки связи между двумя признаками нам потребуется характеристика, которая касается уже не одного признака, а пары признаков. Эта величина должна показывать, как значения одного признака определяют значения другого признака — <dfn title="степень зависимости признаков">степень зависимости признаков</dfn>. Данная характеристика должна иметь смысл и для признаков с разными единицами измерения (кг — м). В статистике для таких задач используют коэффициент корреляции (КК). Давайте разберемся, в чем ее смысл.

### Корреляция: определение, основные характеристики
Изучение связей между переменными, интересует исследователя с точки зрения отражения соответствующих **причинно-следственных отношений**.

<dfn title="корреляция">Корреляция</dfn> (от лат. correlatio «соотношение») — это взаимосвязь между разными показателями в статистике. Например, когда один показатель увеличивается, другой уменьшается — или тоже увеличивается. Это мера степени и направления связи между значениями двух переменных, а также статистический показатель вероятности связи между двумя переменными, измеренными в количественной шкале.

Корреляцию используют, чтобы оценить зависимость переменных друг от друга. Если два показателя коррелируют друг с другом, выше вероятность, что они как-то связаны: например, один зависит от другого или они оба зависят от третьей переменной.

<dfn title="корреляционная зависимость">Корреляционная зависимость</dfn> – это согласованные изменения двух (парная корреляционная связь) или большего количества признаков (множественная корреляционная связь). Суть ее заключается в том, что при изменении значения одной переменной происходит закономерное изменение (уменьшение или увеличение) другой(-их) переменной(-ых).

<dfn title="корреляционный анализ">Корреляционный анализ</dfn> – статистический метод, позволяющий с использованием коэффициентов корреляции определить, существует ли зависимость между переменными и насколько она сильна. Это проверка гипотез о связях между переменными с использованием коэффициентов корреляции.

<dfn title="коэффициент корреляции">Коэффициент корреляции</dfn> – двумерная описательная статистика, количественная мера взаимосвязи (совместной изменчивости) двух переменных.

#### Что показывает корреляция
С помощью корреляции определяют, как одна переменная меняется относительно другой — это определение из статистики. Это нужно, чтобы оценить, насколько показатели могут быть взаимосвязаны.

**Корреляция — это не зависимость**. Если две переменные коррелируют друг с другом — это еще не значит, что между ними есть причинно-следственная связь. Причины корреляции нужно исследовать отдельно — чтобы понять, как именно могут быть связаны показатели.

**Корреляция может быть случайной**. Иногда друг с другом коррелируют показатели, которые вообще не связаны и никак не зависят один от другого. Есть [целый сайт](https://tylervigen.com/spurious-correlations), где собраны абсурдные корреляции: например, чем меньше люди потребляют маргарина, тем меньше разводов в штате Мэн. Корреляция — больше 99%! Понятно, что связи тут, скорее всего, нет, просто совпадение. Такое явление называют spurious correlation, или **ложной корреляцией**.[^korrelyatsiya]

#### Характер связи между переменными
Для чего же тогда нужна корреляция? Несмотря на риск простого совпадения, чаще всего корреляция все же помогает найти неочевидные связи между переменными. Связи могут быть различными:

- прямая зависимость одного фактора от другого;
- непрямая зависимость, например, участвуют еще и вспомогательные факторы;
- зависимость обеих переменных от какой-то третьей;
- еще какая-то связь между переменными.

*Вот пример: продажи мороженого коррелируют с количеством лесных пожаров. Да, эти факторы не связаны напрямую, но есть третья переменная, которая влияет на оба: жаркая погода.*

Вывод не всегда такой очевидный, как в примере выше. Поэтому корреляцию не стоит использовать как окончательный результат исследования, но не нужно и недооценивать возможную связь.

Корреляция может быть:

- **положительной** — когда один показатель растет, другой тоже растет;
- **отрицательной** — когда одна переменная растет, другая уменьшается;
- **нейтральной** — изменения не связаны друг с другом.

![Характер связи между переменными](../img/correlation_01.png)

*Характер связи между переменными*

При **положительной линейной корреляции** более высоким значениям одного признака соответствуют более высокие значения другого, а более низким значениям одного признака – низкие значения другого.

При **отрицательной линейной корреляции** более высоким значениям одного признака соответствуют более низкие значения другого, а более низким значениям одного признака – высокие значения другого.

Корреляция может быть оценена различными методами, включая линейную корреляцию, которая предполагает существование линейной зависимости между переменными, а также непараметрическую корреляцию, которая не требует предположения о форме распределения данных. Для интерпретации корреляции важно учитывать контекст и особенности данных. Например, в анализе данных в науке и бизнесе корреляция может использоваться для прогнозирования и принятия решений.[^korrelyatsiya]

#### Виды связи между переменными
1. **Прямая причинно-следственная связь** — переменная $X$ определяет значение переменной $Y$.

    _Пример_: Наличие воды ускоряет рост растений. Яд вызывает смерть. Температура воздуха прямо влияет на скорость таяния льда.

2. **Обратная причинно-следственная связь** — переменная $Y$ определяет значение переменной $X$.

    _Пример_: Исследователь может думать, что чрезмерное потребление кофе вызывает нервозность. Но, может быть, очень нервный человек выпивает кофе, чтобы успокоить свои нервы?

3. **Связь, вызванная третьей (скрытой) переменной**.

    _Пример_: существует зависимость между числом утонувших людей и числом выпитых безалкогольных напитков в летнее время. Однако, обе переменные связаны с жарой и потребностью людей во влаге?

4. **Связь, вызванная несколькими скрытыми переменными**.

    _Пример_: Исследователь может обнаружить значимую связь между оценками студентов в университете и оценками в школе. Но действуют и другие переменные: IQ, количество часов занятий, влияние родителей, мотивация, возраст, авторитет преподавателей.

5. **Связи нет, наблюдаемая зависимость случайна**.

    _Пример_: Исследователь может найти связь между увеличением количества людей, которые занимаются спортом и увеличением количества людей, которые совершают преступления. Но здравый смысл говорит, что любая связь между этими двумя переменными является случайной.

![Виды связи между переменными](../img/correlation_02.png)

*Виды связи между переменными*

#### Графическое представление корреляции
Наглядное представление о связи двух переменных дает <dfn title="график рассеяния">график рассеяния</dfn> (scatter plot) или <dfn title="диаграмма рассеяния">диаграмма рассеяния</dfn> (scatter diagram), на котором каждый объект представляет собой точку, координаты которой заданы значениями двух переменных. Таким образом, множество объектов представляет собой на графике множество точек. По конфигурации этого множества точек можно судить о характере связи между двумя переменными. 

_Пример_: Рассматриваем две переменные: «Продолжительность подготовки (часов)»
студентов перед экзаменом и «Итоговая оценка» (из 100 балов). Пытаемся визуально
определить связь. Правда ли, что чем больше времени уделено подготовке, тем выше
оценка? (Ответ на этот вопрос будет дан далее при расчете коэффициента корреляции
Пирсона)

![График рассеяния](../img/correlation_03.png)

*График рассеяния*

Характеристики диаграммы:
- **наклон** (направление связи)
- **ширина** (сила, теснота связи)

![Диаграмма рассеяния](../img/correlation_05.png)

*Диаграмма рассеяния*

О силе связи можно судить по тому, насколько тесно расположены точки-объекты около
линии регрессии — чем ближе точки к линии, тем сильнее связь. <dfn title="линия регрессии">Линия регрессии</dfn> — это прямая линия на графике, которая наилучшим образом аппроксимирует (приближает) связь между двумя переменными в данных. В контексте простой линейной регрессии эта линия показывает зависимость одной переменной (зависимой) от другой (независимой) с помощью уравнения прямой, параметры которого выбираются так, чтобы минимизировать сумму квадратов отклонений всех точек данных от этой линии. Таким образом, линия регрессии служит моделью, описывающей и предсказывающей зависимость между переменными.

#### Направление и сила корреляции
_Пример_: На графике видно, что имеет место отрицательная линейная зависимость. Это означает, что увеличение переменной $X$ приводит к уменьшению переменной $Y$.

![Направление корреляции](../img/correlation_06.png)

*Направление корреляции*

**Сила связи** достигает максимума при условии взаимно однозначного соответствия: когда каждому значению одной переменной соответствует только одно значение другой переменной (и наоборот), эмпирическая взаимосвязь при этом совпадает с функциональной линейной связью. Сила связи не зависит от ее направленности и определяется по абсолютному значению коэффициента корреляции.

<dfn title="коэффициент корреляции">Коэффициент корреляции</dfn> ($r$) – количественная мера силы и направления вероятностной взаимосвязи двух переменных; величина данного показателя варьируется в пределах от –1 до +1. Если коэффициент корреляции равен 0, обе переменные линейно независимы друг от друга. По сути представляет собой меру прямой или обратной пропорциональности между двумя переменными. Это двумерная описательная
статистика, количественная мера взаимосвязи (совместимой изменчивости) двух переменных.

Значение (по модулю) | Интерпретация
-- | --
до 0,2 | очень слабая корреляция
до 0,5 | слабая корреляция
до 0,7 | средняя корреляция
до 0,9 | высокая корреляция
свыше 0,9 | очень высокая корреляция

![Сила корреляции](../img/correlation_04.png)

*Сила корреляции*

$r$ — это коэффициент корреляции. $P$ (или $p$-значение) в статистике — это уровень значимости, который показывает вероятность того, что наблюдаемые данные или более экстремальные результаты могли быть получены случайно, при условии, что нулевая гипотеза верна. Это мера, которая помогает оценить значимость статистической связи или эффекта. Чем меньше p-значение, тем выше статистическая значимость результатов, и тем более достоверно можно отвергнуть нулевую гипотезу. Обычно стандартным порогом значимости считается $p = 0,05$: если $p$ меньше этого значения, результат считается статистически значимым. Значение $p$ на обоих изображениях указывает на чрезвычайно высокую статистическую значимость корреляции, то есть вероятность случайного получения такого результата очень низка.

#### Примеры корреляций

![Примеры корреляций](../img/correlation_07.png)

*Примеры корреляций*

- а) строгая положительная корреляция
- б) положительная корреляция
- в) слабая положительная корреляция
- г) нулевая корреляция
- д) отрицательная корреляция
- е) строгая отрицательная корреляция
- ж) нелинейная корреляция
- з) нелинейная корреляция

![Пример корреляции](../img/correlation_08.png)

*Пример корреляции*

#### Ложная и частная корреляция
Если между двумя исследуемыми величинами установлена тесная зависимость, то из этого еще не следует их причинная взаимообусловленность. За счет эффектов одновременного влияния неучтенных факторов смысл истинной связи может искажаться. Поэтому такую корреляцию часто называют «ложной».

_Пример_: «Аисты приносят детей». Изучалась корреляция между числом аистов, свивших гнезда в южных районах Швеции, и рождаемостью в эти же годы в Швеции. Вычисления показали
высокую положительную корреляцию между этими явлениями. Однако причинная зависимость не может быть выведена ни из какого наблюдаемого совместного изменения явлений. Оказалось, что одновременные синхронные изменения числа аистов и детей объясняются изменением среднего уровня жизни жителей Стокгольма. При исключении этой искажающей переменной
прежней корреляции уже не наблюдалось.

Для выявления «ложной» корреляции используются **частные корреляции**.

Если две переменные коррелируют, всегда можно предположить, что эта корреляция обусловлена влиянием третьей переменной, как общей причины совместной изменчивости первых двух переменных. Для проверки этого предположения достаточно **исключить влияние этой третьей переменной** и вычислить корреляцию двух переменных без учета влияния третьей переменой (при фиксированных ее значениях). Корреляция, вычисленная таким образом называется <dfn title="частная корреляция">частной</dfn>.

#### Кто работает с понятием корреляции
**Специалисты по математической статистике**. Они могут использовать корреляцию в расчетах, нужных для научных работ, экспериментов, исследований. Это может быть фундаментальная наука или прикладная — есть и коммерческие исследования, где тоже заняты статисты.

**ML-инженеры**. Математика и статистика активно используются в [машинном обучении](https://blog.skillfactory.ru/glossary/mashinnoe-obuchenie/), например, при создании [нейронных сетей](https://blog.skillfactory.ru/glossary/nejronnaya-set/) и других обучающихся моделей. По корреляции инженер может отследить, что значат или не значат для модели те или иные данные на входе. Например, добавление во входные данные той или иной переменной коррелирует с ростом точности — это поможет лучше понять, что подавать модели на вход.

**Аналитики данных**. [Наука о данных](https://blog.skillfactory.ru/glossary/data-science/) активно использует статистику в бизнес-аналитике, дата-аналитике и других отраслях. Корреляция помогает аналитикам отобрать переменные для статистической модели, плюс в науке о данных есть отдельный метод — корреляционный анализ.

**Маркетологи**. Статистику вообще и корреляцию в частности используют при разработке [маркетинговых](https://blog.skillfactory.ru/glossary/internet-marketing/) стратегий. Если какой-то фактор коррелирует с повышением показателей, возможно, его стоит развивать.

**Журналисты**. Корреляцию используют для оценки разных событий в журналистике, особенно при расследованиях. Если корреляция между событием и каким-то фактором нашлась — это повод рассмотреть событие с другой точки зрения и, возможно, опубликовать новый материал с неочевидными выводами.

Во всех этих случаях важно избегать ложной причинно-следственной связи. Например, корреляция между посещением вебинаров и покупкой продукта не обязательно означает, что вебинары помогают покупать продукт. Может, обеими вещами просто интересуются одни и те же люди.[^korrelyatsiya]

### Коэффициент корреляции
Показателем того, как значения признаков ложатся на прямую, является коэффициент корреляции (КК). <dfn title="коэффициент корреляции">Коэффициент корреляции</dfn> — это статистический показатель, который количественно выражает силу и направление взаимосвязи между двумя переменными. Он принимает значения от -1 до +1: значение +1 означает идеальную положительную линейную зависимость, -1 — идеальную отрицательную, а 0 указывает на отсутствие линейной связи между переменными.

В анализе статистических данных коэффициент корреляции играет важную роль:

- помогает выявлять скрытые закономерности и связи между переменными;

- используется для проверки рабочих гипотез и формирования новых;

- позволяет определять необходимость включения факторов в аналитические модели и регрессионный анализ;

- применяется для количественной оценки эффективности процессов и принятия управленческих решений.

Важно отметить, что корреляция сама по себе не доказывает причинно-следственную связь между переменными, но помогает выдвигать гипотезы для дальнейшего изучения. Также к корреляционному анализу предъявляются требования, такие как достаточный объем наблюдений и качественная однородность данных, чтобы результаты были надежными.

Статистическая корреляция — это мощный инструмент анализа данных, который помогает выявлять связь между двумя или более переменными. Один из наиболее распространенных методов измерения корреляции — коэффициент корреляции, который может быть как положительным, так и отрицательным. Положительная корреляция указывает на то, что увеличение значений одной переменной обычно сопровождается увеличением значений другой, в то время как отрицательная корреляция указывает на обратную связь.

Таким образом, коэффициент корреляции является фундаментальным инструментом в статистическом анализе, который помогает понять и формализовать взаимосвязи между изучаемыми величинами, что в конечном итоге способствует более обоснованным выводам и решениям в исследовательской и практической деятельности.

Существует несколько типов коэффициентов корреляции, например, коэффициент Пирсона для линейных связей с нормально распределенными данными, Спирмена и Кендалла для ранговых данных, что позволяет использовать этот инструмент в разных областях науки, экономики, медицины и других сфер.

#### Виды коэффициентов корелляции
Коэффициенты корреляции — показатели, которые выражают силу корреляции между переменными. Существует несколько основных видов коэффициентов корреляции, которые применяются в зависимости от типа данных, распределения и целей анализа. Какой коэффициент использовать — зависит от ситуации, каждый из них лучше подходит для определенных случаев.

1. **Коэффициент корреляции Пирсона** ($r$)

    Этот коэффициент — самый популярный в статистике, описывается буквой $r$ и показывает прямолинейную связь между переменными. Он принимает значение от -1 до 1. Чем ближе значение к 1, тем выше положительная корреляция между показателями. Если оно, наоборот, ближе к -1 — корреляция отрицательная. А близкое к 0 значение, включая сам ноль, говорит, что корреляции нет.

   - Применяется для количественных (непрерывных) данных с нормальным распределением и при предположении линейной связи между переменными.

   - Характеризует степень линейной зависимости от -1 до +1.

   - Используется, например, при изучении связи роста и веса, температуры и производительности.

2. **Коэффициент корреляции Кендалла** ($\tau$)

    Этот коэффициент описывается буквой $\tau$ или $t$ и показывает корреляцию между факторами, которые можно ранжировать по какому-то признаку. Вместо значений показателя используют ранги — номера, присвоенные значениям при ранжировании. Проверить корреляцию Кендалла можно только для порядковых показателей — таких, которые можно упорядочить. Значение коэффициента — тоже от -1 до 1, и означают цифры то же, что и при корреляции Пирсона. Он тоже подходит только для оценки линейной связи.

    - Также ранговый коэффициент, применяемый для оценки согласованности порядковых данных.

    - Предпочтителен, если исходные данные содержат выбросы и маленькие выборки.

3. **Ранговая корреляция Спирмена** ($\rho$)

    Описывается буквой $\rho$ или $p$. Так же как и коэффициент Кендалла, этот предназначен для оценки ранжированных показателей — но больше подходит для малых выборок. Он использует непараметрические методы, которые могут обрабатывать данные низкого качества — с погрешностями, малым количеством информации и так далее. Принимает те же значения, что и коэффициент Пирсона, и означают они то же самое.

   - Используется для измерения силы монотонной зависимости между переменными, когда данные не обязательно имеют нормальное распределение.

   - Рассчитывается на рангах данных, подходит для порядковых и непараметрических данных.

   - Хорошо работает при наличии выбросов.

4. **Коэффициент корреляции Гудмена-Краскела** ($\gamma$)

    Показатель ассоциации между двумя порядковыми (категориальными упорядоченными) переменными. Он используется для оценки силы и направления связи в таблицах сопряженности с порядковыми данными. Значение коэффициента варьируется от -1 до 1, где 1 означает полную прямую ассоциацию, -1 — полную обратную, а 0 — отсутствие ассоциации. Этот коэффициент подходит для анализа ранжированных категорий и учитывает порядок категорий при оценке их взаимосвязи. Также коэффициент Гудмена-Краскела полезен при работе с номинальными данными с ранжированными уровнями и часто применяется в социальных и поведенческих науках для оценки согласованности или ассоциации между качественными переменными с порядком. Таким образом, он дополняет другие коэффициенты корреляции, предоставляя возможность анализа именно упорядоченных категориальных данных, где методы, такие как Пирсона или Спирмена, могут быть неприменимы или менее информативны.

5. **Коэффициент фи-корреляции для бинарных переменных** ($\phi$)

    Фи-коэффициент (φ) применяется для измерения тесноты связи между двумя категориальными переменными в таблицах сопряженности 2x2. Его значение варьируется от 0 до 1, где 0 означает отсутствие связи, а 1 — очень сильную связь. Этот коэффициент вычисляется на основе [критерии хи квадрат](https://blog.skillfactory.ru/glossary/kriteriy-hi-kvadrat/) статистики и размера выборки и используется для оценки силы ассоциации между переменными, представленными в виде двух бинарных признаков. Фи-коэффициент удобен для анализа небольших таблиц и является аналогом корреляционного коэффициента для категориальных данных. Он широко применяется для оценки связи, например, между полом и использованием интернета или другими бинарными характеристиками. При этом знак коэффициента указывает направление связи, а модуль — её силу.

7. **Коэффициент Крамера для номинальных переменных**

    Мера статистической ассоциации между двумя категориальными переменными в таблицах сопряженности произвольного размера. Он используется для оценки силы связи между качественными переменными и также основан на статистике хи-квадрат. Значение коэффициента Крамера варьируется от 0 до 1, где 0 означает отсутствие связи, а 1 — полную зависимость между переменными. Этот коэффициент особенно полезен, когда таблица сопряженности не является 2x2 (как в случае с фи-коэффициентом), и позволяет определить силу ассоциации без учета направления связи. Коэффициент Крамера широко применяется в статистике для анализа взаимосвязи категориальных данных в социологических, медицинских и маркетинговых исследованиях.

8. **Бисериальный коэффициент корреляции**

    Статистический показатель, который оценивает связь между дихотомической (двоичной) переменной и количественной переменной. Он является модификацией коэффициента корреляции Пирсона, адаптированной для случая, когда одна из переменных принимает два значения (например, "да" или "нет"). Значения бисериального коэффициента корреляции лежат в диапазоне от -1 до +1, однако в контексте интерпретации важна именно степень связи, а не знак. Этот коэффициент позволяет измерить силу и уровень связи, например, между прохождением теста (решил/не решил) и общим баллом по тесту. Проверка значимости бисериального коэффициента проводится с помощью t-критерия Стьюдента с учетом степеней свободы. Он широко используется в психологии, образовании и других областях для оценки эффективности бинарных факторов по отношению к количественным результатам.

9. **Рангово-бисериальный коэффициент корреляции**

    Используется для измерения связи между дихотомической переменной (принимающей только два значения, например, 0 и 1) и переменной, измеренной в ранговой шкале. Этот коэффициент вычисляется по формуле, в которой берутся средние ранги значений ранговой переменной, соответствующие каждой категории дихотомической переменной. Значение коэффициента варьируется от -1 до +1, однако его знак для интерпретации результатов обычно не имеет значения. Рангово-бисериальный коэффициент полезен, когда необходимо оценить, насколько распределение рангов отличается между двумя группами, определяемыми двоичной переменной. Проверка значимости этого коэффициента проводится аналогично точечному бисериальному коэффициенту с помощью t-критерия Стьюдента. Этот коэффициент часто применяется, когда одна из переменных является порядковой, а другая — дихотомической, например, для изучения связи между полом (мужчина/женщина) и порядковой оценкой какого-либо признака.

10. **Частные коэффициенты корреляции**

       - Используются для оценки связи между двумя переменными при исключении влияния третьей или нескольких переменных.

       - Помогают выявить "чистую" корреляцию, учитывая влияние других факторов.

11. **Множественный коэффициент корреляции**

       - Оценивает одновременную связь результативной переменной с несколькими факторными переменными.

       - Изменяется от 0 до 1, где 0 означает отсутствие связи, а 1 — полную линейную зависимость.

Коэффициенты существуют только для линейной корреляции, когда график одного показателя как бы «повторяет» другой. Еще есть нелинейная корреляция: одна переменная изменяется равномерно, а другая неравномерно, но взаимосвязь при этом есть. Для оценки нелинейной корреляции не пользуются коэффициентами, а используют более общий показатель — **корреляционное отношение**.[^korrelyatsiya]

#### Выбор коэффициента корреляции
Выбор вида коэффициента корреляции зависит от размеров выборки, типа шкалы переменных (интервальная, номинальная, порядковая), распределения данных и предполагаемой формы связи (линейная, монотонная). Например, для числовых непрерывных данных с нормальным распределением оптимален коэффициент Пирсона; для ранговых или не нормальных данных лучше использовать Спирмена или Кендалла. Частные и множественные коэффициенты помогают при многомерном анализе с влиянием нескольких факторов.

1. Для порядковых данных используются следующие коэффициенты корреляции:

   - $ρ$ — коэффициент ранговой корреляции Спирмена
   - $τ$ — коэффициент ранговой корреляции Кендалла
   - $γ$ - коэффициент ранговой корреляции Гудмена-Краскела

2. Для переменных с интервальной и номинальной шкалой используется **коэффициент
корреляции Пирсона** (корреляция моментов произведений).
1. Если, по меньшей мере, одна из двух переменных имеет порядковую шкалу, либо не является
нормально распределённой, используется ранговая корреляция **Спирмана** или **τ-Кендалла**. Применение коэффициента **Кендалла** предпочтительно, если в исходных данных имеются выбросы.

<table>
<tr>
    <th colspan=2>Типы шкал</th><th rowspan=2>Мера связи</th>
</tr>
<tr>
    <th>Переменная X</th><th>Переменная Y</th>
</tr>
<tr>
    <td>Интервальная или отношений</td><td>Интервальная или отношений</td><td>Коэффициент Пирсона</td>
</tr>
<tr>
    <td>Ранговая, интервальная или отношений</td><td>Ранговая, интервальная или отношений</td><td>Коэффициент Спирмена</td>
</tr>
<tr>
    <td>Ранговая</td><td>Ранговая</td><td>Коэффициент Кендалла</td>
</tr>
<tr>
    <td>Дихотомическая</td><td>Дихотомическая</td><td>Коэффициент φ</td>
</tr>
<tr>
    <td>Дихотомическая</td><td>Ранговая</td><td>Рангово-бисериальный коэффициент</td>
</tr>
<tr>
    <td>Дихотомическая</td><td>Интервальная или отношений</td><td>Бисериальный коэффициент</td>
</tr>
<tr>
    <td>Интервальная</td><td>Ранговая</td><td>Не разработан</td>
</tr>
</table>

Таким образом, разные виды коэффициентов корреляции позволяют гибко анализировать зависимость между переменными в различных статистических и практических задачах.

### Проверка статистических гипотез
Проверка статистических гипотез – это пятиступенчатая процедура, которая на основании данных выборки и при помощи теории вероятностей позволяет сделать вывод об обоснованности гипотезы. Другими словами, этот способ проверить, действительны ли результаты, полученные на выборке, и для генеральной совокупности.[^proverka-statisticheskix-gipotez]

<dfn title="статистическая гипотеза">Статистическая гипотеза</dfn> — это любое утверждение или предположение о распределении, параметрах или характеристиках изучаемой генеральной совокупности или случайной величины, которое можно проверить на основании выборочных данных. В статистическом анализе различают два основных вида гипотез:

- <dfn title="нулевая гипотеза">Нулевая гипотеза</dfn> (обозначается $H_0$) – утверждение о параметре генеральной совокупности (параметрах генеральных совокупностей) или распределении, которое необходимо проверить. Это предположение о том, что между изучаемыми параметрами нет значимых различий или связи, то есть наблюдаемые различия являются случайными. Проще говоря, нулевая гипотеза – это утверждение об отсутствии эффекта или различий (например, «новый метод лечения не эффективнее старого»).

    Нулевая гипотеза ($H_0$): постулирует отсутствие различий или эффекта. Это статус-кво, которое мы пытаемся опровергнуть. Пример: "Средний рост мужчин и женщин в данном городе одинаков." Например, $H_0 : μ_1 = μ_2$ говорит, что средние значения двух групп равны.

- <dfn title="альтернативная гипотеза">Альтернативная гипотеза</dfn> ($H_A$ или $H_1$) – утверждение, противоположное нулевой гипотезе. Выдвигается, но не проверяется. Это противоположное утверждение о том, что различия или связь существуют, и они статистически значимы. Проще говоря, альтернативная гипотеза – это утверждение, которое исследователь хочет доказать (например, «новый метод лечения эффективнее»).

    Альтернативная гипотеза ($H_1$): опровергает нулевую гипотезу и отражает интерес исследователя. Пример: "Средний рост мужчин и женщин в данном городе не одинаков."  Примеры: $H_1 : μ_1 ≠ μ_2$, $H_1 : μ_1 > μ_2$ или $H_1 : μ_1 < μ_2$.

_Пример_: в эксперименте с двумя группами спортсменов нулевая гипотеза может утверждать, что средние результаты упражнений в обеих группах одинаковы. Альтернативная гипотеза будет утверждать, что результаты в группах различаются или что одна группа показывает лучшие показатели.

Все гипотезы можно разделить на двусторонние (ненаправленные) и односторонние (направленные).

**Двусторонние альтернативы**
- $(H_A : p \neq 0.5)$

**Односторонние альтернативы**
- левосторонние $(H_A : p < 0.5)$
- правосторонние $(H_A : p > 0.5)$

<dfn title="p-значение">$p$-значение</dfn> — это вероятность получить наблюдаемые результаты или более экстремальные значения статистической величины при условии, что нулевая гипотеза верна. Проще говоря, $p$-значение показывает, насколько вероятно, что разница или эффект, выявленные в данных, возникли случайно, если в действительности никакой разницы нет.

В контексте двусторонней альтернативы $H_A : p ≠ 0.5$ $p$-значение отражает вероятность получить результаты, отклоняющиеся от 0.5 в любую сторону настолько же сильно или сильнее, чем наблюдаемое значение. Если $p$-значение меньше выбранного уровня значимости (например, 0.05), то нулевую гипотезу отвергают, и делают вывод о статистически значимом отклонении от 0.5. $p$-значение при односторонних альтернативных гипотезах рассчитывается как вероятность получить значение тестовой статистики, равное наблюдаемому или более экстремальное в одном конкретном направлении (левом или правом), при условии, что нулевая гипотеза верна.

Важно помнить, что $p$-значение не говорит о вероятности истинности гипотезы, а лишь оценивает вероятность данных при условии, что нулевая гипотеза верна. Низкое $p$-значение указывает на маловероятность наблюдать такие данные при $H_0$, что служит основанием для ее отклонения.

Таким образом, p-значение является ключевым параметром в статистическом тестировании для оценки значимости результатов исследования, особенно при двусторонних проверках, когда отклонения в обе стороны представляют интерес.

Проверка гипотезы заключается в анализе выборочных данных для принятия или отклонения нулевой гипотезы с заданным уровнем значимости. Если вероятность получить наблюдаемые данные при условии справедливости $H_0$ ($p$-значение) ниже заранее установленного уровня значимости ($α$, часто 0,05), то нулевая гипотеза отвергается в пользу альтернативной. В противном случае $H_0$ принимается или не отвергается.

Таким образом, гипотезы служат основанием для статистической проверки и принятия решений, помогают формализовать предположения о свойствах и взаимосвязях данных и определить степень достоверности сделанных выводов.

#### Уровень значимости и типы ошибок
<dfn title="уровень значимости">Уровень значимости</dfn> (обозначается как $α$) – вероятность отвергнуть верную нулевую гипотезу. Нулевая гипотеза всегда проверяется на определенном уровне значимости. Например, если мы проверяем нулевую гипотезу на уровне значимости 5%, это означает, что если мы будем проводить аналогичные исследования 100 раз и проверять на основе имеющихся данных интересующую нас нулевую гипотезу, в 5 случаях из 100 мы отвергнем нулевую гипотезу, хотя она будет верной.

Уровень значимости в каком-то смысле является понятием, противоположным уровню доверия. <dfn title="уровень доверия">Уровень доверия</dfn> (<dfn title="доверительная вероятность">доверительная вероятность</dfn>) – вероятность не отвергнуть верную нулевую гипотезу. Имеет место следующее соотношение:

$$ α = 1 − γ $$

Проверить нулевую гипотезу на уровне значимости 5% и проверить нулевую гипотезу на
уровне доверия 95% – это одно и то же.

**Типы ошибок**
- <dfn title="ошибка I рода">Ошибка I рода</dfn> – вероятность *отвергнуть верную* нулевую гипотезу ($α$).
- <dfn title="ошибка II рода">Ошибка II рода</dfn> – вероятность не отвергнуть неверную нулевую гипотезу ($β$). Другими словами, это вероятность того, что *ложная* нулевая гипотеза будет *принята*.

<dfn title="мощность критерия">Мощность критерия</dfn> – вероятность отвергнуть неверную нулевую гипотезу ($1 − β$).

| | $H_0$ верна | $H_0$ неверна |
-- | -- | --
$H_0$ отвергается | ошибка I рода | $+$
$H_0$ не отвергается | $+$ | ошибка II рода

![Типы ошибок](../img/отвергнута.png)

**Аналогия**: суд над маньяком $H_0$: заключенный не виновен.

![Суд над маньяком](../img/отвергнута1.png)

**Аналогия**: шорох в кустах – это лев? $H_0$: льва в кустах нет.

![Лев в кустах](../img/отвергнута2.png)

Уровень значимости – это максимально допустимая вероятность допустить ошибку первого рода, т.е. отвергнуть верную нулевую гипотезу (истинную $H_0$). Его выбирают перед началом анализа, обычно это 5% (0.05), 1% (0.01) или 0.1% (0.001). Если $p$-значение из эксперимента меньше уровня значимости, то нулевую гипотезу отвергают в пользу альтернативной. Так, $\alpha =0.05$ означает, что мы готовы допустить 5% вероятность ошибки, когда на самом деле нулевая гипотеза верна. Более низкий уровень значимости (например, 0.01) снижает риск ошибки первого рода, но требует более веских доказательств для отклонения $H_0$.

Уровни значимости, принятые в маркетинговых исследованиях:

- $α$ – уровень значимости

  - 0,01 (1%)

  - **0,05 (5%)**

- $(1-α)$ – уровень доверия (доверительная вероятность)

  - 0,99 (99%)

  - **0,95 (95%)**

Таким образом, <dfn title="уровень значимости">уровень значимости</dfn> — это заранее установленный порог в статистическом тестировании, который определяет вероятность ошибки первого рода, то есть вероятность отвергнуть нулевую гипотезу, когда она на самом деле верна. Обычно уровни значимости выбираются как 0,05 (5%), 0,01 (1%) или 0,001 (0,1%). Если рассчитанное $p$-значение у теста оказывается меньше уровня значимости, нулевая гипотеза отвергается, и результат считается статистически значимым, то есть с высокой степенью уверенности можно говорить, что наблюдаемое различие или эффект не случайны. Выбор уровня значимости — это компромисс между риском ошибочного отклонения истинной гипотезы и риском не обнаружить реальный эффект. Таким образом, уровень значимости помогает исследователям решать, насколько убедительны полученные результаты и когда можно считать их достоверными.

#### Тестирование статистической значимости
<dfn title="статистическая значимость">Статистическая значимость</dfn> (sig.; от англ. statistical signification – статистическая значимость) — мера случайности полученного результата, равная вероятности того, что в генеральной совокупности этот результат (различия, связь) отсутствует. Чем меньше эта вероятность (значение $р$-уровня), тем выше статистическая значимость результата. Результат считается статистически достоверным (значимым), если $р$-уровень не превышает 0,05.

Односторонний и двусторонний тесты значимости отличаются тем, в каком направлении они проверяют гипотезу о наличии эффекта.

<dfn title="односторонний тест значимости">Односторонний тест</dfn> используется, когда исследователь заранее интересуется только одним направлением эффекта — например, только увеличением или только уменьшением показателя. В этом случае нулевая гипотеза формулируется так, что эффект отсутствует или направлен противоположно. Альтернативная гипотеза проверяет, что эффект существует в одном конкретном направлении. Односторонний тест учитывает критическую область на одной стороне распределения статистики, что повышает чувствительность к обнаружению заданного эффекта в этом направлении.

<dfn title="двусторонний тест значимости">Двусторонний тест</dfn> проверяет значимость отклонений в обоих направлениях, то есть учитывает возможность как увеличения, так и уменьшения показателя по сравнению с нулевой гипотезой. В этом случае нулевая гипотеза предполагает отсутствие эффекта, а альтернативная — что эффект существует, но направление заранее неизвестно. Критические области располагаются на обеих сторонах распределения, поэтому такой тест более консервативен и требует более сильных доказательств для отклонения нулевой гипотезы.

Выбор между односторонним и двусторонним тестом зависит от исследовательской задачи и формулировки гипотез. Если заранее известно направление эффекта (например, тестирование нового препарата на улучшение состояния), односторонний тест более уместен. Если же интересует любой вид отличия (улучшение или ухудшение), применяют двусторонний тест для более объективной проверки.

В итоге:

- односторонний тест — чувствителен к эффекту в одном направлении, критическая область на одной стороне;

- двусторонний тест — проверяет оба направления, критическая область по обеим сторонам, более строгий критерий.

#### Статистические критерии
<dfn title="статистический критерий">Статистический критерий</dfn> (тест) — это математическое правило, по которому на основании результатов выборки принимается или отвергается одна из статистических гипотез (обычно нулевая $H_0$ или альтернативная $H_1$) с заданным уровнем значимости. Критерий представляет собой функцию, которая сопоставляет наблюдаемые данные с гипотезами и помогает определить, насколько данные согласуются с нулевой гипотезой или свидетельствуют о её отклонении.

Статистические критерии — это правила для проверки гипотез на основе данных, которые делятся на параметрические (предполагают знание распределения данных) и непараметрические (не требуют этого). Они используются для сравнения выборок, проверки достоверности различий между средними или другими показателями, а также для выявления связей между признаками. К основным относятся критерии согласия (Пирсона, Колмогорова) и критерии для проверки гипотез (например, Z-тест).

**Типы статистических критериев**
1. <dfn title="параметрические критерии">Параметрические критерии</dfn> — класс статистических методов, используемых для анализа данных, которые образуют известное распределение (обычно нормальное). Названы так потому, что основываются на оценке параметров (таких как среднее или стандартное отклонение) выборочного распределения интересующей величины.

   - Используют предположения о том, что данные соответствуют определенному распределению (например, нормальному).
   - Более чувствительны к различиям, если предположения верны.
   - Примеры: t-критерий Стьюдента, критерий Фишера.

2. <dfn title="непараметрические критерии">Непараметрические критерии</dfn> — непараметрические методы не основываются на оценке параметров (таких как среднее или стандартное отклонение) при описании выборочного распределения интересующей величины. Поэтому эти методы иногда также называются свободными от параметров или свободно распределенными.

     - Не требуют предположений о распределении данных.
     - Хорошо подходят для небольших выборок или данных, не соответствующих нормальному распределению.
     - Примеры: критерий Манна-Уитни, критерий Хи-квадрат.

**Основные области применения**
- **Проверка гипотез**: определение, являются ли наблюдаемые различия статистически значимыми или случайными.
- **Сравнение выборок**: оценка достоверности различий между средними, дисперсиями и другими характеристиками выборок.
- **Выявление связей**: проверка наличия и силы связи между различными признаками.
- **Оценка соответствия**: определение, насколько хорошо эмпирические данные соответствуют теоретическому распределению (критерии согласия).

**Примеры конкретных критериев**
- **Критерий согласия**:
  - **Критерий Пирсона** ($x²$): используется для проверки соответствия наблюдаемого распределения частот теоретическому.
  - **Критерий Колмогорова-Смирнова**: сравнивает эмпирическое распределение выборки с теоретическим.
  - **Критерий Шапиро-Уилка**: один из наиболее мощных тестов на нормальность.
- **Критерии для сравнения**:
  - **Z-тест**: сравнение средних двух выборок при известной дисперсии.
  - **t-критерий Стьюдента**: сравнение средних двух выборок при неизвестной дисперсии.
  - **критерий Манна-Уитни**: непараметрический аналог t-критерия для сравнения двух независимых выборок.

Выбор статистического критерия зависит от нескольких факторов: типа данных (количественные или качественные), распределения данных (нормальное или нет), количества сравниваемых групп (две или более) и их связанности (независимые или зависимые/связанные). Важно учитывать тип данных, число и связь выборок, а также размер выборки, чтобы правильно подобрать подходящий критерий для вашего исследования, например, t-критерий Стьюдента или ANOVA.

**Алгоритм выбора статистического критерия**[^vybor-statisticheskogo-kriteriya]

**Ключевые шаги для выбора критерия**
1. **Определить тип данных** — количественные или качественные:
   - **Количественные**: данные, которые можно измерить (рост, вес, возраст).
   - **Качественные** (категориальные): данные, которые представляют категории (например, пол, цвет глаз).

2. **Определить тип распределения** — в случае количественных данных необходимо уточнить тип распределения (нормальное или отличное от нормального):
   - **Нормальное распределение**: Если данные распределены симметрично и напоминают колокол, используются параметрические критерии (например, t-критерий Стьюдента).
   - **Отличное от нормального**: Если распределение не соответствует нормальному, следует использовать непараметрические критерии (например, U-критерий Манна-Уитни).

3. **Определить количество и связанность групп** — в случае выбора статистического критерия для сравнения количественных данных нужно учитывать распределение признака: является ли оно нормальным или отличным от нормального. В первом случае, как и при описании вариационного ряда, необходимо использовать параметры распределения (отсюда и название этой группы методов: параметрические методы), в случае отличного от нормального распределения следует использовать непараметрические методы:
   - **Две независимые группы**: Сравниваются две разные группы, например, две группы студентов, одна из которых получила тренинг, а другая нет. В этом случае используют парный t-критерий Стьюдента для несвязанных выборок или U-критерий Манна-Уитни.
   - **Две зависимые (связанные) группы**: Одна и та же группа испытуемых сравнивается в двух разных условиях, например, результаты измерений до и после эксперимента. Для этого используют парный t-критерий Стьюдента или критерий Уилкоксона.
   - **Три и более группы**:
     - **Независимые**: Однофакторный дисперсионный анализ (ANOVA) для трех и более независимых групп.
     - **Зависимые**: ANOVA с повторными измерениями.

4. **Оценить размер выборки**:
   - **Небольшие выборки** (менее 30-50 элементов) часто требуют использования непараметрических критериев из-за недостаточной статистики для нормального распределения.

![Выбор статистического параметра](../img/vibor_stat_kriteria-1024x774.png)

*Рисунок 1. Выбор статистического критерия для сравнения статистических совокупностей (По Мильчаков К.С.)*

Связь статистического критерия с p-значением такова:

- Статистический критерий формирует статистику теста — числовую характеристику, рассчитанную из данных, которая служит для проверки гипотезы.

- На основании этой статистики определяется $p$-значение — вероятность получить наблюдаемое (или более экстремальное) значение статистики при условии, что нулевая гипотеза верна.

- Если $p$-значение меньше заранее выбранного уровня значимости $α$, статистический критерий указывает на отклонение нулевой гипотезы.

- Таким образом, 4p$-значение является инструментом, который помогает критерию принять решение о статистической значимости результатов, а критерий задаёт сам процесс проверки гипотезы.

Иными словами, статистический критерий — это правило и метод тестирования, а $p$-значение — числовой показатель, который используется в рамках этого теста для принятия решения.

#### Степени свободы
Статистический анализ играет важную роль в научных исследованиях, коммерческих деятельностях и в других областях. Однако, его результаты могут быть неточными, если не учитывать имеющиеся степени свободы. Степени свободы – это концепция, которая широко используется в статистике, и она позволяет более точно определить, насколько можно доверять полученным результатам.

<dfn title="степень свободы">Степень свободы</dfn> (Degree of Freedom, $df$) в статистике — это количество значений или наблюдений в выборке, которые могут быть изменены независимо друг от друга без изменения ее структуры. Можно сказать, что это количество переменных, которые оставляются свободными для варьирования после того, как структура выборки была определена.

Чем больше степень свободы, тем меньше вероятность ложных выводов и тем более точными будут результаты теста. В случае же, если степень свободы будет низкой, то мы можем получить ложные результаты, так как мы не имеем достаточно информации для адекватной оценки статистических характеристик выборки.

Одним из важных факторов, влияющих на степень свободы, является размер выборки. Чем больше выборка, тем больше степень свободы, значит, чем больше выборка, тем менее вероятно получение ошибочных результатов в статистических тестах.

Также степень свободы важна при выборе статистической модели. К примеру, при построении линейной регрессии, степень свободы может использоваться для определения того, сколько переменных необходимо использовать в модели. Выбор модели слишком сложной или, наоборот, слишком простой (т.е. с недостаточной степенью свободы) может привести к неправильным выводам.[^735800]

<dfn title="таблица степеней свободы">Таблица степеней свободы</dfn> – это таблица, которая заполняется в соответствии с типом и количеством переменных, которые используются в анализе статистических данных. Она используется для определения правильной формулы для расчета критических значений при проведении статистических тестов, таких как t-критерий, F-критерий и хи-квадрат тест.

В таблице степеней свободы могут быть два типа переменных: независимые (IV) и зависимые (DV). Количество степеней свободы для каждой переменной определяется путем вычитания единицы от общего количества наблюдений.

Для каждого теста, количество степеней свободы может быть разным в зависимости от характеристик выборки и типа теста. Например:

- В t-критерии Стьюдента, количество степеней свободы зависит от размера выборки и количества групп, участвующих в сравнении. Если у нас есть две группы, количество степеней свободы будет равно $n_1+n_2-2$ (где $n_1$ и $n_2$ – это размер первой и второй групп соответственно).

    ![таблица степеней свободы](../img/661dc4f7d6b03c5aa960706e6bfe0d56.png)

  - [Онлайн-таблица распределения Стьюдента](https://www.kontrolnaya-rabota.ru/s/teoriya-veroyatnosti/tablica-studenta/)

- В анализе дисперсии (ANOVA), количество степеней свободы будет зависеть от количества групп и количества элементов в каждой группе. Если есть количество групп ($k$) и общее количество элементов ($N$), то количество степеней свободы для межгрупповой дисперсии будет равно $k-1$, а для остаточной дисперсии будет равно $N-k$.

- В хи-квадрат тесте, количество степеней свободы зависит от размера матрицы сопряженности. Если у нас есть матрица 2x2, то количество степеней свободы будет равно 1.

Таблица степеней свободы помогает убедиться, что мы используем правильные статистические формулы для расчетов, что позволяет получать более точные и надежные результаты при анализе статистических данных.

#### Алгоритм проверки статистических гипотез
**Как это работает на практике**

1. **Формулирование гипотез**: четко определить нулевую ($H_0$) и альтернативную ($H_1$) гипотезы до начала исследования. Нулевая гипотеза  — предположение отсутствия эффекта или различий. Альтернативная гипотеза — предположение наличия эффекта или различий.
2. **Выбор уровня значимости**: определить $\alpha$ — порог для принятия решения об отвергании $H_0$ (например, 0.05). Выбор зависит от того, насколько критичны ошибки первого рода для проводимого исследования.
3. **Выбор статистического критерия**: определить подходящий тест, который лучше соответствует данным и типу гипотезы (например, t-тест, хи-квадрат, F-тест и др.).
4. **Формулирование правила принятия решения**: определить критическую область или пороговое значение, сравнить реальное значение теста со статистикой критерия.
5. **Проведение эксперимента и анализ данных**: собрать данные и провести статистический тест. Получить $p$-значение – вероятность получить наблюдаемые данные (или более экстремальные) при условии, что нулевая гипотеза верна.
6. **Принятие решения**:
   - Если $p < \alpha$, то мы отвергаем нулевую гипотезу ($H_0$). Результаты считаются статистически значимыми, и мы принимаем альтернативную гипотезу $H_1$.
   - Если $p \ge \alpha$, то у нас недостаточно оснований, чтобы отвергнуть нулевую гипотезу $H_0$.

### Ковариация
Ковариация — это статистическая мера совместной изменчивости двух случайных величин. Она отражает, как две переменные изменяются относительно своих средних значений вместе. Если при больших (или малых) значениях одной переменной обычно наблюдаются большие (или малые) значения другой, ковариация положительна. Если же большие значения одной переменной сопровождаются малыми значениями другой, ковариация отрицательна.

Геометрически ковариация отражает направление линейной зависимости: положительная ковариация говорит о тенденции переменных изменяться в одном направлении, отрицательная — в противоположных. Однако значение ковариации зависит от размерностей переменных и обычно сложно интерпретировать напрямую. Для измерения силы и направления линейной связи используют нормированную версию — коэффициент корреляции.

В выборке ковариация вычисляется как среднее произведение отклонений каждого значения переменной от её среднего:

$$ S_{xy} = \frac{1}{n-1} \sum_{i=1}^n{(x_i - \bar{x})(y_i - \bar{y})} $$

где $\bar{x}$ и $\bar{y}$ — средние значения выборок $X$ и $Y$ соответственно.

При расчёте выборочной ковариации чаще используется формула с делением на $n−1$, а не на $n$, чтобы получить несмещённую оценку ковариации популяции. Это связано с тем, что средние значения $\bar{X}$ и $\bar{Y}$  вычисляются по той же выборке, что и ковариация, и «теряется» одна степень свободы. Этот вариант используется в статистике для оценки ковариации генеральной совокупности на основании выборочных данных, чтобы получить несмещённую оценку.

Таким образом, ковариация показывает, насколько два признака связаны и в каком направлении эта связь происходит, но для оценки степени связи и ее сравнения используется коэффициент корреляции.

### Парные статистические связи
<dfn title="парные статистические связи">Парные статистические связи</dfn> — это взаимосвязи между двумя признаками (переменными), чаще всего именуемыми **факторным** (независимым) и **результативным** (зависимым). Такие связи показывают, как изменение одного признака связано с изменением другого. В статистике парная связь может иметь характеристики силы, направления и формы. Сила связи отражает степень согласованности изменений двух переменных, направление показывает, увеличивается ли результативный признак при увеличении факторного (прямая связь) или уменьшается (обратная связь), а форма описывает тип зависимости — линейная или криволинейная.

Парные связи бывают вероятностными, то есть не абсолютно фиксированными, а подверженными влиянию других факторов и случайных ошибок измерений. Это приводит к тому, что одному значению одной переменной обычно соответствует множество значений другой. Для количественной оценки тесноты парной связи используется коэффициент корреляции, который может принимать значения от -1 до 1, где 1 означает идеальную прямую связь, -1 — идеальную обратную, а 0 — отсутствие линейной связи.

Важным инструментом анализа парных связей является метод наименьших квадратов, применяемый для определения параметров функции связи (чаще всего линейной), которая описывает зависимость результативного признака от факторного.

Парная статистическая связь изучает взаимосвязь между двумя переменными, описывая характер, силу и направление их зависимости. Коэффициент корреляции — это количественная мера этой связи, которая показывает, насколько тесно связаны две переменные и в каком направлении. Иными словами, парная статистическая связь — это сам факт наличия взаимосвязи между двумя переменными, а коэффициент корреляции — инструмент для измерения и описания этой связи с помощью числового показателя.

#### Шкалы измерения
В контексте парных статистических связей и коэффициентов корреляции шкалы измерения данных делятся на метрические и неметрические. Метрические шкалы (интервальная и шкала отношений) позволяют измерять разницу между объектами, тогда как неметрические (номинальная и порядковая) — лишь классифицируют или упорядочивают их. Метрические шкалы предполагают наличие единицы измерения, а неметрические – нет.

**Метрические шкалы** включают интервальную и шкалу отношений. Эти шкалы характеризуются наличием количественных значений с равными интервалами (для интервальной шкалы) и абсолютным нулём (для шкалы отношений), что позволяет применять арифметические операции и рассчитывать коэффициенты корреляции Пирсона.
- **Интервальная**: отражает не только различие, но и разницу между объектами в определенных единицах. Однако у нее нет истинной нулевой точки. Пример: температура по Цельсию, где $0^\circ$ — это точка замерзания воды, а не полное отсутствие тепла.
- **Шкала отношений**: похожа на интервальную, но имеет истинную нулевую точку и позволяет говорить о соотношении величин. Пример: рост человека (нулевой рост означает его отсутствие) или масса тела. 

**Неметрические шкалы** включают номинальную и порядковую (ранговую) шкалы. Они не обладают свойствами равных интервалов или абсолютного нуля, поэтому для данных, измеренных по таким шкалам, применяют непараметрические методы анализа. В частности, для оценки корреляционной связи между переменными на неметрических шкалах используется коэффициент ранговой корреляции Спирмена, который определяет силу и направление монотонной зависимости между двумя ранжированными переменными. Этот метод не требует нормального распределения данных и может применяться к порядковым шкалам.
- **Номинальная** (**шкала наименований**): группирует объекты по категориям, не предполагая никакой упорядоченности. Пример: виды транспорта, национальность, пол.- **Порядковая**: классифицирует объекты в соответствии с принципом "больше – меньше", но не указывает, насколько велика разница между ними. Пример: результаты опроса "не согласен", "нейтрально", "согласен" или уровень образования.

Таким образом, выбор коэффициента корреляции зависит от типа шкал измерения данных: метрические шкалы — корреляция Пирсона, неметрические шкалы — ранговая корреляция Спирмена. Это важно учитывать при анализе парных статистических связей для корректной интерпретации результатов и выбора подходящих статистических методов.

#### Коэффициент корреляции Пирсона (метрические шкалы)
Коэффициент корреляции $r$-Пирсона является мерой прямолинейной связи между
переменными: его значения достигают максимума, когда точки на графике
двумерного рассеяния лежат на одной прямой линии.

$$ r = \frac{\sum{Z_{X_i} Z_{Y_i}}}{n-1} $$

$$ Z_{X_i} = \frac{X_i - \bar{X}}{S_X} $$

$$ Z_{Y_i} = \frac{Y_i - \bar{Y}}{S_Y} $$

Учитывая что:

$$ S_{xy} = \frac{1}{n-1} \sum_{i=1}^n{(x_i - \bar{x})(y_i - \bar{y})}, S_x^2 = \frac{1}{n-1} \sum_{i=1}^n{(x_i - \bar{x})^2}, S_y^2 = \frac{1}{n-1} \sum_{i=1}^n{(y_i - \bar{y})^2} $$

коэффициент корреляции Пирсона:

$$ r = \frac{S_{xy}}{S_x S_y} $$

Пусть $P = (p_1, p_2, \dots, p_n), Q = (q_1, q_2, \dots, q_n)$ — признаки (столбцы из таблицы). Тогда КК считается по формуле:

$$ r(P, Q) = \frac{\sum_{i=1}^n{p_i q_i} - n \bar{p} \bar{q}}{(n-1) S_p S_q} $$

> Здесь при раскрытии $S_p$ и $S_q$ и умножения на $(n-1)$, $(n-1)$ сокращается, получается привычная формула КК Пирсона.
>
> ![Pearson formula](../img/pearson_formula.png)

##### Свойства коэффициента корреляции
Коэффициент корреляции (КК) – это число из отрезка $[–1,1]$, которое имеет следующий смысл:
1. Если КК=0 (или близок к нему), то очевидной зависимости между признаками $P,Q$ нет.
2. Если КК>0, то б&oacute;льшим значениям признака $P$, как правило, соответствуют б&oacute;льшие значения признака $Q$. Это свидетельствует о наличии прямой зависимости между признаками.
3. Если КК<0, то б&oacute;льшим значениям признака $P$, как правило, соответствуют меньшие значения признака $Q$. Это свидетельствует о наличии обратной зависимости между признаками.
4. Чем ближе значение КК к единице, тем сильнее зависимость между признаками $P,Q$.
5. Если модуль КК равен 1, то между признаками $P,Q$ существует линейная зависимость.

##### Расчет коэффициента корреляции
Например, исследуем взаимосвязь веса и роста студентов.

| Студент | Рост $X$ | Вес $Y$
-- | -- | --
Дима | 160 | 72
Гриша |144 | 66
Миша | 154 | 68
Коля | 210 | 74
Федя | 182 | 68
Рома | 159 | 64
**среднее** | **168.17** | **68.67**

1. **Объем выборки**: $n = 6$.

2. **Выборочные средние**:

    $$
    \bar{x} = \frac{\sum_{i=1}^n{x_i}}{n} \approx 168.17\\[2ex]
    \bar{y} = \frac{\sum_{i=1}^n{y_i}}{n} \approx 68.67
    $$

3. **Выборочные дисперсии**:

$$
S_x^2 = \frac{1}{n-1} \sum_{i=1}^n{(x_i - \bar{x})^2} \approx 575.37\\[2ex]
S_y^2 = \frac{1}{n-1} \sum_{i=1}^n{(y_i - \bar{y})^2} \approx 13.87
$$

4. **Выборочные стандартные отклонения**:

$$
S_x = \sqrt{S_x^2} \approx 23.99\\[2ex]
S_y = \sqrt{S_y^2} \approx 3.72\\[2ex]
$$

5. **Ковариация**:

$$ S_{xy} = \frac{1}{n-1} \sum_{i=1}^n{(x_i - \bar{x})(y_i - \bar{y})} \approx 60.68$$

5. **Коэффициент корреляции Пирсона**:

$$ r = \frac{S_{xy}}{S_x S_y} = \frac{60.68}{23.99 \cdot 3.72} \approx 0.68 $$

##### Интерпретация результатов

![Pearson](../img/pearson.png)

Диапазон | Уровень связи
-- | --
[0,75; 1,00] | очень высокая положительная
[0,50; 0,74] | высокая положительная
[0,25; 0,49] | средняя положительная
[0,00; 0,24] | слабая положительная
[0,00; -0,24] | слабая отрицательная
[-0,25; -0,49] | средняя отрицательная
[-0,50; -0,74] | высокая отрицательная
[-0,75; -1,00] | очень высокая отрицательная

Таким образом, значения коэффициента корреляции Пирсона $r≈0.68$ говорит о наличии умеренно-высокой положительной линейной зависимости между весом и ростом студентов в данной выборке. То есть с увеличением роста, в целом, вес также растет, но связь не является идеально линейной.

##### Оценка статистической значимости коэффициента корреляции
Проверка статистической гипотезы о существовании линейной связи между весом и ростом студентов по алгоритму:

1. **Формулирование гипотез**:

   - $H_0$: Коэффициент корреляции Пирсона равен нулю (нет линейной связи между весом и ростом).

   - $H_1$: Коэффициент корреляции не равен нулю (существует линейная связь).

2. **Выбор уровня значимости**:

   - $\alpha = 0.05$.

   Уровень значимости 0,05 является традиционным порогом, который устанавливается заранее исследователем для оценки того, насколько можно считать результат статистически значимым. Он означает, что допустимая вероятность ошибочного отклонения нулевой гипотезы (ошибки первого рода) составляет 5%. Иными словами, если $p$-значение теста меньше 0,05, то вероятность того, что полученный результат произошёл случайно при условии верности $H_0$, менее 5%, и нулевая гипотеза отвергается.

    Такой порог принят по историческим и практическим причинам: он обеспечивает разумный баланс между риском ложного положительного вывода и чувствительностью теста. В более консервативных областях наук (например, медицина или генетика) применяют более строгие уровни значимости, например 0,01 или 0,001, чтобы снизить вероятность ошибок. В менее строгих или прикладных исследованиях иногда выбирают уровни выше 0,05.

    Таким образом, 0,05 — это общепринятая «планка», позволяющая стандартизировать интерпретацию статистических результатов и принимать решения на основе контролируемой вероятности ошибки.

3. **Выбор статистического критерия**:

    Используем t-тест для оценки значимости коэффициента корреляции. Критическое значение $t$-критерия определяется из таблицы значений $t$-распределения для выбранного уровня значимости $\alpha$ и числа степеней свободы $df = n-2$:

    $$ t = r \sqrt{\frac{n-2}{1-r^2}} $$

    Для данной выборки выбран t-критерий по следующим причинам:

    - Объем выборки мал — всего 6 наблюдений. При малых размерах выборок стандартные нормальные критерии (z-тесты) не подходят, так как распределение выборочных средних может существенно отклоняться от нормального и стандартные ошибки необходимо оценивать более аккуратно.

    - Предполагается, что данные (вес и рост) имеют приблизительно нормальное распределение в генеральной совокупности, что является условием применимости t-критерия.

    - t-критерий учитывает, что дисперсия генеральной совокупности неизвестна и оценивается по выборке, используя несмещённую оценку дисперсии. Это обеспечивает более точную статистическую проверку гипотез при небольшом размере выборки.

    - t-критерий подходит для проверки значимости коэффициента корреляции Пирсона при малых выборках и позволяет оценить, насколько корреляция статистически отлична от нуля.

    Таким образом, выбор t-критерия обусловлен его устойчивостью и корректностью при анализе малых выборок при выполнении предположения нормальности распределения и неизвестности истинной дисперсии.

4. **Формулирование правила принятия решения**:

    Отвергаем $H_0$, если $∣t∣ > t_{α/2,n−2}$ — критическое значение t-распределения с $n−2=4$ степенями свободы.

5. **Проведение теста**:

    - Рассчитаем статистику:

    $$ t = r \sqrt{\frac{n-2}{1-r^2}} = 0.68 \sqrt{\frac{4}{1-0.68^2}} \approx 1.855 $$

   - Критическое значение $t_{0.025,4} ≈2.776$ (двусторонний тест).

   ![Таблица Стьюдента](../img/student-table.png)

6. **Принятие решения**:

   - Поскольку $1.855 < 2.776$, статистика не попадает в критическую область.

   - Таким образом, на уровне значимости 0.05 нет достаточных оснований отвергнуть нулевую гипотезу.

   - Следовательно, данные не подтверждают статистически значимую линейную связь между весом и ростом в данной выборке.

**Итог**: хотя коэффициент корреляции показывает положительную связь (около 0.68), статистическая значимость отсутствует при выбранном уровне $α=0.05$ из-за малого объема выборки.

##### Варианты расчета
Результаты коэффициента корреляции $r$ – Пирсона — для примера со студентами.

Студент | Часы $x$ | Оценка $y$ | $xy$ | $x^2$ | $y^2$
| - | -- | --- | ---- | -- | ----- |
| A | 6  | 82  | 492  | 36 | 6724 |
| B | 2  | 63  | 126  | 4  | 3969  |
| C | 1  | 57  | 57   | 1  | 3249  |
| D | 5  | 88  | 440  | 25 | 7744  |
| E | 2  | 68  | 136  | 4  | 4624  |
| F | 3  | 75  | 225  | 9  | 5625  |
| **Σ** | **19** | **433** | **1476** | **79** | **31935** |

Вычисляем коэффициент корреляции Пирсона:

$$ r(x, y) = \frac{n \cdot xy - \sum{x}\sum{y}}{\sqrt{n \sum{x^2}-(\sum{x})^2} \sqrt{n \sum{y^2}-(\sum{y})^2}} = \frac{6 \cdot 1476 - 19 \cdot 433}{\sqrt{6 \cdot 79-19^2} \sqrt{6 \cdot 31935-433^2}} \approx 0.922 $$

Вычисляем t-статистику для оценки статистической значимости:

$$ t = \frac{r \sqrt{n-2}}{\sqrt{1-r^2}} = \frac{0.922 \sqrt{4}}{\sqrt{1-0.848}} \approx 4.75 $$

Критическое значение $t$ при $α=0.05$ и 4 степенях свободы (двусторонний тест) составляет около 2.776 (по таблице степей свободы).

![Таблица Стьюдента](../img/student-table.png)

Итог:
- Коэффициент корреляции $r≈0.922$ говорит о сильной положительной линейной связи между часами и оценкой.

- Вычисленное $t=4.72$ больше порогового 2.776, значит связь статистически значима на уровне 0.05.

- Отвергается нулевая гипотеза об отсутствии связи.

Таким образом, связь между часами обучения и оценками является значимой и положительной.

##### Особенности коэффициента корреляции Пирсона
- Коэффициент корреляции r-Пирсона оценивает только **линейную связь** переменных. Нелинейную связь данный коэффициент выявить не может.
- Коэффициент корреляции Пирсона очень чувствителен к **аутлаерам** (**выбросам**).
- Корреляция **не подразумевает наличия причинно-следственной связи** между переменными.
- **Нельзя путать** коэффициент корреляции Пирсона с критерием Пирсона ХИ-квадрат.

---

!!! todo Задание

    Вычислить коэффициент корреляции для ваших данных.

!!! todo Задача на понимание

    Однажды я попросил, чтобы студенты ответили на 2 вопроса анкеты «ваш год рождения» и «ваш возраст». Из их ответов я сформировал таблицу, в которой был столбец Р=«год рождения студента» и Q=«возраст студента».

    - **Вопрос 1**: оцените (приближенно) КК $r (P, Q)$.

    - **Вопрос 2**: как зависит $r(P, Q)$ от месяца, в котором проводится опрос (я не шучу)?

#### Бисериальный коэффициент корреляции
<dfn title="бисериальный коэффициент корреляции">Бисериальный коэффициент корреляции</dfn> — это статистический показатель связи между одной дихотомической (бинарной) переменной и одной количественной переменной. Он измеряет, насколько сильно и в каком направлении связаны эти две переменные.

Бисериальный коэффициент корреляции относится к группе параметрических методов, так как он представляет собой модификацию коэффициента линейной корреляции Пирсона. Для его корректного применения обычно предполагаются нормальность распределения количественной переменной в каждой из двух групп бинарной переменной и линейная связь между переменными. Значимость коэффициента оценивается с помощью t-критерия Стьюдента, что также характерно для параметрических методов.

**Основные особенности**:

- Используется, когда одна переменная принимает два значения (например, «есть/нет», «мужчина/женщина»), а другая — количественная (например, рост, вес).

- Рассчитывается как модификация коэффициента Пирсона с учетом бинарного характера одной переменной.

- Позволяет оценить, насколько средние значения количественной переменной различаются между двумя группами дихотомической переменной.

- Принимает значения от -1 до +1, где по величине отражает силу связи, а знак интерпретировать обычно не рекомендуется.

Бисериальный коэффициент корреляции вычисляется по формуле:

$$ r_{bis} = \frac{(\bar{Y_1} - \bar{Y_2}) \cdot \sqrt{p \cdot q}}{S_Y} $$

где:

- $\bar{Y}_1$ — среднее значения количественной переменной $Y$ для группы, у которой бинарная переменная $X=1$,

- $\bar{Y}_0$ — среднее значения количественной переменной $Y$ для группы, у которой бинарная переменная $X=0$,

- $p$ — доля наблюдений с $X=1$ (то есть $p= \frac{n_1}{N}$),

- $q=1−p$ — доля наблюдений с $X=0$,

- $S_Y$ — стандартное отклонение всей количественной переменной,

- $N$ — общее число наблюдений.

Формула отражает разницу средних в двух группах количественной переменной, масштабированную на распределение бинарной переменной и дисперсию количественной. Формула учитывает средние значения количественной переменной для каждой категории бинарного признака, число наблюдений в каждой группе и стандартное отклонение количественной переменной.

Для оценки значимости часто используется t-критерий Стюдента с $N−2$ степенями свободы. Этот коэффициент подходит для оценки связи между бинарной переменной и количественной, отражая влияние бинарного признака на распределение численной переменной.

Бисериальный коэффициент корреляции широко применяется в психологии, педагогике и социальных науках для анализа связи бинарных факторов и измеряемых показателей.

Таким образом, этот коэффициент помогает количественно измерить и интерпретировать связь между бинарной категорией и непрерывной величиной

##### Алгоритм расчета
Если у вас одна группа с двумя признаками — один количественный, другой бинарный (дихотомический), то для вычисления бисериального коэффициента корреляции используется следующая схема:

1. Разделить данные на две подгруппы по бинарному признаку:

   - Группа с признаком, равным 1 (например, $n_1$ элементов),

   - Группа с признаком, равным 0 (например, $n_0$ элементов).

2. Вычислить средние значения количественного признака для каждой из этих групп: $Y_1$ и $Y_0$.

3. Найти общее стандартное отклонение количественного признака $S_Y$.

4. Рассчитать пропорции групп: $p = \frac{n_1}{n_1+n_0}$, $q=1−p$.

5. Подставить в формулу бисериального коэффициента корреляции:

    $$ r_{bis} = \frac{(\bar{Y}_1 - \bar{Y}_2) \cdot \sqrt{p \cdot q}}{S_Y} $$

6. Проверить статистическую значимость выявленной корреляции.

Таким образом, бисериальный коэффициент отражает стандартизированную разницу средних количественной переменной между двумя группами, заданными бинарным признаком. Его величина показывает, насколько сильно отличается количественный признак в зависимости от бинарного фактора.

Это классический способ измерить связь между бинарным и количественным признаками в одной выборке.

### Практическая работа. Исследование меры линейной связи между количественными признаками для метрических шкал

#### Цель
- Овладеть методами вычисления и интерпретации коэффициентов корреляции (Пирсона, Спирмена), умением строить корреляционные модели и проверять статистическую значимость связи.

#### Задачи
Исследовать меру линейной связи между двумя признаками по метрической шкале с помощью коэффициента корреляции в соответствии с индивидуальным вариантом, указанным в прилагаемом файле. 

1. Определить основные числовые характеристики числовых признаков $X$ и $Y$:

   - объем выборки $n$​;
   - минимальные и максимальные значения $x_{\min}$​, $x_{\max}$​, $y_{\min}$​, $y_{\max}$​;
   - размах вариации $R(X)$ и $R(Y)$;
   - выборочные средние​ $\bar{x}$ и $\bar{y}$;
   - выборочные дисперсии $S^2_x$ и $S^2_y$;
   - выборочные стандартные отклонения $S_x$​, $S_y$​;
   - ковариацию $S_{xy}$​.

2. Рассчитать выборочный коэффициент корреляции $r$ Пирсона.
3. Построить диаграмму рассеяния для визуализации связи.
4. Проверить статистическую значимость коэффициента корреляции с помощью t-статистики для уровня значимости $α = 0.05$.*
5. Сделать выводы о силе и характере (направлении), а также статистической значимости связи.

Отчет по проделанной работе оформить в виде отдельного документа в соответствии со стандартами колледжа к оформлению квалификационных работ.

\* Пороговые значения критерия t-Стьюдента приведены в соответствующем приложении в теме "Общее".

#### Комментарий к заданию
Для сгенерированных данных о весе по росту возможна реалистичная зависимость, исходя из стандартных норм индекса массы тела (ИМТ) примерно в пределах 18.5–24.9, что считается нормальным диапазоном веса.

Для генерации данных атмосферного давления для корреляционного анализа среднесуточных температур можно смоделировать синтетические значения давления, приведённого к уровню моря, с учётом следующих рекомендаций:

- Колебания давления обычно лежат в диапазоне примерно от 980 до 1040 гПа.

- Давление меняется постепенно и часто коррелирует с изменениями температуры.

- Можно задать давление с некоторым трендом и случайными колебаниями с нормальным распределением.

- Барометрическую тенденцию (изменение давления по сравнению с предыдущим днем) можно сгенерировать как небольшие случайные изменения.

Эти данные имитируют постепенные изменения давления с некоторой вариабельностью, связанную с изменениями температуры.

Для генерации реалистичных данных об атмосферном давлении при заданных температурах предлагается следующий подход:

- Основной тренд — давление плавно повышается при понижении температуры и понижается при её повышении, но не линейно, а с шумом (случайными отклонениями), отражающим влияние атмосферных фронтов, динамику воздуха и локальные условия.

- Атмосферное давление колеблется в пределах примерно 1010–1035 гПа на уровне моря в умеренных широтах, с суточными и метеорологическими вариациями.

Поэтому к базовому значению давления (например, около 1020 гПа) добавим отрицательную корреляцию с температурой, но с добавлением нормального случайного шума, чтобы избежать идеальной линейности.

Сгенерировать данные об атмосферном давлении на основе заданного распределения значений с учётом естественных колебаний можно, добавив к исходным значениям небольшие случайные отклонения, моделирующие природные вариации давления (например, нормально распределённые шумы с малой дисперсией). Это даст реалистичное вариативное распределение давления в гектопаскалях.

Например, для каждого значения из данного вами списка можно прибавить случайную величину, которая симулирует атмосферные колебания на уровне нескольких единиц гектопаскалей, так как атмосферное давление подвержено суточным и погодным изменениям с порядком величины до нескольких гектопаскалей.

#### Пример расчета
**Задание**:
1. Составить дискретный и интервальный вариационные ряды для выборки данных по признаку $X$ в соответствии с индивидуальным вариантом.
2. Построить гистограмму распределения значений признака $X$.
4. Проанализировать обобщенные характеристики (центральную тенденцию, разброс данных, степень вариативности) и сделать выводы относительно выборки (форме, нормальности, симметричности и однородности).

**Задачи**:
1. Собрать или получить готовый набор парных данных по двум признакам (например, рост и вес, количество часов занятий и успеваемость, температура и посещаемость достопримечательностей).

2. Рассчитать выборочный коэффициент корреляции Пирсона вручную и с помощью Excel (функция КОРРЕЛ).

3. Построить диаграмму рассеяния для визуализации связи.

4. Рассчитать и проанализировать коэффициент корреляции Спирмена (ранговый).

5. Проверить статистическую значимость коэффициента корреляции с помощью критерия t-Стьюдента или встроенных средств Excel.

6. Сделать выводы о силе и характере связи, обсудить возможные причины и ограничения.

Оборудование и ПО:
- Компьютер с MS Excel или другим табличным процессором, калькулятор.

Отчет должен содержать:

- Описание выборки и исходных данных.

- Пошаговый расчет коэффициентов корреляции.

- Построенные графики.

- Результаты проверки значимости.

- Выводы по исследованию и рекомендации.

Такой практикум помогает студентам закрепить теоретические знания на практике, развить навыки статистического анализа и интерпретации корреляционных данных и подготовиться к работе с реальными исследовательскими задачами.

Предлагаю полностью проработанный вариант лабораторной работы по исследованию корреляции между ростом и весом студентов.

Лабораторная работа: Исследование корреляционной связи между ростом и весом студентов

**Цель**:
- Научиться вычислять и интерпретировать коэффициент корреляции Пирсона, оценивать силу и значимость зависимости между ростом и весом.

**Исходные данные**:
- Предполагается набор данных по 10 студентам:

| Студент | Рост (см) | Вес (кг) |
| ------- | --------- | -------- |
| 1       | 170       | 67       |
| 2       | 175       | 73       |
| 3       | 180       | 75       |
| 4       | 178       | 81       |
| 5       | 172       | 79       |
| 6       | 168       | 69       |
| 7       | 165       | 65       |
| 8       | 183       | 80       |
| 9       | 176       | 77       |
| 10      | 174       | 74       |

**Ход работы**:

1. Рассчитать выборочные средние значения роста и веса:

$$ \bar{x} = \frac{1}{n} \sum_{i=1}^n{x_i}, \bar{y} = \frac{1}{n} \sum_{i=1}^n{y_i} $$

2. Вычислить выборочную ковариацию и дисперсии:

$$ S_{xy} = \frac{1}{n-1} \sum_{i=1}^n{(x_i - \bar{x})(y_i - \bar{y})}, S_x^2 = \frac{1}{n-1} \sum_{i=1}^n{(x_i - \bar{x})^2}, S_y^2 = \frac{1}{n-1} \sum_{i=1}^n{(y_i - \bar{y})^2} $$

3. Найти коэффициент корреляции Пирсона:

$$ r = \frac{S_{xy}}{S_x S_y} $$

4. Построить диаграмму рассеяния (scatter plot) данных для визуальной оценки зависимости.

5. Проверить статистическую значимость корреляции с помощью t-статистики:

    $$ t = r \sqrt{\frac{n-2}{1-r^2}} $$

    Сравнить с критическим значением $t$ для степени свободы $n−2$.

Результаты (числовые):

- $\bar{x}$ (средний рост) $≈ 173.1$ см

- $\bar{y}$  (средний вес) $≈ 74.0$ кг

- Выборочная ковариация $S_{xy} ≈ 19.8$

- Дисперсия роста $S_x^2 ≈ 28.9$, стандартное отклонение $S_x ≈5.37$

- Дисперсия веса $S_y^2 ≈ 29.1$, стандартное отклонение $S_y ≈5.39$

- Коэффициент корреляции Пирсона $r≈0.68$, что указывает на умеренно сильную положительную связь.

t-статистика приблизительно равна 2.61 при $n=10$, что выше критического значения для уровня значимости 0.05 (около 2.3), значит корреляция статистически значима.

**Вывод**:
Обнаружена значимая положительная линейная связь между ростом и весом студентов. Чем выше рост, тем больше вес, что укладывается в биологические ожидания. Знание и умение проводить такой анализ поможет студентам применять статистические методы для исследования взаимоотношений между переменными в реальных данных.

Такой пример демонстрирует полное исследование корреляции на учебных данных с пошаговыми расчетами и интерпретацией результатов. Графическая визуализация и статистические проверки позволяют закрепить понимание теоретического материала и развить аналитические навыки.

### Практическая работа. Исследование парных связей между количественными и бинарными признаками

#### Задачи
Исследовать меру линейной связи между количественным и бинарными признаками с помощью бисериального коэффициента корреляции в соответствии с данными индивидуального варианта, приводимыми в прилагаемом файле.

1. Рассчитать бисериальный коэффициент корреляции $r_{bis}$. Расчет всех промежуточных величин привести в отчете.
2. Построить диаграмму рассеяния для визуализации связи.
3. Проверить статистическую значимость коэффициента корреляции с помощью t-статистики для уровня значимости $α = 0.05$.*
5. Сделать выводы о силе и характере (направлении), а также статистической значимости связи.

Отчет по проделанной работе оформить в виде отдельного документа в соответствии со стандартами колледжа к оформлению квалификационных работ.

\* Пороговые значения критерия t-Стьюдента приведены в соответствующем приложении в теме "Общее".

#### Пример расчета
Имеются данные роста (в см) и пола группы студентов:

| Студент | Рост (см) | Пол |
| ------- | --------- | -------- |
| 1       | 170       | ж       |
| 2       | 175       | м       |
| 3       | 180       | м       |
| 4       | 178       | м       |
| 5       | 172       | ж       |
| 6       | 168       | ж       |
| 7       | 165       | ж       |
| 8       | 183       | м       |
| 9       | 176       | м       |
| 10      | 174       | ж       |

1. Данные делятся на две подгруппы по бинарному признаку:

   - Группа с бинарным признаком $X$, равным 1 ("м") (например, $n_1$ элементов): 175, 180, 178, 183, 176. $n_м = 5$.

   - Группа с бинарным признаком $X$, равным 0 ("ж") (например, $n_0$ элементов): 170, 172, 168, 165, 174. $n_ж = 5$.

2. Вычисляются средние значения количественного признака для каждой из этих групп: $Y_1$ и $Y_0$.

    $$ \bar{Y}_м = \frac{175+180+178+183+176}{5} = 178.4 $$

    $$ \bar{Y}_ж = \frac{170+172+168+165+174}{5} = 169.8 $$

3. Находится общее стандартное отклонение количественного признака $S_Y$.

   - Расчет общего среднего:

   $$ \bar{Y} = \frac{170+172+168+165+174+170+172+168+165+174}{10} = 174.1 $$

   - Вычисление квадратов отклонений:

   | Рост ($Y$) | $Y-\bar{Y}$ | $(Y-\bar{Y})^2$
   | --- | ---- | ----- |
   | 170 | -4.1 | 16.81 |
   | 175 | 0.9  | 0.81  |
   | 180 | 5.9  | 34.81 |
   | 178 | 3.9  | 15.21 |
   | 172 | -2.1 | 4.41  |
   | 168 | -6.1 | 37.21 |
   | 165 | -9.1 | 82.81 |
   | 183 | 8.9  | 79.21 |
   | 176 | 1.9  | 3.61  |
   | 174 | -0.1 | 0.01  |

   - Сумма квадратов:

        $$ SS=16.81+0.81+34.81+15.21+4.41+37.21+82.81+79.21+3.61+0.01=274.9 $$

   - Дисперсия:

        $$ S^2 = \frac{274.9}{10-1} = 30.54 $$

   - Стандартное отклонение:

        $$ S_Y = \sqrt{30.54} = 5.53 $$

4. Рассчитать пропорции групп:

    $$
    p = \frac{n_м}{n_м+n_ж} = \frac{5}{10} = 0.5\\[1em]

    q=1−p = 1 - 0.5 = 0.5
    $$

5. Вычислить бисериальный коэффициент корреляции:

    $$
    r_{bis} = \frac{(\bar{Y_1} - \bar{Y_2}) \cdot \sqrt{p \cdot q}}{S_Y} =
    \frac{(178.4 - 169.8) \cdot \sqrt{0.5 \cdot 0.5}}{5.53} = \frac{4.3}{5.53} \approx 0.778
    $$

6. Проверить статистическую значимость выявленной корреляции.

   - Выдвигается нулевая гипотеза $H_0$ о том, что между ростом и полом отсутствует значимая статистическая связь.

   - Вычисление t-статистики:

    $$
    t = r_{bis} \sqrt{\frac{n-2}{1-r_{bis}^2}} =
    0.778 \sqrt{\frac{10-2}{1-0.778^2}} \approx 3.50
    $$

    - Критическое значение для степени свободы $df = n−2$ и уровня значимости $α = 0.05$ $t_к = 2.3636$

    $$ 3.5 > 2.3636, t > t_к $$

    t-статистика приблизительно равна 3.5 при $n=10$ и $df = n−2 = 8$, что выше критического значения для уровня значимости 0.05 (около 2.3636) и дает возможность отвергнуть нулевую гипотезу, позволяя сделать вывод о том, что выявленная между ростом и полом корреляция является статистически значимой и может быть перенесена на генеральную совокупность, что в целом соответствует биологическому ожиданию.

#### Комментарии к заданию
Для дополнения показателей роста студентов данными о поле, чтобы они выглядели как реальные, лучше учитывать распределение пола среди студентов и связанное с полом различие в росте. Для генерации реалистичных данных о поле рекомендуется приблизительно равное распределение мужчин и женщин с учетом того, что у женщин, как правило, рост ниже, чем у мужчин.

Реальное распределение пола в студенческой группе обычно близко к равному, но может немного смещаться (например, около 50-55% женщин и 45-50% мужчин). Средний рост женщин обычно ниже, чем у мужчин, примерно на 10-15 см в студенческом возрасте. Это распределение выглядит правдоподобно и готово к анализу с учётом пола как фактора, влияющего на рост.

Чтобы сгенерировать бинарный признак наличия осадков на основе данных об относительной влажности, нужно выбрать порог влажности, при достижении или превышении которого будем считать, что осадки есть (1), а иначе — их нет (0).

Часто для определения вероятности осадков берут влажность около 80–85%. Для демонстрации примем порог = 85%. Значения влажности >= 85% — осадки есть (1), ниже — нет (0).

При этом следует учитывать, что существует некоторый разброс данных относительно выбранного порога относительной влажности для генерации бинарного признака наличия осадков. Причина в том, что осадки зависят не только от влажности, но и от множества других факторов, таких как температура, давление, ветер и температура точки росы. Относительная влажность показывает лишь, насколько воздух насыщен влагой относительно максимального возможного на данной температуре, но сама по себе не гарантирует выпадение осадков.

На практике иногда осадки могут выпадать при влажности чуть ниже выбранного порога (например, около 80-85%), если другие условия способствуют конденсации влаги, или не выпадать при влажности выше порога, если отсутствуют другие необходимые условия. Также возможны вариации в зависимости от температуры и других атмосферных факторов, влияющих на процесс конденсации и перенасыщения воздуха.

Таким образом, бинарный признак с фиксированным порогом влажности — это упрощение, и ожидается некоторый разброс данных и отклонения от точного совпадения осадков с порогом влажности.

Исходя из общих климатических данных, относительная влажность около 85-90% часто ассоциируется с возможностью осадков, но из-за природных колебаний и факторов (температура, давление, и др.) осадки могут выпадать при влажности немного ниже или не выпадать при влажности немного выше этого порога.

Можно предложить такой подход для бинара:

Влажность >= 88% — осадки есть (1)

Влажность между 83% и 88% — вероятность осадков есть, но с разбросом; признак можно поставить как 1 с некоторой случайной вероятностью или в зависимости от дополнительной информации

Влажность < 83% — осадков нет (0)

### Заключение
Взаимосвязь количественных переменных в статистике — это ситуация, когда изменение одной количественной переменной сопровождается закономерным изменением другой. Такая связь может быть прямой (обе переменные изменяются в одном направлении) или обратной (одна переменная растет, а другая уменьшается). Сила и направление взаимосвязи количественных переменных обычно измеряются с помощью коэффициентов корреляции (например, коэффициент Пирсона), а для описания математической зависимости используется регрессионный анализ, который позволяет построить уравнение регрессии и делать прогнозы на основе известных значений переменных.

Корреляционная связь не обязательно означает причинно-следственную зависимость, а отражает статистическую зависимость в среднем у наблюдаемых данных. В реальных условиях конкретные значения одной переменной могут варьироваться из-за влияния множества факторов, но в среднем наблюдается ожидаемое изменение. Например, урожайность напрямую связана с количеством внесенных удобрений, но плотность почвы, погодные условия и другие факторы также влияют на итоговый результат.

Для анализа взаимосвязи количественных переменных применяют:

- коэффициенты корреляции для измерения степени зависимости;

- регрессионные модели для объяснения и предсказания изменений одной переменной на основе другой;

- анализ взаимодействий, учитывающий совместное влияние нескольких переменных на результат.

Таким образом, взаимосвязь количественных переменных является фундаментальным понятием в статистике для понимания и моделирования сложных зависимостей между числовыми признаками в данных.

### Источники информации
[^korrelyatsiya]: [Корреляция](https://blog.skillfactory.ru/glossary/korrelyatsiya/)
[^proverka-statisticheskix-gipotez]: [5.2 Индуктивная статистика](https://questionstar.ru/uchebnik-kak-provodit-oprosy/analiz-dannix-statisticheskie-metodi/induktivnaya-statistika/proverka-statisticheskix-gipotez)
[^vybor-statisticheskogo-kriteriya]: [Выбор статистического критерия для тестирования гипотез](https://lit-review.ru/biostatistika/vybor-statisticheskogo-kriteriya/)
[^735800]: [Степени свободы в статистике](https://habr.com/ru/companies/otus/articles/735800/)
