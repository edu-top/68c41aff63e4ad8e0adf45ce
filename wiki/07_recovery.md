<link href="../styles.css" rel="stylesheet" />

## Восстановление пропусков

- [Восстановление пропусков](#восстановление-пропусков)
  - [Постановка проблемы](#постановка-проблемы)
  - [Механизмы формирования пропусков](#механизмы-формирования-пропусков)
  - [Методы устранения пропусков](#методы-устранения-пропусков)
  - [Простейшие способы устранения пропусков](#простейшие-способы-устранения-пропусков)
    - [Исключение данных](#исключение-данных)
    - [Общие принципы реализации ad-hoc импутации](#общие-принципы-реализации-ad-hoc-импутации)
    - [Пропуски в вещественных признаках](#пропуски-в-вещественных-признаках)
    - [Пропуски в категориальных и бинарных признаках](#пропуски-в-категориальных-и-бинарных-признаках)
    - [Пропуски в хронологических данных](#пропуски-в-хронологических-данных)
    - [Индикатор пропуска](#индикатор-пропуска)
    - [Особенности ad-hoc методов](#особенности-ad-hoc-методов)
    - [Выбор метода заполнения пропусков](#выбор-метода-заполнения-пропусков)
    - [Выводы об использовании простых методов](#выводы-об-использовании-простых-методов)
  - [Восстановление данных с помощью метрики](#восстановление-данных-с-помощью-метрики)
    - [Свойства метрики](#свойства-метрики)
    - [Евклидово расстояние (расстояние по прямой)](#евклидово-расстояние-расстояние-по-прямой)
    - [Расстояние L1 (расстояние городских кварталов)](#расстояние-l1-расстояние-городских-кварталов)
    - [Расстояние Чебышёва (метрика шахматной доски)](#расстояние-чебышёва-метрика-шахматной-доски)
    - [Использование метрики](#использование-метрики)
    - [Выбор метрики](#выбор-метрики)
    - [Выводы по метрике](#выводы-по-метрике)
    - [Исходный код](#исходный-код)
  - [Нормирование (нормализация) признаков](#нормирование-нормализация-признаков)
    - [Замечание об использовании метрики](#замечание-об-использовании-метрики)
    - [Способы нормирования признака](#способы-нормирования-признака)
      - [Стандартизация](#стандартизация)
      - [Диапазонное шкалирование (min-max scaling)](#диапазонное-шкалирование-min-max-scaling)
      - [Нормализация средним (centered min-max scaling)](#нормализация-средним-centered-min-max-scaling)
      - [Нормализация максимумом по модулю (MaxAbsScaler)](#нормализация-максимумом-по-модулю-maxabsscaler)
      - [Робастное шкалирование (Robust scaler)](#робастное-шкалирование-robust-scaler)
      - [Другие способы](#другие-способы)
    - [Использование нормализации признаков](#использование-нормализации-признаков)
      - [Программный код](#программный-код)
  - [Оценка качества восстановления данных](#оценка-качества-восстановления-данных)
    - [Количественные метрики](#количественные-метрики)
    - [Сравнение распределений](#сравнение-распределений)
    - [Кросс-валидация и диагностика](#кросс-валидация-и-диагностика)
    - [Практическая реализация (Python)](#практическая-реализация-python)
  - [Практическая работа. Восстановление данных с помощью простых и продвинутых методов](#практическая-работа-восстановление-данных-с-помощью-простых-и-продвинутых-методов)
  - [Дополнительные источники](#дополнительные-источники)
  - [Источники информации](#источники-информации)

*[MICE]: Multiple/Multivariate Imputation by Chained Equations
*[MCAR]: Missing Completely At Random
*[MAR]: Missing At Random
*[MNAR]: Missing Not At Random
*[LOCF]: Last observation carried forward

### Постановка проблемы
Идеальные данные бывают лишь в теории. На практике не существует наборов без пропусков или некорректных значений. На практике в реальных данных очень часто встречаются пропуски. Причинами могут быть ошибки ввода данных, сокрытие информации, фрод.  Отвечать на вопрос "Кто в этом виноват?" бесполезно, поэтому в этом разделе мы затронем лишь вопрос "Что с этим делать?"

Очень редко в жизни встречаются таблицы с данными, в которых все ячейки заполнены корректными значениями. Гораздо чаще встречаются таблицы, в которых часть ячеек вообще пустая, а другая часть может быть заполнена заведомо некорректными значениями, противоречащими здравому смыслу? Естественно возникает вопрос: "А что делать в этом случае? Как бороться с пустыми ячейками или ячейками, заполненными заведомо недостоверными данными?" Ниже демонстрируется таблица, часть ячеек которой не заполнена, а часть содержит заведомо некорректные значения.

Студент | Пол | Рост | Вес | Место на олимпиаде
-- | -- | -- | -- | --
Иванов | 1 | 172 | 107 | 3
Запеканка | **?** | 185 | 64 | **-4**
Ватрушкина | 0 | 168 | **666** | 2
Ололоева | 0 | **?** | 85 | 1

Видно, что в таблице могут быть незаполненные ячейки, или же содержимое некоторых ячеек заведомо некорректное и противоречит здравому смыслу. Скорее всего, данные значения являются следствием описки человека, заносившего данные в таблицу. Однако выяснение первопричин пропусков и ошибок не относится к задачам машинного обучения. Наша задача — восстановить пропущенные значения и исправить ошибочные. Какие существуют простые способы для борьбы с пропусками данных и некорректными значениями? 

Таким образом, когда мы измеряем признаки объектов, некоторые из признаков могут отсутствовать. В этом случае вектор признаков будет содержать пропуски (*missing values*). Например, если объект описывает человека по данным заполняемой анкеты, то человек мог не заполнять отдельные поля или сделать это неразборчиво. При этом большинство моделей машинного обучения требуют для обработки _полный вектор признаков без пропусков_.

### Механизмы формирования пропусков
Для того чтобы понять, как правильно обработать пропуски, необходимо определить механизмы их формирования. <dfn title="механизм пропусков">Механизм пропусков</dfn> (*missing data mechanism*) — это классификация причин отсутствия данных по модели Рубина (Rubin), определяющая, зависит ли вероятность пропуска от наблюдаемых или пропущенных значений, что влияет на выбор метода импутации.

Различают следующие 3 механизма формирования пропусков: MCAR, MAR, MNAR:

- <dfn title="полностью случайные пропуски">Полностью случайные пропуски</dfn> **MCAR** (Missing Completely At Random) — механизм формирования пропусков, при котором вероятность пропуска для каждой записи набора одинакова. Например, если проводился социологический опрос, в котором каждому десятому респонденту один случайно выбранный вопрос не задавался, причем на все остальные заданные вопросы респонденты отвечали, то имеет место механизм MCAR. В таком случае игнорирование/исключение записей, содержащих пропущенные данные, не ведет к искажению результатов. Поскольку вероятность отсутствия значения одинакова для всех записей и не зависит ни от самих данных, ни от других переменных (например, сбой оборудования), то простые методы (удаление, среднее) работают без смещения.

- <dfn title="случайные пропуски">Случайные пропуски</dfn> **MAR** (Missing At Random) — на практике данные обычно пропущены не случайно, а ввиду некоторых закономерностей. Здесь вероятность пропуска зависит от наблюдаемых данных, но не от пропущенных самих по себе (например, молодые чаще пропускают доход, но возраст известен). Пропуски относят к MAR, если вероятность пропуска может быть определена на основе другой имеющейся в наборе данных информации (пол, возраст, занимаемая должность, образование…), не содержащей пропуски. В таком случае удаление или замена пропусков на значение «Пропуск», как и в случае MCAR, не приведет к существенному искажению результатов. Подходят KNN, регрессия, MICE.

- <dfn title="неслучайные пропуски">Неслучайные пропуски</dfn> **MNAR** (Missing Not At Random) — механизм формирования пропусков, при котором данные отсутствуют в зависимости от неизвестных факторов. Здесь вероятность зависит от самих пропущенных значений или неизвестных факторов (например, богатые скрывают доход). MNAR предполагает, что вероятность пропуска могла бы быть описана на основе других атрибутов, но информация по этим атрибутам в наборе данных отсутствует. Как следствие, вероятность пропуска невозможно выразить на основе информации, содержащейся в наборе данных. Требуют специальных моделей (модели с выбором, Bayesian), удаление рискует сильным смещением.

Рассмотрим различия между механизмами MAR и MNAR на примере.

Люди, занимающие руководящие должности и/или получившие образование в престижном вузе чаще, чем другие респонденты, не отвечают на вопрос о своих доходах. Поскольку занимаемая должность и образование сильно коррелируют с доходами, то в таком случае пропуски в поле доходы уже нельзя считать совершенно случайными, то есть говорить о случае MCAR не представляется возможным.

Если в наборе данных есть информация об образовании и должности респондентов, то зависимость между повышенной вероятностью пропуска в графе доходов и этой информацией может быть выражена математически, следовательно, выполняется гипотеза MAR. В случае MAR исключение пропусков вполне приемлемо.

Однако если информация о занимаемой должности и образовании у нас отсутствует, то тогда имеет место случай MNAR. При MNAR просто игнорировать или исключить пропуски уже нельзя, так как это приведет к значительному искажению распределения статистических свойств выборки.

### Методы устранения пропусков
Простые методы подразумевают использование следующих стратегий:

- **удаление строк/столбцов**: применяется, если пропусков мало (менее 5-10%) или они сосредоточены в одном атрибуте, минимизируя потери информации, при этом минимизирует искажения, но уменьшает объем данных;
- **замена средним, медианой или модой**: заполняет пропуски статистическими мерами по столбцу, подходит для числовых или категориальных данных с нормальным/симметричным распределением;
- **замена константой или нулем**: использует фиксированное значение, простое для категориальных переменных, но может вносить смещение;
- **интерполяция и экстраполяция**: восстанавливает значения на основе соседних точек в упорядоченных данных, например, временных рядах.​

Продвинутые методы:
- **регрессионная импутация** предсказывает значения с помощью линейной регрессии на основе других признаков, учитывая их взаимосвязи, а **стохастическая** добавляет шум для реалистичности;
- **KNN** (k-ближайших соседей) находит похожие строки и усредняет значения из похожих строк по евклидову расстоянию, эффективно для многомерных данных;
- **MICE** (множественная импутация на основе марковских цепей) генерирует несколько версий датасета для учета неопределенности, а datawig использует нейросети для дискретных значений;
- **глубокое обучение** (нейросети через библиотеки вроде datawig) обучается на полных записях для предсказания отсутствующих значений, особенно полезно для категориальных данных.

### Простейшие способы устранения пропусков
Рассмотрим популярные методы восстановления для различных типов данных.[^Data-imputation]

#### Исключение данных
Часто в данных, с которыми необходимо работать, присутствуют пропуски, в результате чего аналитик оказывается перед выбором: игнорировать, отбросить или же заполнить пропущенные значения. Заполнение пропусков зачастую и вполне обоснованно кажется более предпочтительным решением. Однако это не всегда так. Неудачный выбор метода заполнения пропусков может не только не улучшить, но и сильно ухудшить результаты.[^missing]

Простейшим выходом при работе с пропусками могло бы быть исключение всех объектов, у которых хотя бы один из признаков не указан. При данном подходе удаляются строки таблицы (*объекты*), содержащие пропущенные или некорректные значения. Этот приём подходит, если число объектов с пропущенными значениями невелико относительно общего объема выборки (число пропусков менее 5-10%). Если же таких объектов много, то нужно выработать **метод заполнения пропущенных значений** (*missing data imputation*). <dfn title="импутация">Импутация</dfn> — это процесс заполнения пропущенных значений в датасетах для машинного обучения, чтобы сохранить объем данных и избежать искажений в моделях.

Другим вариантом является удаление столбца таблицы (*признака*), содержащего ошибки и пропуски, особенно в том случае, если в нём очень много пропусков. Данный метод применяется, если пропуски сосредоточены в одном атрибуте. Эти способы являются довольно-таки радикальными и применять их следует с осторожностью, потому что если в таблице будет много пустых ячеек, разбросанных более-менее равномерно по всей таблице, то удаляя объекты или признаки можно уменьшить количество и качество данных настолько, что из них уже будет невозможно извлечь интересующую нас полезную информацию. Оба рассмотренных подхода минимизирует искажения, но уменьшает объем данных.​ Поэтому рекомендуется избегать прямого удаления строк или столбцов таблицы и применять более изощренные методы восстановления данных.

<dfn title="complete-case analysis">Complete-case Analysis</dfn> (он же <dfn title="listwise deletion method">Listwise Deletion Method</dfn>) — метод обработки пропусков, применяемый во множестве прикладных пакетов как метод по умолчанию. Заключается в исключении из набора данных записей/строк или атрибутов/колонок, содержащих пропуски.

В случае первого механизма пропусков (MCAR) применение данного метода не приведет к существенному искажению параметров модели. Однако удаление строк приводит к тому, что при дальнейших вычислениях используется не вся доступная информация, стандартные отклонения возрастают, полученные результаты становятся менее репрезентативными. В случаях, когда пропусков в данных много, это становится ощутимой проблемой.

Кроме того, в случае второго (MAR) и, особенно, третьего механизма пропусков (MNAR) смещение статистических свойств выборки, значений параметров построенных моделей и увеличение стандартных отклонений становятся еще сильнее.

Таким образом, несмотря на широкое распространение, применение данного метода для решения практических задач ограничено.

<dfn title="available-case analysis">Available-case analysis</dfn> (он же <dfn title="pairwise deletion">Pairwise Deletion</dfn>) — методы обработки, основанные на игнорировании пропусков в расчетах. Эти методы, как и Complete-case Analysis, тоже часто применяются по умолчанию.

Статистические характеристики, такие как средние значения, стандартные отклонения, можно рассчитать, используя все непропущенные значения для каждого из атрибутов/столбцов. Как и в случае Complete-case Analysis, при условии выполнения гипотезы MCAR применение данного метода не приведет к существенному искажению параметров модели.

Преимущество данного подхода в том, что при построении модели используется вся доступная информация.

Главный же недостаток данных методов заключается в том, что они применимы для расчета далеко не всех показателей и, как правило, сопряжены с алгоритмическими и вычислительными сложностями, приводящими к некорректным результатам.

Например, рассчитанные значения коэффициентов корреляции могут оказаться вне диапазона [-1; 1]. Кроме того, не всегда удается однозначно ответить на вопрос об оптимальном выборе числа отсчетов, используемого при расчете стандартных отклонений.

К недостаткам первых двух методов обработки пропусков (Complete-case Analysis и Available-case analysis) относится и то, что далеко не всегда исключение строк в принципе приемлемо. Нередко процедуры последующей обработки данных предполагают, что все строки и колонки участвуют в расчетах (например, когда пропусков в каждой колонке не очень много, но при этом строк, в которых нет ни одного пропущенного поля, мало).

Исключение и игнорирование строк с пропущенными значениями стало решением по умолчанию в некоторых популярных прикладных пакетах, в результате чего у начинающих аналитиков может возникнуть представление, что данное решение — правильное. Кроме них существуют довольно простые в реализации и использовании методы обработки пропусков, получившие название ad-hoc методы, простота которых может послужить причиной их выбора.

Далее мы рассмотрим методы, которые предполагают заполнение пропусков на основе имеющейся информации. Часто эти методы объединяют в одну группу, называемую Single-imputation methods.

#### Общие принципы реализации ad-hoc импутации

> Ad hoc (дословно — «к этому») — латинская фраза, означающая «для данного случая», «специально для этого». Как правило, фраза обозначает способ решения специфической проблемы или задачи, который невозможно приспособить для решения других задач и который не вписывается в общую стратегию решений, составляет некоторое исключение. Например, закон ad hoc — это закон, принятый в связи с каким-то конкретным инцидентом или для решения какой-то особой задачи, который не вписывается в законодательную практику и не решает других схожих проблем; отдел ad hoc — это подразделение в организации, созданное для решения какой-то узкой задачи, не попадающей в сферу компетенции ни одной из постоянных структур. В некоторых случаях выражение ad hoc может иметь негативный подтекст, предполагая отсутствие стратегического планирования и реакционные непродуманные действия.[^Ad_hoc]
> 
> Ad hoc задачи — это задачи, которые возникают спонтанно и требуют немедленного решения. Обычно они не являются частью регулярного рабочего процесса или заранее запланированных мероприятий. Такие задачи могут возникать в результате неожиданных ситуаций, проблем или потребностей, которые требуют быстрого реагирования. Например, клиент изменил условия сделки и срочно нужно переделать договор, государство повысило налоги — нужно срочно пересмотреть бюджет компании.[^chto-takoe-ad-hoc-zadachi]

Ad-hoc методы (среднее, медиана, мода) применяют для быстрого заполнения пропусков в тренировочных таблицах: выбор зависит от типа признака (числовой/категориальный), распределения (симметрия/выбросы) и % пропусков (<20% для простоты). Ниже приведен шаблон на Python с обоснованиями и оценкой. Предполагаем датасет `df` с числовыми (например, `age`, `income`) и категориальными (например, `gender`) признаками. Обоснование: среднее — для симметричных числовых без выбросов; медиана — при выбросах; мода — для категориальных с majority-классом

```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error
import numpy as np

# Загрузка данных (замените на ваш файл)
# df = pd.read_csv('your_data.csv')

# 1. Числовой признак 'age' (<10% пропусков, нормальное распределение) — среднее
imp_mean = SimpleImputer(strategy='mean')
df['age_mean'] = imp_mean.fit_transform(df[['age']])
# Обоснование: сохраняет центр распределения без искажения при MCAR.[web:81]

# 2. Числовой 'income' (выбросы) — медиана
imp_median = SimpleImputer(strategy='median')
df['income_median'] = imp_median.fit_transform(df[['income']])
# Обоснование: устойчива к выбросам, лучше среднего для скошенных данных.[web:2]

# 3. Категориальный 'gender' (0/1, мода=0) — мода
imp_mode = SimpleImputer(strategy='most_frequent')
df['gender_mode'] = imp_mode.fit_transform(df[['gender']])
# Обоснование: сохраняет доминирующий класс, подходит при <20% пропусков.[web:81]
```

Качество оценивают метриками на тестовых пропусках (маск-аут: искусственно удалить известные значения, импутировать, сравнить): MAE/RMSE для числовых ($MAE = \frac{1}{n} \sum{|y_i - \hat{y_i}|}$), accuracy для категориальных.

```python
# Тест: маска известных, MAE
mask = ~df['age'].isna()
mae_mean = mean_absolute_error(df.loc[mask, 'age'], df.loc[mask, 'age_mean'])
print(f'MAE age (mean): {mae_mean}')  # Вывод: низкий MAE → эффективно
# Аналогично для других; медиана лучше при выбросах (MAE ниже на 10-20%).[web:81]
```

**Выводы** (обобщенные): Среднее эффективно для `age` (MAE<5%, сохраняет mean); медиана — для `income` (MAE<10%, игнорирует выбросы); мода — для `gender` (accuracy>90%, но смещает minority). Интерполяция лучше для временных рядов.



#### Пропуски в вещественных признаках
Для вещественных признаков пропуски можно заполнять

- константой;

- выборочным средним[^Arithmetic_mean];

- выборочной медианой[^Median];

Замена константой или нулем в импутации данных — это простой метод, при котором все пропущенные значения (NaN) в столбце заменяются фиксированным значением, таким как 0, -1, без учета распределения данных. Данный метод подходит для числовых данных, где отсутствие значения логически равно нулю (возраст 0 для несуществующих записей). Он быстрый и не искажает корреляции, но может вносить смещение в средние значения и снижать вариативность датасета, что вредно для моделей машинного обучения.

**Заполнение пропуска средним значением** (*Mean Substitution*) (другие варианты: заполнение нулем, медианой и тому подобные) — название метода говорит само за себя. 
Выборочные статистики считаются по присутствующим значениям признака в других объектах. Медиана более предпочтительна, поскольку является мерой оценки центра распределения, устойчивой к наличию выбросов (*robust to outliers*), в отличие от среднего.

!!! info Задача

    Оцените, насколько сильно может сместиться среднее и медиана при внесении одного очень большого или очень малого наблюдения в выборку.

Итак, для числовых признаков можно заменить значение в ячейке на среднее (медиану, моду, ...) из значений столбца. Например для ряда (столбца) значений признака $<0, 1, 2, 4, 4, 5, ?>$ последнее значение можно восстановить, заменив его на среднее ($2.67$), медиану ($3$) или моду ($4$).

> Может смутить, что малый перевес в сторону нулей в случае применения моды или медианы, увеличивает этот перевес, что может привести к ещё большему дисбалансу в сторону "большинства". Тогда, если на самом деле реальные данные соответствовали равномерному распределению объектов по полу, наблюдаемая вероятность нахождения единицы в признаке с 40 % исказится до 33 %, а не приблизится к действительной оценке в 50 %. Отсюда вопрос, применяются ли какие-то "компенсаторные" методы при восстановлении пропущенных значений? Если да, от по какому ключевому слову поискать о них информацию?
>
> В больших наборах данных метод вероятностной подстановки не приводит к смещению, т.к. нули и единицы будут генерироваться в той же пропорции. В маленькой таблице, как в этом примере, любая подставновка приведет к дисбалансу.

Всем вариантам данного метода свойственны одни и те же недостатки. Рассмотрим эти недостатки на примере одного из наиболее простых способов заполнить пропуски непрерывной характеристики: заполнения пропусков средним арифметическим значением и модой.

_Пример 1_. На рисунке 1 показано распределение значений непрерывной характеристики до заполнения пропусков средним значением и после него.

![Распределение значений непрерывной характеристики до заполнения пропусков](../img/blog_schemes_027-article-missing-data_continuous-char-val.svg)

*Рисунок 1a — Распределение значений непрерывной характеристики до заполнения пропусков*

![Распределение значений непрерывной характеристики после заполнения пропусков](../img/blog_schemes_027-article-missing-data_distortion-continuous.svg)

*Рисунок 1б — Распределение значений непрерывной характеристики после заполнения пропусков*

На рисунке 1 хорошо видно, что распределение после заполнения пропусков выглядит крайне неестественно. Это в итоге проявляется в искажении всех показателей, характеризующих свойства распределения (кроме среднего значения), заниженной корреляции и завышенной оценке стандартных отклонений.

Таким образом, данный метод приводит к существенному искажению распределения характеристики даже в случае MCAR.[^missing]

#### Пропуски в категориальных и бинарных признаках
Для категориальных признаков, принимающих одно из $K$ дискретных значений, таких как марка машины, профессия и город проживания человека пропуски можно заполнять:

- максимально частой категорией (в статистике это называется **модой распределения**, *mode imputation*);

- сгенерированным вероятностно рандомизированными значением — **стохастическая импутация по распределению** (*stochastic imputation based on empirical distribution*) или **импутацией по вероятностным весам** (*probability-weighted imputation*), где пропуски заполняются случайными значениями из наблюдаемого распределения с сохранением исходных пропорций (в примере 3/5 для 0 и 2/5 для 1);

- новой категорией — пропуск заменяется уникальной строкой ("unknown", "missing"), превращая его в отдельный класс без смещения моды; лучше моды, когда категорий много, и позволяет моделям учить "отсутствие" как сигнал;

В случае категориальной дискретной характеристики наиболее часто используется заполнение модой.

На рисунке 2 показано распределение категориальной характеристики до и после заполнения пропусков.

![Распределение дискретной характеристики до заполнения пропусков модой](../img/blog_schemes_027-article-missing-data_categorical-char-val.svg)

*Рисунок 2а — Распределение дискретной характеристики до заполнения пропусков модой*

![Распределение дискретной характеристики после заполнения пропусков модой](../img/blog_schemes_027-article-missing-data_distortion-categorical.svg)

*Рисунок 2б — Распределение дискретной характеристики после заполнения пропусков модой*

Таким образом, при заполнении пропусков категориальной характеристики модой проявляются те же недостатки, что и при заполнении пропусков непрерывной характеристики средним арифметическим (нулем, медианой и тому подобным).[^missing]

Второй подход учитывает частоты классов в столбце (эмпирическое распределение), генерируя значения случайно, но с заданными вероятностями, чтобы избежать смещения моды и сохранить вариативность данных — в отличие от простой замены на моду, которая всегда ставит majority-класс. Например, есть такой признак, как пол человека ($<0, 0, 0, 1, 1, ?>$) с пропущенным значением. Для него можно сгенерировать случайную величину, которая примет значение 0 с вероятностью 3/5 ($A = 0, p(A) = \frac{3}{5}$) или 1 с вероятностью 2/5 ($B=1, p(B) = \frac{2}{5}$). Полученное значение следует записать вместо пропуска.

Если возник вопрос как сгенерировать 0 или 1 с разной вероятностью, то на питоне можно сделать так:  `random.choices([0, 1], weights=[3/5, 2/5])[0]`. В Python это реализуется через random.choices() с weights, как указано, или в scikit-learn через комбинацию `SimpleImputer(strategy='most_frequent')` с последующей рандомизацией; в продвинутых библиотеках (`iterativeimputer`) добавляется шум для стохастичности

Для нашего примера ($<0,0,0,1,1,?>$): `random.choices([0,1], weights=[0.6,0.4])[0]` даст 0 в ~60% случаев, сохраняя баланс без искажения корреляций.​ Данный метод подходит для категориальных признаков с несбалансированными классами, лучше моды при >2 категорий, но требует осторожности с большой долей пропусков (>20%), чтобы не усилить шум.

Третий способ подразумевает использование фиксированного значение ("missing", "unknown" и т.п.), простого для категориальных переменных, что, очевидно, лучше, когда число категорий велико, и нет оснований во всех неопределённых случаях предпочитать самую частую категорию. Метод подходит для категориальных переменных, где пропуски интерпретируют как отдельную категорию (например, fill_value='unknown'), он быстрый и не искажает корреляции, но может вносить смещение в средние значения и снижать вариативность датасета, что вредно для моделей машинного обучения.

Замена константой или нулем и заполнение новой категорией — это частные случаи одного метода импутации (SimpleImputer с strategy='constant'), но они отличаются по применению: первая подходит для числовых данных (например, 0), вторая — специально для категориальных, где пропуск вводится как отдельная категория вроде "missing" или "[UNK]".

**Различия в подходах**
- **Константа/ноль для числовых**: Заполняет NaN фиксированным числом (0, -1), что логично, если пропуск означает отсутствие (например, нулевой доход), но искажает статистику и снижает вариативность.​

- **Новая категория для категориальных**: Пропуск заменяется уникальной строкой ("unknown", "missing"), превращая его в отдельный класс без смещения моды; лучше моды, когда категорий много, и позволяет моделям учить "отсутствие" как сигнал.

Более продвинутая техника заключается в предсказании пропущенной категории отдельным классификатором, обученным предсказывать значение признака с пропусками по оставшимся известным признакам. Это позволит заполнять значения не константой, а переменной величиной в зависимости от других характеристик объекта. Например, если человек не указал город проживания, но указал место работы, то его местоположение можно восстановить с некоторой точностью.[^Data-imputation]

#### Пропуски в хронологических данных
Для данных, полученных в хронологическом порядке, правильнее всего брать предыдущее значение, считая, что оно не изменилось. Либо брать значение за аналогичный период времени из прошлого (время суток). Данные подходы правомерно применять только для временных рядов (1 и 2 метода), имеющих компоненту цикличности (2 метода). Опять же не помешало бы при этом рассчитать скользящее среднее, а для второго варианта скорее всего более уместно будет применить что-то типа "гусеницы".

- **LOCF** (*Last observation carried forward*) — повторение результата последнего наблюдения. Данный метод применяется, как правило, при заполнении пропусков во временных рядах, когда последующие значения априори сильно взаимосвязаны с предыдущими.

- **Интерполяция и экстраполяция** — методы заполнения пропусков в хронологических данных (временных рядах), где интерполяция оценивает значения внутри известного интервала по соседним точкам, а экстраполяция — за его пределами (в начало или конец).

Рассмотрим 2 случая, когда применение LOCF обосновано.

**Случай 1**. Если мы измеряем температуру воздуха в некоторой географической точке на открытом пространстве, причем измерения проводятся каждую минуту, то при нормальных условиях — если исключить природные катаклизмы — измеряемая величина априори не может резко (на 10–20 °C) измениться за столь короткий интервал времени между последующими измерениями. Следовательно, заполнение пропусков предшествующим известным значением в такой ситуации обоснованно.

**Случай 2**. Если данные представляют собой результаты измерения (допустим, той же температуры воздуха) в один и тот же момент времени в близких географических точках таким образом, что гипотеза о малых изменениях значений от одной точки набора данных до другой остается справедливой, то опять же использование LOCF логично.

> Ситуации, когда использование LOCF обосновано, не ограничиваются только этими двумя случаями.

Хотя в описанных выше ситуациях метод логичен и обоснован, он тоже может привести к существенным искажениям статистических свойств даже в случае MCAR [Molenberghs, 2007]. Так, возможна ситуация, когда применение LOCF приведет к дублированию выброса (заполнению пропусков аномальным значением). Кроме того, если в данных много последовательно пропущенных значений, то гипотеза о небольших изменениях уже не выполняется и, как следствие, использование LOCF приводит к неправильным результатам.

<dfn title="линейная интерполяция">Линейная интерполяция</dfn> строит прямую между соседними точками: для значений $y_0$ в $x_0$ и $y_1$ в $x_1$ пропуск в $x$ заполняется как:

$$ y = y_0 + \frac{(y_1-y_0)(x-x_0)}{x_1-x_0} $$

Полиномиальная (квадратичная, кубическая) или сплайн-интерполяция использует кривые для трендовых данных, минимизируя осцилляции (феномен Рунге); в pandas: `df.interpolate(method='linear')` или `'cubic'`. Для временных индексов применяется `method='time'`, учитывая даты.

<dfn title="экстраполяция">Экстраполяция</dfn> продлевает тренд за интервал: линейная — по последней/первой паре точек, полиномиальная — с риском ошибок из-за неопределенности; в pandas через `limit_direction='both'` или `'forward/backward'` с `limit_area='outside'`. Подходит для прогнозирования на краях ряда, но менее точна, чем интерполяция, и требует осторожности с шумом.

Применение в Python: временной ряд с пропусками: `df['value'].interpolate(method='linear', limit_direction='both')` заполнит внутри и снаружи; для сплайнов `method='spline', order=3`. Методы эффективны при регулярных интервалах и трендах, но требуется проверка на выбросы.

#### Индикатор пропуска
Можно создать дополнительный бинарный признак, характеризующий, было ли значение первоначального признака известно заранее или было пропущено с последующим заполнением. Это позволит модели машинного обучения различать эти две ситуации и относиться к автоматически заполненному значению признака _с большим недоверием_.

При заполнении условной модой/средним/медианой можно также генерировать признак условного стандартного отклонения, чтобы подсказать модели, с каким уровнем неопределённости признак был предсказан.

<dfn title="индикаторный метод">Индикаторный метод</dfn> (*Indicator Method*) — метод, предполагающий замену пропущенных значений нулями и добавление специального атрибута-индикатора, принимающего нулевые значения для записей, где данные изначально не содержали пропусков, и ненулевые значения там, где ранее были пропуски. Проще и нагляднее продемонстрировать данный метод на примере.

**Пример**. В таблице 3 приведены данные до заполнения пропусков.

| Параметр | 12,7 | 7,5 | ?  | 3,1 | ?  | ?  | 5 | 5,8 | 3,7 |
| -------- | ---- | --- | -- | --- | -- | -- | - | --- | --- |
| Пропуск  |      |     | Да |     | Да | Да |   |     |     |

*Таблица 3 — Данные до заполнения пропусков*

> Знаком ? обозначены пропуски в наборе данных.

В таблице 4 приведены данные после заполнения пропусков.

| Параметр | 12,7 | 7,5 | 0 | 3,1 | 0 | 0 | 5 | 5,8 | 3,7 |
| -------- | ---- | --- | - | --- | - | - | - | --- | --- |
| Флаг     | 0    | 0   | 1 | 0   | 1 | 1 | 0 | 0   | 0   |

*Таблица 4 —Таблица после заполнения пропусков*

На практике применяются и модификации этого метода, предполагающие заполнение пропусков ненулевыми значениями. Стоит отметить, что при таком заполнении (например, средним) допустимо использование инверсных значений поля флагов (то есть 0 — для случая, когда в исходных данных значения изначально были пропущены, и ненулевое значение для случаев, когда значение поля исходных данных было известно).

Также при заполнении пропусков ненулевыми значениями часто добавляется взаимодействие поля-флага и исходного поля.[^missing]

Таким образом, <dfn title="индикаторный метод">индикаторный метод</dfn> (*indicator method* или *dummy variable method*) — подход к обработке пропусков, при котором пропущенные значения заполняются константой (часто средним/медианой или модой), а дополнительно создается бинарный индикаторный признак (0/1), отмечающий наличие пропуска, чтобы модель могла учитывать информацию об отсутствии данных.

Пропуски в столбце заменяются фиксированным значением (например, средним по колонке), а параллельно добавляется новый столбец-индикатор: 1 для пропусков, 0 — для заполненных. Это позволяет моделям машинного обучения (регрессия, деревья) интерпретировать пропуски как отдельный сигнал, снижая смещение от простой импутации, особенно при MAR/MNAR механизмах.

Данный метод используется для категориальных/числовых признаков с 5-20% пропусков, когда удаление строк нежелательно. Пример в Python (pandas): `df['feature_na'] = df['feature'].isna().astype(int); df['feature'].fillna(df['feature'].mean())` — добавляет индикатор и заполняет средним.

**К преимуществам данного метода относятся**:

1. Использование всего набора данных (репрезентативность выборки не страдает).
2. Явное использование информации о пропущенных значениях.

**Недостатки**

1. Метод увеличивает размерность датасета и может привести к неэффективным оценкам по сравнению с анализом полных наблюдений при MCAR; не рекомендуется при >20% пропусков или MNAR.
2. Несмотря на эти преимущества, даже при выполнении гипотезы MCAR и небольшом числе пропущенных значений данный метод может привести к существенному искажению результатов [Vach, 1991, Knol, 2010].

#### Особенности ad-hoc методов
Часто, однако, признаки неправильно заполнять значением "в среднем", поскольку сам факт того, что значение пропущено, может говорить о _нестандартности реального значения признака_. Например, человек мог не указать величину своей зарплаты, если ему кажется, что она слишком маленькая или, наоборот, слишком большая. Для таких ситуаций нужно предсказывать значение пропущенного признака по другим признакам, решая задачу регрессии. Либо просто подставлять условное среднее или медиану при условии другого известного признака, связанного с рассматриваемым.

!!! tip Индикатор пропуска

    Можно создать дополнительный бинарный признак, характеризующий, было ли значение первоначального признака известно заранее или было пропущено с последующим заполнением. Это позволит модели машинного обучения различать эти две ситуации и относиться к автоматически заполненному значению признака _с большим недоверием_.

    При заполнении условной модой/средним/медианой можно также генерировать признак условного стандартного отклонения, чтобы подсказать модели, с каким уровнем неопределённости признак был предсказан.

Программные способы заполнения пропущенных значений в данных представлены в библиотеках pandas[^missing_data], sklearn[^impute] и feature-engine[^imputation].

Выборочные статистики считаются по присутствующим значениям признака в других объектах. Медиана более предпочтительна, поскольку является мерой оценки центра распределения, устойчивой к наличию выбросов (robust to outliers), в отличие от среднего.[^Data-imputation]

Вероятно, именно из-за своей простоты ad-hoc методы широко использовались на заре развития современной теории обработки пропусков. И, хотя по состоянию на сегодняшний день известно, что применение этих методов может приводить к искажению статистических свойств выборки и, как следствие, к ухудшению результатов, получаемых после такой обработки пропусков [Horton, 2007], их по-прежнему часто используют.

Так, известны статьи, посвященные сбору и оценке статистики использования методов заполнения пропусков в научных работах медицинской тематики [Burton, 2004, Karahalios, 2012, Rezvan, 2015], из результатов которых можно сделать вывод, что даже ученые часто отдают предпочтение интуитивно-понятным ad-hoc методам и игнорированию/удалению строк, несмотря на то, что применение этих методов в контексте решаемой задачи порой неуместно.

Применение ad-hoc методов и удаление строк таит в себе множество подводных камней, о которых необходимо знать каждому аналитику.[^missing]

#### Выбор метода заполнения пропусков
Выбор метода импутации пропусков основан на типе данных, механизме пропусков (MCAR/MAR/MNAR), проценте пропусков, распределении, корреляциях между признаками и задаче анализа (ML или статистика).

| Метод                            | Тип данных              | % пропусков | Когда использовать                                   | Преимущества/недостатки ​                            |
| -------------------------------- | ----------------------- | ----------- | ---------------------------------------------------- | ---------------------------------------------------- |
| Среднее/медиана                  | Числовые                | <20%        | Нормальное/симметричное распределение, мало выбросов | Быстро; искажает дисперсию                           |
| Мода                             | Категориальные          | <10%        | Несколько классов, сбалансированные данные           | Простота; смещает majority-класс                     |
| Константа/ноль/новая категория   | Числовые/категориальные | Любые       | Пропуск = отсутствие (0); категории с "missing"      | Логично для редких случаев; снижает вариативность    |
| Стохастическая (по вероятностям) | Категориальные          | 10-30%      | Несбалансированные классы, сохранение распределения  | Сохраняет пропорции; добавляет шум                   |
| KNN                              | Любые                   | <30%        | Многомерные данные с локальными паттернами           | Учитывает сходство; вычислительно затратно           |
| Регрессионная                    | Числовые                | <20%        | Сильные корреляции между признаками                  | Точна при зависимостях; риск мультиколлинеарности    |
| Интерполяция/экстраполяция       | Временные ряды          | <15%        | Хронологические данные с трендами (линейная/сплайн)  | Сохраняет последовательность; плохо для нерегулярных |
| MICE/Deep Learning               | Любые                   | >20%        | Сложные зависимости, высокая неопределенность        | Учитывает неопределенность; требует ресурсов         |

#### Выводы об использовании простых методов
В таблицах с данными очень часто присутствуют ячейки с пропущенными или некорректными значениями. Нужно осознать важность и сложность данной проблемы. Методы восстановления данных в тренировочных таблицах (датасетах для машинного обучения) в основном подразумевают обработку пропущенных значений (импутацию), чтобы избежать искажений в моделях.​ Наиболее простой способ борьбы с такими ячейками — это удаление строк (столбцов), вместе с тем, существуют и более сложные методы восстановления данных.

Мы рассмотрели простые методы заполнения пропусков. Хотя применение этих методов может приводить к существенному искажению статистических свойств набора данных (среднее значение, медиана, вариация, корреляция…) даже в случае MCAR, они остаются часто используемыми не только среди обычных пользователей, но и в научной среде (как минимум в областях, связанных с медициной).

Так, согласно [Burton, 2004], из 100 работ, посвященных проблеме раковых заболеваний, которые были опубликованы в 2002 году, в 82% случаев авторы указали, что столкнулись с не-обходимостью заполнения пропусков в данных. При этом в 32 случаях был явно указан метод заполнения пропусков. В 12 из этих 32 работ использовался Complete Case Analysis, еще в 12 — Available Case Analysis, в 4 — Indicator Method, в 3— ad-hoc методы, и только в 1 случае использовался более сложный метод.

Спустя десятилетие ситуация не сильно изменилась к лучшему. [Karahalios, 2012] пишут, что среди рассмотренных ими научных трудов в 54% случаев (в 21 статье) использовался Complete Case Analysis, в 7 случаях – LOCF, в 3 случаях – заполнение средним значением, в 1 случае — Indicator Method.

И даже по состоянию на 2014 год рекомендуемые к использованию методы заполнения пропусков (Multiple Imputation, методы функции максимального правдоподобия) в научных статьях медицинской тематики по-прежнему применяются редко [Rezvan, 2015].

В качестве заключения хотелось бы отметить, что использование простых методов, таких как удаление строк или применение ad-hoc методов, не всегда приводит к ухудшению результатов. Более того, когда это уместно, использование простых методов более предпочтительно.[^missing]

### Восстановление данных с помощью метрики
Как восстановить данные, используя меру близости объектов друг к другу? Метрика является математическим аналогом понятия близости объектов друг другу. Это пригодится для решения проблемы восстановления пропущенных данных. Допустим, у какого-то объекта (строки таблицы) пропущено какое-либо значение признака (пустая ячейка таблицы). Зная, как вычислить меру близости объектов друг другу, можно для требуемого объекта найти наиболее близкий к нему по свойствам другой объект, у которого значение в соответствующем столбце известно, и с помощью значения из ячейки близкого объекта (ближайшего соседа) восстановить значение ячейки искомого объекта. Для этого и нужно знать, как вычислить меру близости друг другу.

Что такое метрика? Это обобщение понятия расстояние из геометрии — например, знакомого всем из школы расстояния между двумя точками. В отличие от расстояния метрика может быть вычислена не только для пары точек, но также для объектов произвольной природы, при этом она не обязана вычисляться по известной формуле из школьного учебника геометрии: для точек $A(x_1, y_1)$ и $B(x_2, y_2)$ на плоскости расстояние $d = \sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}$. Существует несколько формул для вычисления метрики.

Допустим, даны два набора чисел:

$$ P = (p_1, p_2, \dots, p_n) $$

$$ Q = (q_1, q_2, \dots, q_n) $$

Как найти расстояние между $P$ и $Q$, т.е. вычислить значение метрики на паре $P$ и $Q$?

1. **Евклидова метрика** (L2) (как в школьном учебнике геометрии): чувствительна ко всем измерениям, подходит для нормализованных данных с равновзвешенными признаками; в sklearn: `metric='euclidean'`.

    $$ \rho(P, Q) = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2 + \dots + (p_n-q_n)^2} $$

    $$ d = \sqrt{\sum{(x_i-y_i)^2}} $$

2. **Метрика Манхеттен** (L1): устойчива к выбросам, лучше для высокомерных данных или городских сетей (grid-like); `metric='manhattan'`.

    $$ \rho(P, Q) = |p_1-q_1| + |p_2-q_2| + \dots + |p_n-q_n| $$

    $$ d = \sum{|x_i-y_i|} $$

    При чём тут Манхеттен? Если нужно обойти квартал, то нужно идти по его сторонам, так как нельзя пройти сквозь здание.

    ![Manhattan](../img/manhattan.png)

     Зеленый отрезок на рисунке выше, выражает прямолинейный путь — кратчайшее расстояние между двумя точками. Длина этого отрезка вычисляется как евклидова метрика. В реальности же приходится идти не по прямой, а огибая кварталы. Соответственно, путь, который пешеход пройдет внутри жилого квартала, двигаясь от одной точки до другой, будет выражен синей линией, длина которой вычисляется по метрике Манхеттен. Именно отсюда метрика и заимствует свое название.

3. **Метрика Чебышёва** или **max-метрика** (L∞): фокусируется на максимальном расхождении, идеальна при доминирующем признаке или шахматных паттернах; `metric='chebyshev'`.

    $$ \rho(P, Q) = \max\{|p_1-q_1|, |p_2-q_2|, \dots, |p_n-q_n|\} $$

    $$ d = \max{|x_i-y_i|} $$

Приведенный список не исчерпывает все возможные функции, которые могут быть приняты за метрику. Естественно, не любая функция подойдет в качестве метрики. Она должна удовлетворять некоторым свойствам.

#### Свойства метрики

1. Значение метрики на одинаковых объектов должно быть равно 0:

    $$\rho(P, P) = 0 $$

    Это означает, что расстояние от объекта до него же самого должно быть равно 0 (что, в принципе, логично).

2. Для метрики должен соблюдаться аналог симметричности:

    $$ \rho(P, Q) = \rho(Q, P) $$

    То есть расстояние от первого объекта до второго должно быть равным таковому от второго до первого. Это также разумно. Однако если такие ситуации в жизни, когда расстояние от первой точки до второй не равно таковому от второй точки до первой? Примером такой ситуации могут быть участки городской транспортной сети с односторонним движением.

3. Неравенство треугольника:

    $$ \rho(P, Q) \leq \rho(P, T) + \rho(T, Q) $$

    ![Triangle inequality](./img/triangle-inequality.png)

    Это свойство часто используется в анализе данных. Суть его заключается в том, что если имеются три объекта (точки в пространстве или на плоскости), то расстояние от точки $P$ до точки $Q$ должно быть меньше суммы расстояний через любую промежуточную точку $T$, то есть путь $PQ$ должен быть меньше $PT + TQ$. В качестве примера из жизни, когда такое неравенство не выполняется, можно привести транспортную сеть, где $P$, $T$ и $Q$ — это населенные пункты, причем дорога от $P$ до $Q$ очень извилистая и неудобная, а отрезки $PT$ и $TQ$ — прямые и максимально скоростные. Для такой тройки населенных пунктов неравенство треугольников выполняться не будет.

#### Евклидово расстояние (расстояние по прямой)
Евклидово расстояние самое интуитивное для понимания: именно Евклидову метрику мы представляем, когда кто-то просит нас измерить расстояние между точками.

<dfn title="евклидово расстояние">Евклидово расстояние</dfn> — это прямая линия между двумя точками с координатами X и Y. Например, одной из таких точек может быть город на карте с его координатами долготы и широты.

Евклидово расстояние характеризуется прямой линией. Допустим, вам нужно измерить расстояние по прямой между точками A и B на карте города, приведённой ниже.

![L2](../img/EuclidMap.webp)

Для расчёта Евклидового расстояния вам понадобятся лишь координаты этих двух точек. Дистанцию между ними можно будет рассчитать по формуле Пифагора.

Теорема Пифагора гласит, что можно рассчитать длину «диагональной стороны» (гипотенузы) прямого треугольника, зная длины его горизонтальной и вертикальной стороны (катетов). Формула выглядит так: a² + b² = c².

![L2 расчет](../img/EuclidFormul.webp)

Прим. ред. В четвёртой строке вычислений допущена ошибка: (-260)^2 = 67 600, а не 76 600. Тогда результат будет равен ~321.[^3-basic-distances-in-data-science]

#### Расстояние L1 (расстояние городских кварталов)
Расстояние L1 также известно как расстояние городских кварталов, манхэттенское расстояние, расстояние такси, метрика прямоугольного города — оно измеряет дистанцию не по кратчайшей прямой, а по блокам. Расстояние L1 измеряет дистанцию между городскими блоками: это расстояние всех прямых линий пути.

На следующем изображении показано расстояние L1 между двумя точками.

![L1](../img/L1DistMap.webp)

Кроме показанного пути существует несколько альтернативных способов. Например, от точки A можно подняться на два блока вверх, а потом на три блока вправо, либо же на три блока вправо и два блока вверх.

Но расстояние L1 — это всё же просто дистанция, а поэтому траектория здесь не имеет значения. Единственное, что нужно понимать, это примерный путь: нужно пройти какое-то количество X блоков на восток и Y блоков на север. Сумма расстояний этих блоков и будет расстоянием L1 от точки A до точки B.[^3-basic-distances-in-data-science]

![L1 расчет](../img/L1Formul.webp)

#### Расстояние Чебышёва (метрика шахматной доски)
Расстояние Чебышёва известно ещё как расстояние шахматной доски. Чтобы понять принцип такой метрики, нужно представить короля на шахматной доске — он может ходить во всех направлениях: вперёд, назад, влево, вправо и по диагонали.

![Linf](../img/ChebyshevMap.webp)

Разница расстояния L1 и расстояния Чебышёва в том, что при переходе на одну клетку по диагонали в первом случае засчитывается два хода (например вверх и влево), а во втором случае засчитывается всего один ход.

Ещё эти оба расстояния отличаются от Евклидового расстояния тем, что у Евклидового движение по диагонали рассчитывается по теореме Пифагора.

![Евклидова, L1 и Чебышёва — 3 основные метрики, которые пригодятся в Data Science 6](../img/3vars.webp)

Расстояние Чебышёва можно представить как проход по шахматной доске.

Вот ещё один пример представления расстояния Чебышёва. Допустим, у вас есть дрон с двумя независимыми моторами: первый мотор тянет дрон вперёд, второй — в сторону. Оба мотора могут работать одновременно и равномерно на максимуме своей мощности.

Поэтому дрон может передвинуться на одну клетку по диагонали так же быстро, как по горизонтали или вертикали.

Посмотрите ещё раз на карту города по расстоянию Чебышёва. Первый шаг — оба мотора работают одновременно, второй шаг идентичен первому, а на третьем шаге мотор, тянущий дрон вперёд, отключается, и дрон смещается в сторону.

Таким образом, расстояние Чебышёва определяется как самая большая дистанция на одной оси.

![Linf расчет](../img/ChebyshevFormul.webp)

Прим. ред. Полученный результат является условным и некорректно сравнивать его с другими результатами.[^3-basic-distances-in-data-science]

#### Использование метрики
Давайте попробуем применить метрику для решения нашей задачи. Итак, представим, что есть некоторый объект, представленный строкой таблицы. Пусть у объекта $A$ значение признака $P$ отсутствует или некорректно. Как восстановить значение $P$ для $A$?

Основная идея — рассчитать расстояние от объекта $A$ до других объектов таблицы для того, чтобы найти объекты, наиболее близко расположенные к объекту $A$. Тогда значение признака $P$ из ближайших $k$ объектов можно взять за значение признака $P$ объекта $A$.

1. Исключим пока из таблицы столбец с признаком $P$, содержащим пропуски (мы предполагаем, что остальные ячейки в таблице нормальные).

2. Найдем расстояния (с помощью некоторой метрики) от строки $A$ до остальных объектов таблицы. Получим числа

    $$ \rho(A, A_1), \rho(A, A_2), \dots, \rho(A, A_n) $$

    Эти числа выражают расстояния от объекта $A$ до соответствующего объекта из таблицы.

3. Пусть значения признака $P$ для объектов $A_1, A_2, \dots, A_n$ равны $P(A_1), P(A_2), \dots, P(A_n)$.

Итак, у нас есть числа $P(A_1), P(A_2), \dots, P(A_n)$ и $\rho(A, A_1), \rho(A, A_2), \dots, \rho(A, A_n)$. Как их собрать в одну формулу, которая позволит адекватным образом вычислять значения признака $P$ для объекта $A$?

Пример:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 3 | 4 | 5 | 3 | 4
$A_2$ | 5 | 5 | 5 | 4 | 3
$A_3$ | 4 | 3 | 3 | 2 | 5
$A$  | 5 | 4 | 3 | 3 | **?**

Если пропуск заменить на среднее или медиану по столбцу (здесь они равны друг другу), то нужно писать 4. Попытаемся заполнить пропуск с помощью различных метрик.

Сперва удаляем столбец $P$ из таблицы и находим расстояние от объекта $A$ до остальных объектов из таблицы.

Вид метрики | От $A$ до $A_1$ | От $A$ до $A_2$ | От $A$ до $A_3$
-- | -- | -- | --
Евклид | 2,83 | 2,45 | 1,73
Манхеттен | 4 | 4 | 3
Макс | 2 | 2 | 1

Теперь вопрос: как скомпоновать расстояния от объекта $A$ до всех других объектов таблицы и значения признака $P$ у остальных объектов таблицы? Метрика с точки зрения математики выражает степень близости объектов друг к другу, причем зависимость обратная — чем больше расстояние, тем меньше мера близости, и наоборот — чем меньше расстояние, тем больше мера близости между объектами.

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **?**

Для ответа на этот вопрос нужно взять комбинацию значений признака $P$ у тех объектов, для которых этот признак известен, и меры близости этих объектов к объекту $A$. **Ключевая идея**: признак $P$ для объекта $A$ должен быть близок к значению признака $P$ у близких к $A$ объектов. Линейная комбинация со значениями признака $P$ по евклидовой метрике будет выглядеть следующим образом:

$$ \cfrac{1}{\cfrac{1}{2.83}+\cfrac{1}{2.45}+\cfrac{1}{1.73}}\left( \frac{4}{2.83} + \frac{3}{2.45} + \frac{5}{1.73} \right) = 4.15 $$

Значения признака $P$ здесь у объекта домножается на меру близости (делится на значение метрики, поскольку мера близости по своему смыслу обратно пропорциональна значению метрики, поэтому если метрика очень большая, то мера близости должна быть очень маленькая) и нормирующий множитель (дробь перед скобками). Таким образом получается оценка на значение признака $P$ для объекта $A$ — восстановленное значение признака.

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **4.15**

Естественно, восстанавливаемое значение признака объекта $A$ существенно зависит от выбранной метрики.

Например, матрикс-метрика даёт:

$$ \cfrac{1}{\cfrac{1}{2}+\cfrac{1}{2}+\cfrac{1}{1}}\left( \frac{4}{2} + \frac{3}{2} + \frac{5}{1} \right) = 4.25 $$

Объекты | $P$
-- | --
$A_1$ | 4
$A_2$ | 3
$A_3$ | 5
$A$  | **4.25**

- По Евклиду = 4,15
- По Макс = 4,25
- По Манхеттену = 4,1

Формула = 1/(1/AA1 + 1/AA2 + 1/AA3) * (значA1/AA1 + значA2/AA2 + значA3/AA3)

Расстояния AA1,AA2,AA3 берутся разные в зависимости от метода. ЗначA1, ЗначA2, ЗначA3 берутся из столбца P.

Поэтому выбор оптимальной метрики для решения задачи является делом непростым.

Таким образом, формула для восстановления данных с помощью метрики выглядит следующим образом:

$$ P(A) = \cfrac{1}{\sum_{i=1}^n \cfrac{1}{\rho(A, A_i)}} \left( \sum_{j=1}^n{\cfrac{P(A_j)}{\rho(A, A_j)}} \right) $$

Это формула получения оценки признака $P$ для объекта $A$, зная значения этого признака у других объектов таблицы и зная расстояния от объекта $A$ до других объектов в таблице.

> Что происходит, если у нас встречается объект, совершенно такой же, как и восстанавливаемый (то есть метрики будут равны нулю)?
>
> Это исключительная ситуация, поскольку расстояние между восстанавливаемым объектом и его клоном будет равна 0 (и во многих формулах произойдет деление на 0). Так что способ тут простой: берем значение из клона и пишем его в пропущенную ячейку. Или в знаменатель к метрике можно добавить +1, тогда деления на 0 не будет. Ещё можно придумать такой вариант обращения метрики: max(pi)-pj.

#### Выбор метрики
Обозначения L1, L2, L∞ происходят из семейства Lp-норм (Minkowski norms), где метрика расстояния между точками $x$, $y$ в $n$-мерном пространстве задается как:

$$ d_p(x, y) = \left( \sum_{i=1}^n{|x_i-y_i|^p} \right)^{1/p} $$

где $p$ — параметр нормы.

Объяснение обозначений метрик:

- L1 (манхэттенская): $p=1, d_1 = \sum{|x_i-y_i|}$, сумма абсолютных разностей; устойчива к выбросам, как путь по осям в городе.

- L2 (евклидова): $p=2, d_2 = \sqrt{\sum{(x_i-y_i)^2}}$, корень из суммы квадратов; классическое евклидово расстояние, гладкая оптимизация.

- L∞ (Чебышёва, max-метрика): $p→∞, d_∞ = \max_i{|x_i-y_i|}$, предел прироста $p$ — максимальная абсолютная разность по координатам.

С ростом $p$ вклад доминирующих (больших) разностей усиливается: L1 суммирует равномерно, L2 квадратирует (штрафует средние ошибки), L∞ фокусируется только на худшей координате. В пределе $p=∞$ сумма сводится к максимуму, а для $p=0$/дробных — дискретные нормы (редко в импутации). В ML L1/L2 популярны из-за выпуклости, L∞ — для консервативных оценок.

<dfn title="штрафование ошибок">Штрафование ошибок</dfn> (error penalization) в метриках расстояния — это механизм, при котором разные Lp-нормы по-разному усиливают вклад отдельных разностей $|x_i-y_i|$ между координатами точек, делая акцент на малых, средних или крупных отклонениях для более точного измерения "расстояния" в KNN-импутации или кластеризации.

Механизм штрафования в Lp-нормах: в формуле $d_p(x, y) = \left( \sum_{i=1}^n{|x_i-y_i|^p} \right)^{1/p}$ возведение в степень $p$ "штрафует" разности:

- L1 ($p=1$): Линейное штрафование $∑∣diff∣$, равномерно суммирует все ошибки без усиления; устойчиво к выбросам (одна большая разность не доминирует).​

- L2 ($p=2$): Квадратичное штрафование $\sqrt{∑diff^2}$, средние ошибки растут быстрее (2x ошибка → 4x вклад), крупные — еще сильнее; оптимально для гауссовых данных.​

- L∞ ($p=∞$): Максимальное штрафование $max∣diff∣$, игнорирует мелкие ошибки, фокусируется только на худшей координате (консервативно).

Пример влияния для разностей:​

- L1: 1+3+10=14 (выброс влияет линейно)

- L2: √(1+9+100)≈10.5 (выброс доминирует: 10→100)

- L∞: 10 (только худшая ошибка)​

Это определяет выбор: L1 для шумных данных, L2 для гладких, L∞ для "худшего случая".


#### Выводы по метрике
- Пропущенные значения можно восстанавливать, используя меру близости объектов друг к другу.
- Мера близости объектов вычисляется с помощью метрики.

#### Исходный код

```py
import pandas as pd
import numpy as np
from numpy import NaN

# Создадим датафрейм с пропущенным значением признака Р для объекта А
df = pd.DataFrame({'P1':[3,5,4,5], 'P2':[4,5,3,4], 'P3':[5,5,3,3], 'P4':[3,4,2,3], 'P':[4,3,5,NaN]}, index=['A1', 'A2', 'A3', 'A'])

# Посчитаем метрики
dict_metrics = {'A1':[], 'A2':[], 'A3':[]}
for i in df.index[:-1]:
  dict_metrics[i].append(np.power((df.loc['A'][:-1]-df.loc[i][:-1]).pow(2).sum(), 0.5).round(2)) # считаем Евклидово расстояние
  dict_metrics[i].append((df.loc['A'][:-1]-df.loc[i][:-1]).abs().sum()) # считаем Манхэттеновское расстояние
  dict_metrics[i].append((df.loc['A'][:-1]-df.loc[i][:-1]).abs().max()) # считаем max-метрику

metrics = pd.DataFrame(dict_metrics, index=['Euclid', 'Manhatten', 'Max'])

# Считаем варианты значений для каждой метрики
dict_value = {'Euclid':[], 'Manhatten':[], 'Max':[]}
for i in metrics.index:
  norm_mul = (1/((1/metrics.loc[i]).sum())) # нормирующий множитель
  similarity = ((df.loc[:]['P'][:-1]/metrics.loc[i]).sum()) # значение признака * мера близости(=величина, обратно пропорциональная мере расстояния)
  value_P = (norm_mul*similarity).round(2)
  dict_value[i].append(value_P)
  print(f'значение признака P для А по метрике {i}: {value_P}')
```

```py
import pandas as pd
import numpy as np

df = pd.DataFrame(
    data = {
        "Объекты": ["A1", "A2", "A3", "A"],
        "P1": [3, 5, 4, 5],
        "P2": [4, 5, 3, 4],
        "P3": [5, 5, 3, 3],
        "P4": [3, 4, 2, 3],
        "P":  [4, 3, 5, None]
    }
)
df.set_index("Объекты", inplace=True)
display(df)


# Euclidean metric
metrics = pd.DataFrame(data={"euclidean": np.sqrt(np.sum((df.loc['A'] - df.loc['A1':'A3'])**2, axis=1))})
# manhattan metric
metrics['manhattan'] = np.sum(np.abs(df.loc['A'] - df.loc['A1':'A3']), axis=1)
# max
metrics['max'] = np.max(
    np.abs(df.loc['A'] - df.loc['A1': 'A3']),
    axis=1
)
display('метрики', metrics)

# нормирующий множитель
coeff = 1 / np.sum(1 / metrics, axis=0)
display('нормирующий множитель', coeff)


# меры близости для метрик
p1 = 1 / metrics    # расчет мер близости
display('меры близости', p1)
weights = p1.T @ df.loc['A1':'A3', 'P'] #умножение мер на значения известных призоков P и суммируем
display(weights)
unknown_feature = weights * coeff
display(unknown_feature)
```

работать в ide не будет, если не добавить вначале строку
```py
from IPython.display import display
```

Более общий код. Он заменяет все возможные пропуски nan в любой таблице.

```py
def recover_by_metric(dataFrame, metric):
  clear_data = dataFrame[~dataFrame.isna().any(axis=1)]
  nan_data = dataFrame[dataFrame.isna().any(axis=1)]
  recovered_data = np.zeros(nan_data.shape)
  for i, nan_row in enumerate(nan_data.values):
    clear_indices = np.where(~np.isnan(nan_row))[0]
    metrics = np.array([metric(obj[clear_indices], nan_row[clear_indices]) for obj in clear_data.values])
    args = np.argsort(metrics)
    if metrics[args[0]] == 0:
      recovered_data[i, :] = clear_data.iloc[args[:np.sum(metrics == 0)], :].values.mean(axis=0)
    else:
      recovered_data[i, :] = ((1 / metrics) @ clear_data.values) / np.sum(1 / metrics)
      recovered_data[i, clear_indices] = nan_row[clear_indices]

  return pd.DataFrame(np.concatenate([clear_data.values, recovered_data], axis=0), columns=dataFrame.columns, index=dataFrame.index) 
```

Либо такой вариант, здесь уже масштаб признаков не имеет значения.
```py
from sklearn.preprocessing import StandardScaler

def recover_by_metric(data, metric):
  data_scaled = StandardScaler().fit_transform(data.values)
  clear_data = data_scaled[~np.isnan(data_scaled).any(axis=1)]
  clear_data_origin = data[~data.isna().any(axis=1)]
  nan_data = data_scaled[np.isnan(data_scaled).any(axis=1)]
  nan_data_origin = data[data.isna().any(axis=1)]
  recovered_data = np.zeros(nan_data.shape)
  for i, nan_row in enumerate(nan_data):
    clear_indices = np.where(~np.isnan(nan_row))[0]
    metrics = np.array([metric(obj[clear_indices], nan_row[clear_indices]) for obj in clear_data])
    args = np.argsort(metrics)
    if metrics[args[0]] == 0:
      recovered_data[i, :] = clear_data_origin.iloc[args[:np.sum(metrics == 0)], :].values.mean(axis=0)
    else:
      recovered_data[i, :] = ((1 / metrics) @ clear_data_origin.values) / np.sum(1 / metrics)
      recovered_data[i, clear_indices] = nan_data_origin.iloc[i, clear_indices]

  return pd.DataFrame(np.concatenate([clear_data_origin.values, recovered_data], axis=0), columns=data.columns, index=data.index) 
```

**Вопрос**: В функции `recover_by_metric(data, metric)` в `metric` что необходимо вводить?

**Ответ**: функцию, измеряющую какую-нибудь метрику. Например, из библиотеки sklearn можно подгрузить функции
```py
from sklearn.metrics import mean_squared_error

recover_by_metric(dataFrame, mean_squared_error)
```

Либо свою руками функцию можете написать
```py
def my_MSE(y_true, y_predicted):
    return ((y_true - y_predicted) ** 2).mean()


recover_by_metric(dataFrame, my_MSE)
```

Для дальнейших задач пригодится.

*Пример c тремя признаками-координатами*:
```py
# Координаты векторов P1 и P2
x1, y1, z1 = 0, 1, 2
x2, y2, z2 = 2, 1, 0

# Вычисление Евклидова расстояния
euclidean_distance = ((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)**0.5
print('Евклидово расстояние между векторами Р1 и Р2:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = abs(x2 - x1) + abs(y2 - y1) + abs(z2 - z1)
print('Расстояние Манхэттен между векторами Р1 и Р2:', manhattan_distance)

# Вычисление расстояния в max-метрике
max_metric_distance = max(abs(x2 - x1), abs(y2 - y1), abs(z2 - z1))
print('Расстояние в max-метрике между векторами Р1 и Р2:', max_metric_distance)
```

*Более общее объяснение (формулы с n кол-вом признаков)*:
```py
# Векторы признаков объектов a и b
a = (0, 1, 2, 3, 4)
b = (4, 3, 2, 1, 0)

# Количество признаков
n = len(a)

# Вычисление Евклидова расстояния
euclidean_distance = sum((b[i] - a[i])**2 for i in range(n))**0.5
print('Евклидово расстояние между объектами a и b:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = sum(abs(b[i] - a[i]) for i in range(n))
print('Расстояние Манхэттен между объектами a и b:', manhattan_distance)

# Вычисление расстояния в max-метрике
max_metric_distance = max(abs(b[i] - a[i]) for i in range(n))
print('Расстояние в max-метрике между объектами a и b:', max_metric_distance)
```

Решение с использованием библиотеки scipy (в терминале `pip install scipy`):
```py
from scipy.spatial.distance import euclidean, cityblock, chebyshev

# Векторы признаков объектов a и b
a = (0, 1, 2, 3, 4)
b = (4, 3, 2, 1, 0)

# Вычисление Евклидова расстояния
euclidean_distance = euclidean(a, b)
print('Евклидово расстояние между объектами a и b:', euclidean_distance)

# Вычисление расстояния Манхэттен
manhattan_distance = cityblock(a, b)
print('Расстояние Манхэттен между объектами a и b:', manhattan_distance)

# Вычисление расстояния в max-метрике (метрика Чебышева)
max_metric_distance = chebyshev(a, b)
print('Расстояние в max-метрике между объектами a и b:', max_metric_distance)
```

```py
import numpy as np

class VectorOperations:

    def euclidean_distance(self, vector_a, vector_b):

        """Вычисляет Евклидово расстояние между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        return np.linalg.norm(vector_a - vector_b)



    def manhattan_distance(self, vector_a, vector_b, normalize=True):

        """Вычисляет расстояние Манхэттена (L1-метрика) между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        if normalize:

            vector_a = self._normalize(vector_a)

            vector_b = self._normalize(vector_b)

        return np.sum(np.abs(vector_a - vector_b))



    def max_metric_distance(self, vector_a, vector_b, normalize=True):

        """Вычисляет расстояние Чебышева (L∞-метрика) между двумя векторами."""

        self._check_vectors(vector_a, vector_b)

        if normalize:

            vector_a = self._normalize(vector_a)

            vector_b = self._normalize(vector_b)

        return np.max(np.abs(vector_a - vector_b))



    def _normalize(self, vector):

        """Нормирует вектор по L2-норме."""

        norm = np.linalg.norm(vector)

        if norm == 0:

            return vector  # Избегаем деления на ноль

        return vector / norm



    def _check_vectors(self, vector_a, vector_b):

        """Вспомогательная функция для проверки векторов."""

        if not isinstance(vector_a, np.ndarray) or not isinstance(vector_b, np.ndarray):

            raise TypeError("Векторы должны быть NumPy массивами.")

        if vector_a.shape != vector_b.shape:

            raise ValueError("Векторы должны иметь одинаковую размерность.")

        if not np.issubdtype(vector_a.dtype, np.number) or not np.issubdtype(vector_b.dtype, np.number):

            raise ValueError("Векторы должны содержать только числовые значения.")  



#Пример

vector_operations = VectorOperations()

a = np.array([5, 4,3, 3])

b = np.array([3, 4, 5, 3])



manhattan_normalized = vector_operations.manhattan_distance(a, b)

manhattan_unnormalized = vector_operations.manhattan_distance(a, b, normalize=False)



max_metric_distance_normalized = vector_operations.max_metric_distance(a, b)

max_metric_distance = vector_operations.max_metric_distance(a, b, normalize=False)



print(f"Расстояние Манхэттена (с нормированием): {manhattan_normalized}")

print(f"Расстояние Манхэттена (без нормирования): {manhattan_unnormalized}")



print(f"расстояние Чебышева (с нормированием): {max_metric_distance_normalized}")

print(f"расстояние Чебышева (без нормирования): {max_metric_distance}")
```

### Нормирование (нормализация) признаков

#### Замечание об использовании метрики
Обсудим тонкости, которые возникают при использовании метрики в задачах анализа данных. К сожаление, в данном вопросе не все так однозначно. Работа с метрикой требует аккуратности и большой осторожности. Это относится не только к проблеме восстановления данных, но и к любым другим алгоритмам, использующих понятие метрики.

При вычислении метрики исследуемые объекты представляются в виде точек в некотором пространстве (пространстве признаков), между которыми вычисляется расстояние (метрика). Например, объекты с двумя признаками можно представить в виде точек на плоскости:

![Feature space](../img/feat-space.png)

При данном подходе для адекватной работы алгоритмов анализа данных, использующих метрику, необходимо, чтобы все признаки (значения по осям) имели одинаковый масштаб. Если масштаб признака не одинаков по всем ося, то могут иметь место нежелательные эффекты. Рассмотрим следующую таблицу признаков.

Студент | Вес, кг | Рост, м
-- | -- | --
Иванов | 61 | 1,76
Сидорова | 56 | 1,50
Петров | 100 | 1,98

В данной таблице признаки заведомо имеют различный масштаб. При таком масштабе разница в весе между некоторыми объектами (студентами) может достигать 50 условных единиц, а разница в росте быть ничтожной из-за выбранной единицы измерения. Можно отметить, что в основе всех формул для вычисления метрик положено вычисление разности значений признаков. Таким образом, когда будут вычисляться расстояния между объектами, то для признака веса разница будет почти 50, а для роста она будет незначительной, что приведет к тому, что, например, некоторые признаки фактически будут проигнорированы (в след. примере различия в росте и различия в весе имеют ОЧЕНЬ разную ценность – это из-за разных единиц измерения). Так, в данном примере какая угодно разница в росте по сути не будет иметь никакого значения из-за большой разницы в весе. Это произошло из-за того, что мы имеем два признака с совершенно разными масштабами, что приведет к неадекватной работе нашего алгоритма.

Что нужно сделать, чтобы запустить корректно алгоритм, использующий метрики? При вычислении метрики все признаки необходимо приводить к единой шкале (нормировать).

#### Способы нормирования признака
Входные признаки в большинстве случаев будут иметь разный масштаб (диапазон изменения значений признака): одни признаки могут изменяться в диапазоне $[−0.01,0.01]$, другие — в $[0,100]$ и т.д. Как мы впоследствии увидим из описаний методов машинного обучения, для большинства из них масштаб признаков будет оказывать влияние на прогноз. Для таких моделей чем выше разброс значений признака, тем сильнее он будет влиять на прогноз, перекрывая влияние признаков меньшего масштабаз, поэтому, чтобы влияние всех признаков было одинаковым, их необходимо нормализовать, то есть привести к одному масштабу.[^Feature-normalization]

Допустим, имеется некоторый признак $P = (p_1, p_2, \dots, p_n)$.

- $\bar{p}$ — среднее значение;
- $s$ — отклонение (среднее квадратическое).

Наиболее распространены следующие способы

##### Стандартизация
<dfn title="стандартизация">Стандартизация</dfn> или <dfn title="z-стандартизация">Z-стандартизация</dfn> (*standard scaler, standardization, Z-score normalization*) — метод масштабирования, при котором из каждого значения вычитается среднее $μ$ по столбцу, а результат делится на стандартное отклонение $σ$:

$$ p'_i = \frac{p_i - \bar{p}}{s} $$

Полученные числа будут являться нормированными значениями признака $P$. Данное преобразование гарантирует, что после его выполнения у признака $P$ среднее значение и отклонение будут равны 0 и 1 соответственно.

$$ x'^j = \frac{x^j - \mu_j}{\sigma_j} $$

**Выходные свойства**: нулевое среднее и единичная дисперсия (ско).

**Отличительные особенности**:
- устойчив к выбросам;
- сохраняет форму распределения

**Условия использования**
- идеален для алгоритмов с евклидовой метрикой (KNN, SVM, градиентный спуск), где равный вклад фич критичен;
- подходит для предположительно нормальных данных или временных рядов (центрирование тренда).

*Реализация в Python*
```python
from sklearn.preprocessing import StandardScaler
import pandas as pd

scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(df_scaled.describe())  # mean≈0, std≈1 по столбцам
```

##### Диапазонное шкалирование (min-max scaling)
<dfn title="диапазонное шкалирование">Диапазонное шкалирование</dfn> (*min-max scaling*) — метод нормализации, который преобразует значения признаков так, чтобы они попадали в заданный диапазон, чаще всего от 0 до 1, сохраняя при этом относительные расстояния между точками и свойство «разреженных» данных, где большинство значений нули, что важно для эффективности хранения и обработки. Это противоположность стандартизации, и используется для перевода исходных значений ($x$) в новые ($x'$) по формуле:

$$ p'_i = \frac{p_i - p_{\min}}{p_{\max}-p_{\min}} $$

При этом минимальное значение признака $p$ гарантированно перейдет в 0, максимальное — в 1, а все промежуточные разместятся между ними.

$$ x'^j = \frac{x^j - \min(x^j)}{\max(x^j)-\min(x^j)} $$

**Выходные свойства**: принадлежит интервалу $[0,1]$.

**Отличительные особенности**:
- сохраняет нулевые значения (для sparse data);
- чувствителен к выбросам.

**Условия применения**:
- подходит для алгоритмов, чувствительных к масштабу, например, нейронные сети, метод k-ближайших соседей, обработки изображений/пикселей (значения 0-255 → 0-1) и когда известны теоретические границы признака;
- идеально подходит для работы с разреженными (sparse) данными, где много нулей, так как нули остаются нулями, а ненулевые значения масштабируются в $[0,1]$;
- не подходит для данных с выбросами (>1% экстремальных значений) — один outlier растягивает весь диапазон.​

*Реализация в Python (sklearn)*
```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Базовое [0,1]
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Кастомный диапазон [-1,1]
scaler_custom = MinMaxScaler(feature_range=(-1, 1))
df_custom = scaler_custom.fit_transform(df)

print(df_scaled.describe())  # min=0, max=1 по столбцам
```

##### Нормализация средним (centered min-max scaling)
Центрированная Min-Max нормализация, сочетающая вычитание среднего $μ_j$ по признаку $j$ с масштабированием по диапазону $max(x^j)−min(x^j)$, приводя данные к диапазону примерно [-0.5, 0.5] с центром в 0.

$$ x'^j = \frac{x^j - \mu_j}{\max(x^j)-\min(x^j)} $$

**Выходные свойства**: нулевое среднее, с единичным диапазоном ($[-0.5, 0.5]$ для симметричных данных).

**Отличительные особенности**:
- сохраняет относительные расстояния в пределах диапазона, как Min-Max, но центрирована для градиентного спуска.

**Условия использования**:
- алгоритмы с расстояниями (KNN), где нужен нулевой центр + ограниченный диапазон;
- градиентный спуск (нейросети), где симметрия [-0.5,0.5] ускоряет сходимость;
- данные с известными границами без сильных выбросов.

##### Нормализация максимумом по модулю (MaxAbsScaler)
Масштабирует каждый признак так, чтобы его значения находились в диапазоне [-1, 1] путём деления каждого значения на максимальное по абсолютной величине значение этого признака.

$$ x′ = \frac{x}{\max{(∣x∣)}} $$

**Выходные свойства**: диапазон [-1,1].

**Отличительные особенности**:
- не сдвигает данные (среднее и медиана остаются на месте), в отличие от MinMaxScaler и StandardScaler;
-  устойчив к выбросам лучше, чем MinMaxScaler, так как не зависит от минимального значения.

**Условия использования**:
- особенно полезен для разреженных данных (sparse data), так как не делает значения отрицательными и сохраняет нулевые значения;
- хорошо подходит для алгоритмов, чувствительных к масштабу, например, KNN, SVM, методы с евклидовой метрикой.

*Реализация в Python (sklearn)*
```python
from sklearn.preprocessing import MaxAbsScaler
import pandas as pd

scaler = MaxAbsScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(df_scaled.describe())  # все признаки в [-1,1]
```

##### Робастное шкалирование (Robust scaler)
Масштабирует признаки, используя медиану и межквартильный размах ($IQR$ — разница между 75-м и 25-м процентилями (квартилями), то есть $Q3 - Q1$), а не среднее значение и стандартное отклонение, что делает его устойчивым к выбросам (аномальным значениям — *outliers*). Он вычитает медиану и делит на $IQR$, сохраняя при этом форму распределения данных и эффективно нейтрализуя влияние экстремальных значений, что предпочтительнее в случаях, когда данные содержат выбросы, влияющие на StandardScaler. 

$$ x′ = \frac{x- m_e}{IQR} = \frac{x- m_e}{Q_3(x) - Q_1(x)} $$

**Выходные свойства**: максимальные и минимальные значения каждого признака после трансформации будут $≤1$ и $≥−1$ соответственно.

**Отличительные особенности**:

- **Устойчивость к выбросам**: Медиана и IQR менее чувствительны к экстремальным значениям, чем среднее и стандартное отклонение.
- **Сохранение формы распределения**: Не меняет форму исходных данных так сильно, как другие методы.

**Условия использования**:

- для скошенных распределений;
- когда в данных есть выбросы (экстремальные значения);
- когда важно сохранить относительные расстояния между значениями без сильного искажения;
- данные с выбросами + расстояния (KNN с L1/L2), градиентный спуск, PCA.

*Пример использования (в Python с scikit-learn)*
```python
from sklearn.preprocessing import RobustScaler
import numpy as np

data = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [100, 200]]) # С выбросом 100, 200
scaler = RobustScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)
```

##### Другие способы
Помимо этого к признакам можно предварительно применять разные функции, выполняющие некоторые промежуточные преобразования (например, $log$). Так, логарифмирование оправдано, когда значение признака $P$ сильно разрежено и значения отличаются друг от друга на порядок: $\{1, 10, 100\}$. После применения ко всем признакам логарифма по основанию 10 получается более компактный числовой ряд: $\{0, 1, 2\}$

#### Использование нормализации признаков


Каждый вид шкалирования применяется к каждому признаку (столбцу в матрице объекты-признаки X) _независимо_.

Самым популярным методом нормализации признака является **стандартизация признака**. Вторым по популярности является **диапазонное шкалирование**. Оно хорошо тем, что значения признака из отрезка [0,MAX] переводятся в отрезок [0,1], причём ноль переходит в ноль, что полезно для разреженных данных (*sparse data*), в которых большинство значений — нули. Такие данные часто возникают на практике и эффективно кодируются разреженными матрицами (*sparse matrix*[^sparse-matrix]), которые экономично их хранят и производят операции над ними, оперируя только ненулевыми элементами. Диапазонное шкалирование _позволяет сохранить свойство разреженности_.[^Feature-normalization]

Возвращаясь к примеру из предыдущей темы:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 3 | 4 | 5 | 3 | 4
$A_2$ | 5 | 5 | 5 | 4 | 3
$A_3$ | 4 | 3 | 3 | 2 | 5
$A$  | 5 | 4 | 3 | 3 | **?**

По-хорошему, перед использованием метрики для восстановления значения признака $P$ у объекта $A$ необходимо нормировать все признаки кроме $P$:

Объекты | $P_1$ | $P_2$ | $P_3$ | $P_4$ | $P$
-- | -- | -- | -- | -- | --
$A_1$ | 0 | 0.5 | 1 | 0.5 | 4
$A_2$ | 1 | 1 | 1 | 1 | 3
$A_3$ | 0.5 | 0 | 0 | 0 | 5
$A$  | 1 | 0.5 | 0 | 0.5 | **?**

Здесь минимальное значение (3) переходит в 0, максимальное (5) — в 1, а значение посередине (3) — соответственно, в среднее между 0 и 1 (0.5). Стоить также помнить о том, что если нужно считать расстояние между объектами (строками), то нормируют не по строкам (объектам), а по колонкам (признакам). В колонке P4: мин  2= 1, макс 4 =1, а 3 = 0.5.

> Почему не нормируется признак с пропущенными значениями? Ведь теперь все расстояния между ним и остальными исказились.

> Подспудно признак с пропусками таки нормируется. Его нормировка "зашита" в формулу восстановления пропущенного значения: там мы делим на сумму расстояний. Значения столбца $P$ остаются теми же, а потом берется их взвешенная комбинация (эти числа домножаются на числа, получаемые из метрики). И таким образом в итоге получается число где-то посередке от исходных значений.

> Если нормировать по второй формуле, используя среднее и отклонение, то для признака А1Р1 нормированное значение меньше 0 получится? А1(3) - среднее АР1(4,25) / 0,96 = -1,25 / 0,96.

> При такой нормировке среднее значение признака получается равным нулю, а остальные значения распределяются вокруг него — то есть, могут быть как отрицательными, так и положительными.

> Маленько "словил" когнитивный диссонанс, пытаясь понять, что делать, когда "отклонение" равно нулю. Но потом понял, что в этом случае можно выражение "0/0" (неопределённость) принять равным либо единице, либо какому нибудь другому числу из интервала, соответствующего используемому методу нормировки. То есть [0; 1] или [-1; 1]..

**Выводы**:
- При использовании метрики нужно привести все признаки к одному масштабу (нормировать).
- Существует несколько способов нормирования значений признаков.

##### Программный код

```py
pip install neulab

from neulab.RestoreValue import MetricRestore

d = {'P1': [3, 5, 4, 5], 'P2': [4, 5, 3, 4], 'P3': [5, 5, 3, 3], 'P4': [3, 4, 2, 3], 'P5': [4, 3, 5, np.NaN]}
df = pd.DataFrame(data=d)

# Euclid
euclid_m = MetricRestore(df, row_start=0, row_end=9, metric='euclid')
# Manhattan
mnht_m = MetricRestore(df, row_start=0, row_end=9, metric='manhattan')
# Max
mx_m = MetricRestore(df, row_start=0, row_end=9, metric='max')

Output:
euclid_m = 4.13
mnht_m = 4.1
mx_m = 4.25
```


### Оценка качества восстановления данных
Методы оценки качества импутации проверяют, насколько заполненные значения близки к истинным, и сохраняют ли свойства датасета (распределение, корреляции), используя метрики, визуализацию и статистические тесты.

#### Количественные метрики
*[MAE]: Mean Absolute Error
*[RMSE]: Root Mean Square Error
*[MAPE]: Mean Absolute Percentage Error
*[KS]: Kolmogorov-Smirnov

Для числовых признаков применяют регрессионные метрики на маскированных данных (искусственно удаляют известные значения, импутируют, сравнивают):

- **MAE** (Mean Absolute Error): устойчива к выбросам, низкие значения указывают на точность.​

$$ MAE = \frac{1}{n} \sum{|y_i - \hat{y_i}|} $$

- **RMSE** (Root Mean Square Error): штрафует большие ошибки, стандарт для временных рядов.​

$$ RMSE = \sqrt{\frac{1}{n} \sum{(y_i - \hat{y_i})^2}} $$

- **MAPE** (Mean Absolute Percentage Error): относительная точность, но не для нулевых значений.​

$$ MAPE = \frac{100}{n} \sum{\left| \frac{y_i-\hat{y_i}}{y_i} \right|}, \% $$

Для категориальных: accuracy, F1-score на предсказанных классах.

#### Сравнение распределений
Проверяют, не исказило ли заполнение статистику:

- **KS-тест (Kolmogorov-Smirnov)**: Максимальное расхождение КСД исходного и импутированного распределений (p-value >0.05 — сходны).​

- **Q-Q plots и гистограммы**: Визуально сравнивают формы распределений до/после.​

- **Сравнение моментов**: Mean, variance, skewness — дисперсия не должна падать >10%.

#### Кросс-валидация и диагностика
- **CV на downstream модели**: Обучают ML-модель (e.g. RandomForest) на импутированных данных, сравнивают CV-score (R², AUC) с baseline без пропусков.​

- **Little's MCAR test**: Проверяет механизм пропусков (p>0.05 — MCAR, простые методы OK).​

- **Residual analysis**: ACF/PACF остатков для временных рядов, проверка на автокорреляцию после импутации.

#### Практическая реализация (Python)
Комплекс: метрики <5-10% ошибки + стабильные распределения + рост CV-score:
```python
from sklearn.metrics import mean_absolute_error
from scipy.stats import ks_2samp
# После импутации на маске
mae = mean_absolute_error(true_values, imputed_values)
ks_stat, p_val = ks_2samp(original_dist, imputed_dist)
# Низкий MAE + высокий p-value KS = хорошая импутация
```

### Практическая работа. Восстановление данных с помощью простых и продвинутых методов
Реализовать различные варианты восстановления данных в соответствии с данными индивидуального варианта, приводимыми в прилагаемом файле:

- ad-hoc методами (выборочным средним, медианой, модой и т.п.)
- при помощи метрик (L1, L2, L∞);
- с помощью коэффициента корреляции.

Для каждого восстанавливаемого признака обосновать выбор того или иного метода. Сделать выводы об эффективности реализованных методов восстановления для каждого признака на основе методов сравнения результатов восстановления с фактическими значениями отсутствующих признаков (метрик и т.п.).

### Дополнительные источники
- [Burton, 2004] — Burton A., Altman D. G. Missing covariate data within cancer prognostic studies: A review of current reporting and proposed guidelines. British Journal of Cancer, 2004, 91(1):4–8.
- Wa[Horton, 2007] — Horton N.J., Kleinman K.P. Much ado about nothing: A comparison of missing data methods and software to fit incomplete data regression models. Am. Stat. 2007; 61: pp 79–90.
- [Karahalios, 2012] — Karahalios A., Baglietto L., Carlin J.B., English D.R., Simpson J.A. A review of the reporting and handling of missing data in cohort studies with repeated assessment of exposure measures. BMC Med Res Methodology, 2012;12:96.
- [Knol, 2010] — Knol, M. J., Janssen, K. J. M., Donders, A. R. T., Egberts, A. C. G., Heerdink, E. R., Grobbee, D. E., Moons, K. G. M., and Geerlings, M. I. (2010). - Unpredictable bias when using the missing indicator method or complete case analysis for missing confounder values: an empirical example. Journal of Clinical Epidemiology, 63: pp 728–736.
- [Miettinen, 1985] — Miettinen, O. S. Theoretical Epidemiology: Principles of Occurrence Research in Medicine. John Wiley & Sons, New York. 1985, p. 232.
- [Molenberghs, 2007] — Molenberghs, G. and Kenward, M. G. Missing Data in Clinical Studies. John Wiley & Sons, Chichester, UK. 2007 — pp. 47-50.
- [Rezvan, 2015] — Panteha Hayati Rezvan, Katherine J Lee, Julie A Simpson -The rise of multiple imputation: a review of the reporting and implementation of the method in medical research. BMC Medical Research Methodology, 15(30), pp 1–14.
- [Vach, 1991] — Vach, W. and Blettner, M. (1991). Biased estimation of the odds ratio in case-control studies due to the use of ad hoc methods of correcting for missing values for confounding variables. American Journal of Epidemiology, 134(8), pp 895–907.
- [Van Buuren, 2012] — Van Buuren S. Flexible Imputation of Missing Data. Chapman and Hall/CRC; 1 ed., 2012 — 342 p.
- [Loginom Data Quality. Очистка клиентских данных. Деморолик](https://loginom.ru/blog/demo-ldq)
- [Как найти и объединить дубли клиентов](https://loginom.ru/blog/how-search-dublicate)
- "Статистика для тех,  кто (думает, что) ненавидит статистику" (Нил Дж. Салкинд)
- "Общая теория статистики" (Громыко)
- "Практикум по общей теории статистики" (Ефимова, Ганченко, Петрова)

### Источники информации
[^Data-imputation]: [Заполнение пропусков в данных](https://deepmachinelearning.ru/docs/Machine-learning/Data-preprocessing/Data-imputation)
[^Arithmetic_mean]: [Wikipedia: arithmetic mean](https://en.wikipedia.org/wiki/Arithmetic_mean)
[^Median]: [Median](https://en.wikipedia.org/wiki/Median)
[^missing_data]: [Документация pandas: missing data](https://pandas.pydata.org/docs/user_guide/missing_data.html)
[^impute]: [Документация scikit-learn: восстановление пропущенных значений](https://scikit-learn.ru/stable/modules/impute.html#impute)
[^imputation]: [Документация feature-engine: imputation](https://feature-engine.trainindata.com/en/1.6.x/user_guide/imputation/index.html)
[^missing]: [Обработка пропусков в данных](https://loginom.ru/blog/missing)
[^Ad_hoc]: [Ad hoc](https://ru.wikipedia.org/wiki/Ad_hoc)
[^chto-takoe-ad-hoc-zadachi]: [Что такое ad hoc задачи, зачем они нужны и как ими управлять](https://practicum.yandex.ru/blog/chto-takoe-ad-hoc-zadachi/)
[^3-basic-distances-in-data-science]: [Евклидова, L1 и Чебышёва — 3 основные метрики, которые пригодятся в Data Science](https://tproger.ru/translations/3-basic-distances-in-data-science)
[^Feature-normalization]: [Нормализация признаков](https://deepmachinelearning.ru/docs/Machine-learning/Data-preprocessing/Feature-normalization)
[^sparse-matrix]: [Python-school: введение в разреженные матрицы](https://python-school.ru/blog/python/sparse-matrix/)
